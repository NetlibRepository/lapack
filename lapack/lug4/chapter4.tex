\chapter{Accuracy and Stability}\label{chapaccstab}

\newcommand{\calS}{{\cal S}}
\newcommand{\hatcalS}{{\hat{\cal S}}}
\newcommand{\shat}{\hat{s}}
\newcommand{\Shat}{\hat{S}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\alg}{{\rm alg}}
\def\stacksymbols #1#2#3#4{\def\theguybelow{#2}
    \def\verticalposition{\lower#3pt}
    \def\spacingwithinsymbol{\baselineskip0pt\lineskip#4pt}
    \mathrel{\mathpalette\intermediary#1}}
\def\intermediary#1#2{\verticalposition\vbox{\spacingwithinsymbol
      \everycr={}\tabskip0pt
      \halign{$\mathsurround0pt#1\hfil##\hfil$\crcr#2\crcr
               \theguybelow\crcr}}}
\def\leapproxeq{\mathrel{\raisebox{-.75ex}{$\mathop{\sim}\limits^{\textstyle <}$}}}
\def\geapproxeq{\mathrel{\raisebox{-.75ex}{$\mathop{\sim}\limits^{\textstyle >}$}}}
\def\ltapprox{\stacksymbols{<}{\sim}{2.5}{.2}}
\def\gtapprox{\stacksymbols{>}{\sim}{2.5}{.2}}

\newcommand{\nrminf}[1]{ \| #1 \|_\infty }
\newcommand{\nrmtwo}[1]{ \| #1 \|_2 }
\newcommand{\nrmfro}[1]{ \| #1 \|_F }
\newcommand{\nrmone}[1]{ \| #1 \|_1 }

\newcommand{\norma}[1]{ \| #1 \| }
\newcommand{\ds}{\displaystyle}
\newcommand{\ba}{\bar}
\newcommand{\ti}{\tilde}
\newcommand{\ha}{\widehat}

In addition to providing faster routines than previously available,
LAPACK provides more comprehensive and better\index{error bounds}
error\index{stability} bounds\index{accuracy}.
Our goal is to provide error bounds for most quantities computed by LAPACK.

In this chapter we explain our overall approach to obtaining error bounds,
and provide enough information to use the software.
The comments at the beginning of the individual routines should be consulted
for more details.
It is beyond the scope of this chapter to justify all the bounds
we present. Instead, we give references to the literature.
For example, standard material on error analysis can be found
in \cite{demmelMA221,GVL2,higham02,stewartsun90}.

In order to make this chapter easy to read, we have labeled sections
not essential for a first reading as {\bf Further Details}. The sections
not labeled as {\bf Further Details} should provide all the information
needed to understand and use the main error bounds computed by
LAPACK.  The {\bf Further Details} sections provide mathematical background,
references, and tighter but more expensive error bounds, and may be read later.

In section~\ref{secroundoff}
we discuss the sources of numerical error, in particular roundoff
error.
Section~\ref{secnormnotation} discusses how to measure errors,
as well as some standard notation.
Section~\ref{secwhereerrorboundscomefrom} discusses further details
of how error bounds are derived.
Sections~\ref{secAx=b} through \ref{secGSVDbound} present error bounds
for linear equations, linear least squares problems,
generalized linear least squares problems,
the symmetric eigenproblem,
the nonsymmetric eigenproblem,
the singular value decomposition,
the generalized symmetric definite eigenproblem,
the generalized nonsymmetric eigenproblem
and the
generalized (or quotient) singular value decomposition respectively.
Section~\ref{secfastblas} discusses the impact of fast
Level 3 BLAS\index{BLAS!Level 3, fast} on
the accuracy\index{accuracy} of LAPACK routines.


\section{Sources of Error in Numerical Calculations}\label{secroundoff}

\index{numerical error, sources of}
\index{numerical error, sources of!roundoff error}
\index{numerical error, sources of!input error}
\index{roundoff error}
\index{input error}
There are two sources of error whose effects can be measured by the bounds in
this chapter: {\em roundoff error} and {\em input error}. Roundoff error
arises from rounding results of floating-point operations during
the algorithm.
Input error is error in the input to the algorithm from prior calculations or
measurements.  We describe roundoff error first, and then input error.

Almost all the error bounds LAPACK provides are multiples of
{\em machine epsilon},\index{machine precision}\index{floating-point arithmetic!machine precision}\index{floating-point arithmetic}
which we abbreviate
by $\epsilon$. Machine epsilon bounds the roundoff in individual
floating-point operations.
It may be loosely defined as the largest relative error
\index{error!relative}\index{relative error}
in any floating-point operation that neither overflows nor underflows.
(Overflow means the result is too
large to represent accurately, and underflow means the result is too
small to represent accurately.) Machine epsilon is available by
the function call
\indexR{xLAMCH}SLAMCH('Epsilon') (or simply xLAMCH('E')).
See section~\ref{secbackgroundfloatingpoint} and
Table~\ref{tabIEEEvalues} for a discussion of common values of machine
epsilon.
\index{floating-point arithmetic!overflow}\index{overflow}
\index{floating-point arithmetic!underflow}\index{underflow}

Since underflow is almost always less significant than roundoff,
we will not consider it further.
Overflow usually means the computation is invalid, but there
are some LAPACK routines that routinely generate and handle overflows
using the rules of IEEE arithmetic
(see section~\ref{secbackgroundfloatingpoint}).

Bounds on {\em input errors}, or errors in the input parameters inherited
from prior computations or measurements, may be easily incorporated
into most LAPACK error bounds.
Suppose the input data is accurate to, say, 5 decimal digits
(we discuss exactly what
this means in section~\ref{secnormnotation}). Then one simply replaces
$\epsilon$ by $\max(\epsilon, 10^{-5})$ in the error bounds.

\subsection{Further Details:  Floating Point Arithmetic}\label{secbackgroundfloatingpoint}

\index{numerical error, sources of!roundoff error}
\index{roundoff error}
Roundoff error is bounded in terms of the {\em machine precision}
$\epsilon$,\index{floating-point arithmetic!machine precision}\index{machine precision}
which is the smallest value satisfying
\[
| fl(a \oplus b ) - (a \oplus b ) | \leq \epsilon \cdot | a \oplus b | \; \; ,
\]
where $a$ and $b$ are floating-point numbers\index{floating-point arithmetic!roundoff error},
$\oplus$ is any one of the four operations $+$, $-$, $\times$ and $\div$,
and $fl(a \oplus b)$ is the floating-point result of $a \oplus b$.
Machine epsilon, $\epsilon$, is the smallest value for which this inequality
is true for all $\oplus$, and for all $a$ and $b$ such that
$a \oplus b$ is neither too large (magnitude exceeds the overflow
threshold)\index{floating-point arithmetic!overflow}\index{overflow}
nor too small
(is nonzero with magnitude less than the underflow threshold)\index{floating-point arithmetic!underflow}\index{underflow}
to be represented accurately in the machine.
We also assume $\epsilon$ bounds the relative error in unary\index{error!relative}\index{relative error}
operations like square root:
\[
| fl( \sqrt{a} ) - (\sqrt{a} ) | \leq \epsilon \cdot | \sqrt{a} | \; .
\]

A precise characterization of $\epsilon$ depends on the details of the
machine arithmetic and sometimes even of the compiler.
For example, if addition and
subtraction are implemented without a guard digit (as is the
case on Cybers, Cray X-MP, Cray Y-MP, Cray~2 and Cray C90)
we must redefine $\epsilon$ to be the smallest number
such that
\[
| fl(a \pm b ) - (a \pm b ) | \leq \epsilon \cdot ( |a| + |b| ).
\]

In order to assure portability\index{portability},
machine parameters such as machine epsilon, the overflow threshold and
underflow threshold are computed at runtime by the auxiliary routine xLAMCH\index{installation!xLAMCH}\indexR{SLAMCH}; 
see subsection~\ref{subsecnaming} for an explanation
of the naming convention used for LAPACK routines. The alternative,
keeping a fixed table of machine parameter values, would degrade portability
because the table would have to be changed when moving from one machine,
or even one compiler, to another.

Actually, most machines, but not yet all, do have the same machine
parameters because they implement IEEE Standard Floating Point Arithmetic
\index{floating-point arithmetic!IEEE standard}
\cite{ieee754,ieee854}, which exactly specifies floating-point number
representations and operations. For
these machines, including all modern workstations and PCs,
the values of these parameters are given in
Table~\ref{tabIEEEvalues}.

\begin{table}[h]
\caption{Values of Machine Parameters in IEEE Floating Point Arithmetic}
\label{tabIEEEvalues}
\begin{center}
\begin{tabular} { | l | l | l | }
\hline
Machine parameter & Single Precision (32 bits) & Double Precision (64 bits) \\ \hline
Machine epsilon $\epsilon$ = xLAMCH('E') & $2^{-24}   \approx 5.96 \cdot 10^{-8}$
                                         & $2^{-53}   \approx 1.11 \cdot 10^{-16}$ \\
Underflow threshold = xLAMCH('U')& $2^{-126}  \approx 1.18 \cdot 10^{-38}$
                                 & $2^{-1022} \approx 2.23 \cdot 10^{-308}$ \\
Overflow  threshold = xLAMCH('O')& $2^{128} (1-\epsilon) \approx 3.40 \cdot 10^{38}$
                                 & $2^{1024}(1-\epsilon) \approx 1.79 \cdot 10^{308}$ \\
\hline
\end{tabular}
\end{center}
\end{table}

As stated above, we will ignore overflow and underflow in discussing error
bounds. References \cite{demmel84,higham02} discuss extending error bounds
to include
underflow, and show that for many common computations, when underflow occurs it
is less significant than roundoff. With some important exceptions described below,
overflow usually means that a computation has failed so the error bounds do not apply.
\index{floating-point arithmetic!overflow}
\index{overflow}
\index{floating-point arithmetic!underflow}
\index{underflow}

\ignore{
Overflow generally causes an error
message and stops execution, so the error bounds do not
apply.
Machines implementing IEEE arithmetic can continue to compute
past overflows, and even division by zero, square roots of negative numbers, etc.,
by producing infinity and NaN (``Not a Number'') symbols. These are
special floating-point numbers subject to special rules of arithmetic.
The default on many systems is to
continue computing with these symbols, rather than giving an error message,
which would often be more convenient for debugging.
It is also possible to stop with an error message. The user should consult
the system manual to see how to turn error messages on or off.
\index{floating-point arithmetic!infinity}
\index{infinity}
\index{NaN}
\index{Not-a-Number}
\index{floating-point arithmetic!NaN}
\index{floating-point arithmetic!Not-a-Number}
\index{floating-point arithmetic!overflow}
\index{overflow}
\index{floating-point arithmetic!underflow}
\index{underflow}
}

Therefore, most of our error bounds will simply be proportional to machine epsilon.
This means, for example, that if the
same problem in solved in double precision and single precision, the error bound
in double precision will be smaller than the error bound in single precision
by a factor
of $\epsilon_{\rm double} / \epsilon_{\rm single}$. In IEEE arithmetic, this
ratio is $2^{-53}/2^{-24} \approx 10^{-9}$,
meaning that one expects the double precision answer to have approximately nine
more decimal digits correct than the single precision answer.

LAPACK routines are generally insensitive to the details of rounding and
exception handling.
One algorithm, xLASV2\indexR{SLASV2},
can return significantly more accurate results if addition and subtraction
have a guard digit, but is still quite accurate if they do not
(see the end of section \ref{secsvd}).

However, several LAPACK routines do make assumptions about details of
the floating point arithmetic. We list these routines here.
\begin{itemize}
\item Infinity and NaN arithmetic. In IEEE arithmetic, there are specific rules
for evaluating quantities like $1/0$ and $0/0$. Overflowed quantities and
division-by-zero (like $1/0$) result in a $\pm \infty$ symbol, which continues
to propagate through the computation using rules like $3/\infty = 0$.
In particular, there is no error message or termination of execution.
Similarly, quantities like $0/0$ and $\infty / \infty$ must be replaced by
NaN (the ``Not a Number'' symbol) and propagated as well. See \cite{ieee754,ieee854}
for details. The following LAPACK routines, and the routines that call them,
assume the presence of this infinity and NaN arithmetic for their correct functioning:
   \begin{itemize}
   \item xSTEMR, which computes eigenvalues and eigenvectors of symmetric tridiagonal
         matrices. It is called by the drivers for the symmetric and Hermitian
         eigenproblems xSYEVR, xHEEVR and xSTEVR. (xSYEVR and the
         other drivers call ILAENV to
         check if IEEE arithmetic is available, and uses another algorithm if it is not.
         If LAPACK is installed by following the directions in Chapter~6, then
         ILAENV will return the correct information about the availability of
         IEEE arithmetic. If xSYEVR or the other drivers are used without this
         installation procedure,
         then the default is for ILAENV to say the IEEE arithmetic is available,
         since this is most common and faster.)
\indexR{SSTEMR}\indexR{CSTEMR}
\indexR{SSYEVR}\indexR{CSYEVR}
\indexR{SHEEVR}\indexR{CHEEVR}
\indexR{SSTEVR}
   \end{itemize}
\item Accuracy of add/subtract. If there is a {\em guard digit} in addition and
   subtraction, or if there is no guard digit but addition and subtraction are
   performed in the way they are on the Cray C90, Cray YMP, Cray XMP or Cray 2,
   then we can guarantee that the following routines work correctly. (They could
   theoretically fail on a hexadecimal or decimal machine without a guard digit,
   but we know of no such machine.)
   \begin{itemize}
   \item xSTEDC, which uses divide and conquer to find the eigenvalues and eigenvectors
         of a symmetric tridiagonal matrix. It is called by all the drivers for
         the symmetric, Hermitian, generalized symmetric definite and
         generalized Hermitian definite eigenvalue drivers with names ending in -EVD
         or -GVD.
\indexR{SSTEDC}
\indexR{CSTEDC}
\indexR{SSYEVD}
\indexR{SSBEVD}
\indexR{SSPEVD}
\indexR{SSTEVD}
\indexR{CHEEVD}
\indexR{CHBEVD}
\indexR{CHPEVD}
\indexR{SSYGVD}
\indexR{CHEGVD}
\indexR{SSPGVD}
\indexR{CHPGVD}
\indexR{SSBGVD}
\indexR{CHBGVD}
   \item xBDSDC, which uses divide and conquer to find the SVD
         of a bidiagonal matrix. It is called by xGESDD.
\indexR{SBDSDC}
\indexR{SGESDD}\indexR{CGESDD}
   \item xLALSD, which uses divide and conquer to solve a bidiagonal least squares
         problem with the SVD. It is called by xGELSD.
\indexR{SLALSD}
\indexR{SGELSD}\indexR{CGELSD}
   \end{itemize}
\end{itemize}
\index{floating-point arithmetic!guard digit}
\index{guard digit}
\index{floating-point arithmetic!infinity}
\index{infinity}
\index{NaN}
\index{Not-a-Number}
\index{floating-point arithmetic!NaN}
\index{floating-point arithmetic!Not-a-Number}
\index{floating-point arithmetic!overflow}
\index{overflow}
\index{floating-point arithmetic!underflow}
\index{underflow}

\section{How to Measure Errors}\label{secnormnotation}

\index{error!measurement of}
LAPACK routines return four types of floating-point output arguments:
\begin{itemize}
\index{error!measurement of!scalar}
\item {\em Scalar}, such as an eigenvalue of a matrix,
\index{error!measurement of!vector}
\item {\em Vector}, such as the solution $x$ of a linear system $Ax=b$,
\index{error!measurement of!matrix}
\item {\em Matrix}, such as a matrix inverse $A^{-1}$, and
\index{error!measurement of!subspace}
\item {\em Subspace}, such as the space spanned by one or more eigenvectors of a matrix.
\end{itemize}
This section provides measures for errors in these quantities, which we
need in order to express error bounds.

\index{error!measurement of!scalar}
First consider {\em scalars}. Let the scalar $\hat{\alpha}$ be an approximation of
the true answer $\alpha$. We can measure the difference between $\alpha$
and $\hat{\alpha}$ either by the {\bf absolute error}
$| \hat{\alpha} - \alpha |$, or, if $\alpha$ is nonzero, by the {\bf relative error}
$| \hat{\alpha} - \alpha | / | \alpha |$. Alternatively, it is sometimes more convenient
to use $| \hat{\alpha} - \alpha | / | \hat{\alpha} |$ instead of the standard expression
for relative error (see section~\ref{secbackgroundnormnotation}).
If the relative error of $\hat{\alpha}$ is, say $10^{-5}$, then we say that
$\hat{\alpha}$ is {\em accurate to 5 decimal digits}.
\index{error!absolute}\index{absolute error}
\index{error!relative}\index{relative error}

\index{error!measurement of!vector}
In order to measure the error in {\em vectors}, we need to measure the {\em size}
or {\em norm} of a vector $x$\index{norm!vector}. A popular norm
is the magnitude of the largest component, $\max_{1 \leq i \leq n} |x_i|$, which we denote
$\| x \|_{\infty}$. This is read {\em the infinity norm of} $x$.
See Table~\ref{tabnorms} for a summary of norms.

\begin{table}[h]
\caption{Vector and matrix norms}
\index{norm!vector}
\index{norm!matrix}
%\index{norm!one-norm}
\index{norm!1-norm}
%\index{norm!two-norm}
\index{norm!2-norm}
%\index{norm!infinity-norm}
\index{norm!$\infty$-norm}
\index{norm!Frobenius norm}
\label{tabnorms}
\begin{center}
\begin{tabular}{|l|l|l|} \hline
   &  Vector  & Matrix \\ \hline
1-norm      & $\|x\|_{1} = \sum_i |x_i|$ &
                $\|A\|_{1} = \max_j \sum_i |a_{ij}|$ \\
2-norm      & $\|x\|_2 = ( \sum_i |x_i|^2 )^{1/2}$ &
                $\|A\|_2 = \max_{x \neq 0} \|Ax\|_2 / \|x\|_2$ \\
Frobenius norm& $\|x\|_F = \|x\|_2$   &
                $\|A\|_F = ( \sum_{ij} |a_{ij}|^2 )^{1/2}$ \\
$\infty$-norm & $\|x\|_{\infty} = \max_i |x_i|$ &
                $\|A\|_{\infty} = \max_i \sum_j |a_{ij}|$ \\
\hline
\end{tabular}
\end{center}
\end{table}

If $\hat{x}$ is an approximation to the
exact vector $x$, we will refer to $\| \hat{x} - x \|_{p}$ as the
absolute error in $\hat{x}$ (where $p$ is one of the values in Table~\ref{tabnorms}),
\index{error!absolute}\index{absolute error}
\index{error!relative}\index{relative error}
and refer to $\| \hat{x} - x \|_{p} / \| x \|_{p}$ as the relative error
in $\hat{x}$ (assuming $\| x \|_{p} \neq 0$). As with scalars,
we will sometimes use $\| \hat{x} - x \|_{p} / \| \hat{x} \|_{p}$
for the relative error.
As above, if the relative error of $\hat{x}$ is, say $10^{-5}$, then we say
that $\hat{x}$ is accurate to 5 decimal digits.
The following example illustrates these ideas:

\[
x = \bmat{c} 1 \\ 100 \\ 9 \emat \; \; , \; \;
\hat{x} = \bmat{c} 1.1 \\ 99 \\ 11 \emat
\]
\begin{eqnarray*}
\| \hat{x} - x \|_{\infty} = 2 \; , \;  & \displaystyle{
\frac{\| \hat{x} - x \|_{\infty}}{\| x \|_{\infty}} = .02 }  \; , \; &
\frac{\| \hat{x} - x \|_{\infty}}{\| \hat{x} \|_{\infty}} = .0202  \\
\| \hat{x} - x \|_{2} = 2.238  \; , \;  & \displaystyle{
\frac{\| \hat{x} - x \|_{2}}{\| x \|_{2}} = .0223 }  \; , \; &
\frac{\| \hat{x} - x \|_{2}}{\| \hat{x} \|_{2}} = .0225  \\
\| \hat{x} - x \|_{1} = 3.1  \; , \; & \displaystyle{
\frac{\| \hat{x} - x \|_{1}}{\| x \|_{1}} = .0282 }  \; , \; &
\frac{\| \hat{x} - x \|_{1}}{\| \hat{x} \|_{1}} = .0279
\end{eqnarray*}
Thus, we would say that $\hat{x}$ approximates $x$ to 2
decimal digits.

\index{error!measurement of!matrix}
Errors in {\em matrices} may also be measured with norms\index{norm!matrix}.
The most obvious
generalization of $\|x\|_{\infty}$ to matrices would appear to be
$\| A \| = \max_{i,j} |a_{ij}|$, but this does not have certain
important mathematical properties that make deriving error bounds
convenient (see section~\ref{secbackgroundnormnotation}).
Instead, we will use
$\| A \|_{\infty} = \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{ij}|$,
where $A$ is an $m$-by-$n$ matrix, or
$\| A \|_{1} = \max_{1 \leq j \leq n} \sum_{i=1}^m |a_{ij}|$;
see Table~\ref{tabnorms} for other matrix norms.
As before $\| \hat{A} - A \|_{p}$ is the absolute
error\index{error!absolute}\index{absolute error}
in $\hat{A}$, $\| \hat{A} - A \|_{p} / \| A \|_{p}$
is the relative error\index{error!relative}\index{relative error}
in $\hat{A}$, and a relative error in $\hat{A}$ of
$10^{-5}$ means $\hat{A}$ is accurate to 5 decimal digits.
The following example illustrates these ideas:
\[
A = \bmat{ccc} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 10 \emat \; \; , \; \;
\hat{A} = \bmat{ccc} .44 & 2.36 & 3.04 \\ 3.09 & 5.87 & 6.66 \\ 7.36 & 7.77 & 9.07 \emat
\]
\begin{eqnarray*}
\| \hat{A} - A \|_{\infty} = 2.44 \; , \;  & \displaystyle{
\frac{\| \hat{A} - A \|_{\infty}}{\| A \|_{\infty}} = .0976 }  \; , \;  &
\frac{\| \hat{A} - A \|_{\infty}}{\| \hat{A} \|_{\infty}} = .1008 \\
\| \hat{A} - A \|_{2} = 1.75 \; , \;  & \displaystyle{
\frac{\| \hat{A} - A \|_{2}}{\| A \|_{2}} = .1007 }  \; , \;  &
\frac{\| \hat{A} - A \|_{2}}{\| \hat{A} \|_{2}} = .1020 \\
\| \hat{A} - A \|_{1} = 1.83 \; , \;  & \displaystyle{
\frac{\| \hat{A} - A \|_{1}}{\| A \|_{1}} = .0963 }  \; , \;  &
\frac{\| \hat{A} - A \|_{1}}{\| \hat{A} \|_{1}} = .0975  \\
\| \hat{A} - A \|_{F} = 1.87 \; , \;  & \displaystyle{
\frac{\| \hat{A} - A \|_{F}}{\| A \|_{F}} = .1075 }  \; , \;  &
\frac{\| \hat{A} - A \|_{F}}{\| \hat{A} \|_{F}} = .1082
\end{eqnarray*}
so $\hat{A}$ is accurate to 1 decimal digit.

Here is some related notation we will use in our error bounds.
The {\bf condition number of a matrix} $A$ is defined as
\index{condition number}
$\kappa_p (A) \equiv \|A\|_p \cdot \|A^{-1}\|_p$, where $A$
is square and invertible, and $p$ is $\infty$ or one of the other
possibilities in Table~\ref{tabnorms}. The condition number
measures how sensitive $A^{-1}$ is to changes in $A$; the larger
the condition number, the more sensitive is $A^{-1}$. For example,
for the same $A$ as in the last example,
\[
A^{-1} \approx \bmat{ccc} -.667 & -1.333 & 1 \\ -.667 & 3.667 & -2 \\ 1 & -2 & 1 \emat
\; \; {\rm and} \; \; \kappa_{\infty} (A) = 158.33 \; \; .
\]
LAPACK
error estimation routines typically compute a variable called
{\tt RCOND}\index{arguments!RCOND}, which is the reciprocal of the condition number (or an
approximation of the reciprocal). The reciprocal of the condition
number is used instead of the condition number itself in order
to avoid the possibility of overflow when the condition number is very large.
\index{overflow}
\index{floating-point arithmetic!overflow}
Also, some of our error bounds will use the vector of absolute values
of $x$, $|x|$ ($|x|_i = |x_i |$), or similarly $|A|$
($|A|_{ij} = |a_{ij}|$).

\index{error!measurement of!subspace}
Now we consider errors in {\em subspaces}. Subspaces are the
outputs of routines that compute eigenvectors and invariant
subspaces of matrices.  We need a careful definition
of error in these cases for the following reason. The nonzero vector $x$ is called a
{\em (right) eigenvector} of the matrix $A$ with {\em eigenvalue}
$\lambda$ if $Ax = \lambda x$. From this definition, we see that
$-x$, $2x$, or any other nonzero multiple $\beta x$ of $x$ is also an
eigenvector. In other words, eigenvectors are not unique. This
means we cannot measure the difference between two supposed eigenvectors
$\hat{x}$ and $x$ by computing $\| \hat{x} - x \|_2$, because this may
be large while $\| \hat{x} - \beta x \|_2$ is small or even zero for
some $\beta \neq 1$. This is true
even if we normalize $x$ so that $\|x\|_2 = 1$, since both
$x$ and $-x$ can be normalized simultaneously. So in order to define
error in a useful way, we need to instead consider the set $\cal S$ of
all scalar multiples $\{ \beta x \; , \; \beta {\rm ~a~scalar} \}$ of
$x$. The set $\cal S$ is
called the {\em subspace spanned by $x$}, and is uniquely determined
by any nonzero member of $\cal S$. We will measure the difference
between two such sets by the {\em acute angle} between them.
Suppose $\hat{\cal S}$ is spanned by $\{ \hat x \}$ and
$\cal S$ is spanned by $\{ x \}$. Then the acute angle between
$\hat{\cal S}$ and $\cal S$ is defined as
\index{subspaces!angle between}
\index{angle between vectors and subspaces}
\[
\theta ( \hat{\cal S}, {\cal S} ) =
\theta ( \hat{x} , x ) \equiv \arccos
\frac{| \hat{x}^T x |}{ \| \hat{x} \|_2 \cdot \| x \|_2 } \; \; .
\]
One can show that $\theta ( \hat{x} , x )$ does not change when either
$\hat{x}$ or $x$ is multiplied by any nonzero scalar. For example, if
\[
x = \bmat{c} 1 \\ 100 \\ 9 \emat \; \; {\rm and} \; \;
\hat{x} = \bmat{c} 1.1 \\ 99 \\ 11 \emat
\]
as above, then $\theta ( \gamma \hat{x} , \beta x ) = .0209$ for any
nonzero scalars $\beta$ and $\gamma$.

Here is another way to interpret the angle $\theta$ between $\hat{\cal S}$ and
$\cal S$.
\index{subspaces!angle between}
\index{angle between vectors and subspaces}
Suppose $\hat{x}$ is a unit vector ($\| \hat{x} \|_2 = 1$).
Then there is a scalar $\beta$ such that
\[
\| \hat{x} - \beta x \|_2 = \frac{\sqrt{2} \sin \theta}{\sqrt{1+ \cos \theta}} \approx \theta
\; .
\]
The approximation $\approx \theta$ holds when $\theta$ is much less than 1
(less than .1 will do nicely).  If $\hat{x}$ is an approximate
eigenvector with error bound $\theta ( \hat{x} , x ) \leq \bar{\theta} \ll 1$,
where $x$ is a true eigenvector, there is another true eigenvector
$\beta x$ satisfying $\| \hat{x} - \beta x \|_2 \leapproxeq \bar{\theta}$.
For example, if
\[
\hat{x} = \bmat{c} 1.1 \\ 99 \\ 11 \emat \cdot (1.1^2 + 99^2 + 11^2)^{-1/2}
\; {\rm so} \; \| \hat{x} \|_2 = 1
\; \; {\rm and,} \; \;
x = \bmat{c} 1 \\ 100 \\ 9 \emat,
\]
then $\| \hat{x} - \beta x \|_2 \approx .0209$ for $\beta \approx .001$.

Some LAPACK routines also return subspaces spanned by more than one
vector, such as the invariant subspaces of matrices returned by xGEESX.
\indexR{SGEESX}\indexR{CGEESX}
The notion of angle between subspaces also applies here;
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
see section~\ref{secbackgroundnormnotation} for details.

Finally, many of our error bounds will contain a factor $p(n)$ (or $p(m,n)$),
which grows as a function of matrix dimension $n$ (or dimensions $m$ and $n$).
It represents a potentially different function for each problem.
In practice, the true errors usually grow just linearly; using
$p(n)=10n$ in the error bound formulas will often give a reasonable bound.
Therefore, we will refer to $p(n)$ as a ``modestly growing'' function of $n$.
However it can occasionally be much larger, see
section~\ref{secbackgroundnormnotation}.
{\bf For simplicity, the error bounds computed by the code fragments
in the following sections will use} $p(n)=1$.
{\bf This means these computed error bounds may occasionally
slightly underestimate the true error. For this reason we refer
to these computed error bounds as ``approximate error bounds''.}

\subsection{Further Details:  How to Measure Errors}\label{secbackgroundnormnotation}

\index{relative error}\index{error!relative}
The relative error $| \hat{\alpha} - \alpha | / | \alpha |$ in the approximation
$\hat{\alpha}$ of the true solution $\alpha$ has a drawback: it often cannot
be computed directly, because it depends on the unknown quantity
$| \alpha |$. However, we can often instead estimate
$| \hat{\alpha} - \alpha | / | \hat{\alpha} |$, since $\hat{\alpha}$ is
known (it is the output of our algorithm). Fortunately, these two
quantities are necessarily close together, provided either one is small,
which is the only time they provide a useful bound anyway. For example,
$| \hat{\alpha} - \alpha | / | \hat{\alpha} | \leq .1$ implies
\[
.9 \frac{| \hat{\alpha} - \alpha |}{| \hat{\alpha} |} \leq
\frac{| \hat{\alpha} - \alpha |}{| {\alpha} |} \leq
1.1 \frac{| \hat{\alpha} - \alpha |}{| \hat{\alpha} |} \; \; ,
\]
so they can be used interchangeably.

Table~\ref{tabnorms} contains a variety of norms we will use to
measure errors.
These norms have the properties that
$\|Ax\|_p \leq \|A\|_p \cdot \|x\|_p$, and
$\|AB\|_p \leq \|A\|_p \cdot \|B\|_p$, where $p$ is one of
$1$, $2$, $\infty$, and $F$. These properties are useful for deriving
error bounds.

An error bound that uses a given norm may be changed into an error bound
that uses another norm. This is accomplished by multiplying the first
error bound by an appropriate function of the problem dimension.
Table~\ref{tableVectorNormFpq} gives the
factors $f_{pq}(n)$ such that $\| x \|_p \leq f_{pq}(n) \|x \|_q$, where
$n$ is the dimension of $x$.

\begin{table}[h]
\caption{Bounding One Vector Norm in Terms of Another}
\label{tableVectorNormFpq}

\begin{center}
Values of $f_{pq}(n)$ such that $\| x \|_p \leq f_{pq}(n) \|x \|_q$, where $x$ is an $n$-vector \\

\begin{tabular}{|cc|c|c|c|}
\hline
    &          & \multicolumn{3}{|c|}{$q$} \\
    &          & \multicolumn{1}{|c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c|}{$\infty$}
\\ \hline
    &     1    & 1 & $\sqrt{n}$ & $n$        \\ \cline{3-5}
$p$ &     2    & 1 &     1      & $\sqrt{n}$ \\ \cline{3-5}
    & $\infty$ & 1 &     1      &     1      \\ \hline
\end{tabular}
\end{center}
\end{table}

Table~\ref{tableMatrixNormFpq} gives the
factors $f_{pq}(m,n)$ such that $\| A \|_p \leq f_{pq}(m,n) \| A \|_q$, where
$A$ is $m$-by-$n$.

\begin{table}[t]
\caption{Bounding One Matrix Norm in Terms of Another}
\label{tableMatrixNormFpq}

\begin{center}
Values of $f_{pq}(m,n)$ such that $\| A \|_p \leq f_{pq}(m,n) \| A \|_q$, where $A$ is $m$-by-$n$ \\

\begin{tabular}{|cc|c|c|c|c|}
\hline
    &          & \multicolumn{4}{|c|}{$q$} \\
    &          & \multicolumn{1}{|c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{$F$}
    & \multicolumn{1}{c|}{$\infty$} \\ \hline
    &     1    & 1          &     $\sqrt{m}$      & $\sqrt{m}$ &    $m$      \\ \cline{3-6}
$p$ &     2    & $\sqrt{n}$ &         1           &       1    & $\sqrt{m}$  \\ \cline{3-6}
    &   $F$    & $\sqrt{n}$ & $\sqrt{\min (m,n)}$ &       1    & $\sqrt{m}$  \\ \cline{3-6}
    & $\infty$ & $n$        &     $\sqrt{n}$      & $\sqrt{n}$ &     1       \\ \hline
\end{tabular}
\end{center}
\end{table}

The 2-norm of $A$, $\|A\|_2$, is also called the {\bf spectral
norm} of $A$, and is equal to the {\bf largest singular value}
$\sigma_{\max}(A)$ of $A$.
We shall also need to refer to the {\bf smallest singular value}
$\sigma_{\min}(A)$ of $A$; its value can be defined in a similar way to
the definition of the 2-norm in Table~\ref{tabnorms}, namely as
$\min_{x \neq 0} \|Ax\|_2 / \|x\|_2$ when $A$
has at least as many rows as columns, and defined as
$\min_{x \neq 0} \|A^Tx\|_2 / \|x\|_2$ when $A$ has more
columns than rows.  The 2-norm,
Frobenius norm\index{norm!Frobenius norm}\index{norm!2-norm},
and singular values of a matrix do not change
if the matrix is multiplied by a real orthogonal (or complex unitary) matrix.

Now we define {\em subspaces} spanned by more than one vector,
and {\em angles between subspaces}.
\index{subspaces}
\index{subspaces!angle between}
\index{angle between vectors and subspaces}
Given a set of $k$
$n$-dimensional vectors $\{ x_1 , ... , x_k \}$, they determine
a subspace $\cal S$ consisting of all their possible linear combinations
$\{ \sum_{i=1}^k \beta_i x_i$, $\beta_i$ scalars $ \}$. We also
say that $\{ x_1 , ... , x_k \}$ {\em spans} $\cal S$.
The difficulty in measuring the difference between subspaces is that
the sets of vectors spanning them are not unique.
For example, $\{ x \}$, $\{ -x \}$ and $\{ 2x \}$ all determine the
same subspace.
This means we cannot simply compare the subspaces spanned by
$\{ \hat{x}_1 , ... , \hat{x}_k \}$ and $\{ x_1 , ... , x_k \}$ by
comparing each $\hat{x}_i$ to $x_i$. Instead, we will measure the {\em angle}
between the subspaces, which is independent of the spanning set
of vectors. Suppose subspace $\hat{\cal S}$ is spanned by
$\{ \hat{x}_1 , ... , \hat{x}_k \}$ and that subspace $\cal S$
is spanned by $\{ x_1 , ... , x_k \}$. If $k=1$, we instead write more
simply $\{ \hat{x} \}$ and $\{ x \}$.
When $k=1$, we defined
the angle $\theta (\hat{\cal S}, {\cal S})$ between
$\hat{\cal S}$ and $\cal S$ as the acute angle
between $\hat{x}$ and $x$.
When $k>1$, we define the acute angle between $\hat{\cal S}$ and
${\cal S}$ as the largest acute angle between any vector $\hat{x}$ in
$\hat{\cal S}$, and the closest vector $x$ in $\cal S$ to $\hat{x}$:

\begin{eqnarray*}
\theta (\hat{\cal S}, {\cal S}) & \equiv  &
\max_{\hat{x} \in {\hat{\cal S}} \atop \hat{x} \neq 0}
\min_{x \in {\cal S} \atop x \neq 0}
\theta ( \hat{x},x ) \; \; .
\end{eqnarray*}
LAPACK routines which compute subspaces return
vectors $\{ \hat{x}_1 , ... , \hat{x}_k \}$ spanning a subspace
$\hat{\cal S}$ which are {\em orthonormal}. This means the
$n$-by-$k$ matrix $\hat{X}=[\hat{x}_1 , ... , \hat{x}_k]$
satisfies $\hat{X}^H \hat{X} = I$. Suppose also that
the vectors $\{ x_1 , ... , x_k \}$ spanning $\cal S$
are orthonormal, so $X = [ x_1 , ... , x_k ]$ also
satisfies $X^HX = I$.
Then there is a simple expression for the angle between
$\hat{\cal S}$ and $\cal S$:
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
\[
\theta ( \hat{\cal S}, {\cal S} ) = \arccos \sigma_{\min} ( \hat{X}^H X ) \; \; .
\]
For example, if
\[
\hat{X} = \bmat{cc} -.79996 & .60005 \\ -.59997 & -.79990 \\ -.01 & -.01 \emat
\; \; {\rm and} \; \;
X = \bmat{cc} 1 & 0 \\ 0 & 1 \\ 0 & 0 \emat
\]
then $\theta ( \hat{\cal S}, {\cal S} ) = .01414$.

As stated above, all our bounds will contain a factor
$p(n)$ (or $p(m,n)$), which measure how roundoff errors can grow
as a function of matrix dimension $n$ (or $m$ and $m$).
In practice, the true error usually grows just linearly with $n$,
but we can generally only prove much weaker bounds of the form $p(n)=O(n^3)$.
This is because we can not rule out the extremely unlikely possibility of rounding
errors all adding together instead of canceling on average. Using
$p(n) = O(n^3)$ would give very pessimistic and unrealistic bounds, especially
for large $n$, so we content ourselves with describing $p(n)$ as a
``modestly growing'' polynomial function of $n$. Using $p(n)=10n$ in
the error bound formulas will often give a reasonable bound.
For detailed derivations of various
$p(n)$, see \cite{GVL2,higham02,wilkinson1}.

There is also one situation where $p(n)$ can grow as large as $2^{n-1}$:
Gaussian elimination. This typically  occurs only on specially constructed
matrices presented in numerical analysis courses \cite[p. 212]{wilkinson1}\cite{higham02}.
However, the expert drivers for solving linear systems, xGESVX and xGBSVX,\indexR{SGESVX}\indexR{CGESVX}\indexR{SGBSVX}\indexR{CGBSVX}
provide error bounds incorporating $p(n)$, and so this rare possibility
can be detected.

\section{Further Details:  How Error Bounds Are Derived}\label{secwhereerrorboundscomefrom}

\subsection{Standard Error Analysis}\label{secbackwarderror}

\index{error!analysis}
We illustrate standard error analysis with the simple example of
evaluating the scalar function $y=f(z)$. Let the output of the
subroutine which implements $f(z)$ be denoted $\alg (z)$; this includes
the effects of roundoff. If $\alg (z) = f(z+\delta)$
where $\delta$ is small,
then we say $\alg$ is a {\bf backward stable}
\index{backward stability}
\index{stability!backward}
algorithm for $f$,
or that the {\bf backward error} $\delta$ is small.
\index{error!backward}
\index{backward error}
In other words, $\alg (z)$ is the
exact value of $f$ at a slightly perturbed input $z+\delta$.
(Sometimes our algorithms satisfy only $\alg (z)=f(z+ \delta) + \eta$ where both
$\delta$ and $\eta$ are small. This does not significantly change the following
analysis.)

Suppose now that $f$ is a smooth function, so that
we may approximate it near $z$ by a straight line:
$f(z+\delta) \approx f(z) + f'(z) \cdot \delta$.
Then we have the simple error estimate
\[
\alg (z)-f(z) = f(z+\delta) - f(z) \approx f'(z) \cdot \delta .
\]
Thus, if $\delta$ is small, and the derivative $f'(z)$ is
moderate, the error $\alg (z)-f(z)$ will be small. (More generally,
we only need Lipschitz continuity of $f$, and may use the Lipschitz
constant in place of $f'$ in deriving error bounds.)
This is often written
in the similar form
\[
\left| \frac{\alg (z)-f(z)}{f(z)} \right|
\approx
\left| \frac{f'(z) \cdot z}{f(z)} \right| \cdot
\left| \frac{\delta}{z} \right| \equiv \kappa (f,z)
\cdot \left| \frac{\delta}{z} \right|
.\]
This approximately bounds the {\bf relative error}
\index{error!relative}\index{relative error}
$\left| \frac{\alg (z)-f(z)}{f(z)} \right|$ by the product of
the {\bf condition number of}
$f$ {\bf at} $z$, $\kappa (f,z)$, and the
{\bf relative backward error} $|\frac{\delta}{z}|$.
\index{error!backward}
\index{backward error}
Thus we get an error bound by multiplying a
condition\index{condition number} number and
a backward error (or bounds for these quantities). We call a problem
{\bf ill-conditioned}\index{ill-conditioned} if its condition number is large,
and {\bf ill-posed}\index{ill-posed}
if its condition number is infinite (or does not exist). (This is
a different use of the term ill-posed than used in other contexts. For
example, to be well-posed (not ill-posed) in the sense of Hadamard,
it is sufficient for $f$ to be continuous,
whereas we require Lipschitz continuity.)

If $f$ and $z$ are vector quantities, then $f'(z)$ is a matrix
(the Jacobian). So instead of using absolute values as before,
we now measure $\delta$ by a vector norm $\| \delta \|$ and $f'(z)$
by a matrix norm $\|f'(z)\|$. The conventional (and coarsest) error analysis
uses a norm such as the $\infty$-norm. We therefore call
this {\bf normwise backward stability}.
\index{backward stability!normwise}
\index{stability!backward}
For example, a normwise stable
method for solving a system of linear equations $Ax=b$ will
produce a solution $\xhat$ satisfying $(A+E)\xhat=b+f$ where
$\|E\infnorm / \|A\infnorm$ and
$\|f\infnorm / \|b\infnorm$ are both small (close to machine epsilon).
In this case the condition number is
\mbox{$\kappai (A) = \|A\infnorm \cdot \|A^{-1}\infnorm$}
(see section \ref{secAx=b} below).
\index{condition number}

Almost all of the algorithms in LAPACK
are stable in the sense just described.
when applied to a matrix $A$
they produce the exact result for a slightly different matrix $A+E$,
where $\|E\infnorm / \|A\infnorm$ is of order $\epsilon$.
In a certain sense, a user can hardly ask for more, provided the
data is at all uncertain.

(There are some
caveats to this statement. When computing the inverse of a matrix,
the backward error $E$ is small taking the
columns of the computed inverse one at
a time, with a different $E$ for each column \cite{lapwn27}.
The same is true when computing
the eigenvectors of a nonsymmetric matrix.
When computing the eigenvalues and eigenvectors
of $A- \lambda B$, $AB- \lambda I$, $BA- \lambda I$ or $A B^{-1} - \lambda I$
with $A$ symmetric and $B$ symmetric and positive definite
(using xSYGV or xHEGV), then the method may not be backward normwise stable if
\indexR{SSYGV}\indexR{CHEGV}
$B$ has a large condition number $\kappai (B)$,
although it has useful error bounds in this case too
(see section \ref{secgendef}). Solving the Sylvester equation\index{Sylvester
equation}
$AX+XB=C$ for the matrix $X$ may not be backward stable, although
there are again useful error bounds for $X$ \cite{higham93}.)

It is often possible to compute the norm $\|E\|$ of the actual backward
error by computing a residual $r$, such as $r=Ax-b$ or $r=Ax - \lambda x$,
and suitably scaling its norm $\|r\|$. The expert driver routines for
solving $Ax=b$ do this, for example.
For details see \cite{GVL2,higham02,parlett,stewartsun90}.

Condition numbers may be expensive to compute
exactly.
For example, it costs about $\frac{2}{3} n^3$ operations to solve $Ax=b$
for a general matrix $A$, and computing $\kappai (A)$ {\em exactly} costs
an additional $\frac{4}{3} n^3$ operations, or twice as much.
But $\kappai (A)$ can be {\em estimated} in only $O(n^2)$
operations beyond those $\frac{2}{3} n^3$ necessary for solution,
a tiny extra cost.  Therefore, most of LAPACK's condition numbers
and error bounds are based on estimated condition
numbers\index{condition number!estimate}, using the method
of~\cite{hager84,higham1,nick2}.
The price one pays for using an estimated rather than an
exact condition number is occasional
(but very rare) underestimates of the true error; years of experience
attest to the reliability of our estimators, although examples
where they badly underestimate the error can be constructed \cite{higham90}.
Note that once a condition estimate is large enough,
(usually $O( 1/ \epsilon )$), it confirms that the computed
answer may be completely inaccurate, and so the exact magnitude
of the condition estimate conveys little information.

\subsection{Improved Error Bounds}\label{seccomponentwise}

The standard error analysis just outlined has a drawback: by using the
$\infty$-norm $\| \delta \infnorm$ to measure the backward error,
entries of equal magnitude in $\delta$ contribute equally to the final
error bound $\kappa (f,z) (\| \delta \|/\|z\|)$. This means that
if $z$ is sparse or has some very tiny entries, a normwise backward
stable algorithm may make very large changes in these entries
compared to their original values. If these tiny values are known accurately
by the user, these errors may be unacceptable,
or the error bounds may be unacceptably large.

For example, consider solving a diagonal system of linear equations $Ax=b$.
Each component of the solution is computed accurately by
Gaussian elimination: $x_i = b_i / a_{ii}$.
The usual error bound is approximately
$\epsilon \cdot \kappai (A) = \epsilon \cdot \max_i |a_{ii}| / \min_i |a_{ii}|$,
which can arbitrarily overestimate the true error, $\epsilon$, if at least one
$a_{ii}$ is tiny and another one is large.

LAPACK addresses this inadequacy by providing some algorithms
whose backward error $\delta$ is a tiny relative change in
each component of $z$: $| \delta_{i} | = O( \epsilon ) | z_{i} |$.
This backward error retains both the sparsity structure of $z$ as
well as the information in tiny entries. These algorithms are therefore
called {\bf componentwise relatively backward stable}.
Furthermore, computed error bounds reflect this stronger form of backward
error.
(For other algorithms, the answers and computed error bounds
are as accurate as though the algorithms were componentwise relatively backward
stable, even though they are not. These algorithms are called
{\bf componentwise relatively forward stable}.)
\index{forward stability!componentwise relative}
\index{forward stability}
\index{backward stability!componentwise}
\index{stability!backward}
\index{stability!forward}

If the input data has independent uncertainty in each component,
each component must have at least a small {\em relative} uncertainty,
since each is a floating-point number.
In this case, the extra uncertainty contributed by the algorithm is not much
worse than the uncertainty in the input data, so
one could say the answer provided by a componentwise
relatively backward stable algorithm is as accurate as the data
warrants \cite{lawn20}.

When solving $Ax=b$ using expert driver xyySVX or computational routine xyyRFS,
for example, we almost always
compute  $\xhat$ satisfying $(A+E) \xhat = b+f$, where
$e_{ij}$ is a small relative change in $a_{ij}$ and
$f_k$ is a small relative change in $b_k$. In particular, if $A$ is diagonal,
the corresponding error bound is always tiny, as one would
expect (see the next section).

LAPACK can achieve this accuracy \index{accuracy!high}
for linear equation solving,
the bidiagonal singular value decomposition, and
the symmetric tridiagonal eigenproblem,
and provides facilities for achieving this accuracy for least squares problems.
Future versions of LAPACK will also achieve this
accuracy for other linear algebra problems, as discussed below.

\ignore{
\section{How to Read Error Bounds}
\index{error bounds, how to read}\label{sechowtomeasureerrors}

Here we discuss some notation\index{error bounds, notation} used in all
the error bounds of later subsections.

All our bounds will contain the factor $p(n)$ (or $p(m,n)$), which grows as
a function of matrix dimension $n$ (or matrix dimensions $m$ and $n$).
It measures how errors can grow\index{error growth} as a function of
matrix dimension,
and represents a potentially different function for each problem.
In practice, it usually grows just linearly; $p(n) \leq 10 n$ is often true.
But we can generally only prove much weaker bounds of the form $p(n)=O(n^3)$,
since we can not rule out the extremely unlikely possibility of rounding
errors all adding together instead of canceling on average. Using
$p(n) = O(n^3)$ would give very pessimistic and unrealistic bounds, especially
for large $n$, so we content ourselves with describing $p(n)$ as a
``modestly growing'' function of $n$. For detailed derivations of various
$p(n)$, see \cite{GVL2,wilkinson1}.

There is also one situation where $p(n)$ can grow as large as $2^{n-1}$:
Gaussian elimination. This only occurs on specially constructed
matrices presented in numerical analysis courses \cite[p. 212]{wilkinson1}.
Thus we can assume
$p(n) \leq 10 n$ in practice for Gaussian elimination too.

For linear equation and least squares solvers for $Ax=b$, we will bound the
relative error $\|x- \hat{x}\|/\|x\|$ in the computed solution $\hat{x}$
\index{error!relative}\index{relative error}
where $x$ is the true solution (the choice of norm $\| \cdot \|$ will differ).
For eigenvalue problems we bound the error $|\lambda_i - \hat{\lambda}_i|$ in
the $i^{th}$ computed eigenvalue $\hat{\lambda}_i$, where $\lambda_i$ is
a true $i^{th}$ eigenvalue. For singular value problems we similarly bound
$| \sigma_i - \hat{\sigma}_i |$.

Bounding the error in computed eigenvectors and singular vectors $\hat{v}_i$
is more subtle because these vectors are not unique: even though we
restrict $\|\hat{v}_i\|_2 = 1$ and $\|v_i\|_2 = 1$, we may still multiply
them by arbitrary constants of absolute value 1. So to avoid ambiguity we bound
the {\em angular difference} between $\hat{v}_i$ and a true vector
$v_i$:
\begin{eqnarray}\label{defthetav}
\theta ( v_i , \hat{v}_i ) & = & {\mbox {\rm acute\ angle\ between\ }}
v_i \; {\mbox {\rm and\ }} \hat{v}_i \nonumber \\
& = & \arccos |v_i^H \hat{v}_i|.
\end{eqnarray}
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
When $\theta ( v_i , \hat{v}_i )$ is small, one can choose a constant $\alpha$
with absolute value 1 so that
$\| \alpha v_i - \hat{v}_i \|_2 \approx \theta ( v_i , \hat{v}_i )$.

In addition to bounds for individual eigenvectors, we supply bounds for
the spaces spanned
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
by collections of eigenvectors, because these may be
much more accurately determined than the individual eigenvectors which
span them. These spaces are called {\em invariant subspaces} in the case
of eigenvectors,
\index{invariant subspaces}
because if $v$ is any vector in the space, $Av$ is also
in the space, where $A$ is the matrix. Again, we will use angle to
measure the difference between a computed space $\hatcalS$ and
the true space $\calS$:
\begin{eqnarray}\label{defthetas}
\theta ( \calS  , \hatcalS ) & = & {\mbox {\rm acute\ angle\ between\ }}
\calS \; {\mbox {\rm and\ }} \hatcalS  \nonumber \\
& = &
\max_{s \in \calS \atop s \neq 0}
\min_{\shat \in \hatcalS \atop \shat \neq 0}
\theta (s,\shat)
\; \; {\rm or} \; \;
\max_{\shat \in \hatcalS \atop \shat \neq 0}
\min_{s \in \calS \atop s \neq 0}
\theta (s,\shat).
\end{eqnarray}
We may compute $\theta ( \calS  , \hatcalS )$ as follows. Let $S$ be a
matrix whose columns are orthonormal and span $\calS$. Similarly let
$\Shat$ be an orthonormal matrix with columns spanning $\hatcalS$. Then
\[
\theta ( \calS  , \hatcalS ) = \arccos \sigma_{\min} ( S^H \Shat ).
\]

Finally, we remark on the accuracy \index{accuracy} of our bounds when they are large.
Relative errors like {\mbox {$\|\hat{x} - x \|/\|x\|$}} and angular errors like
$\theta (\hat{v}_i , v_i )$ are only of interest when they are much less
than 1. We have correspondingly stated some bounds so that they are not
strictly true when they are close to 1, since rigorous bounds would have
been more complicated and supplied little extra information in the
interesting case of small errors. We have indicated these bounds by
using the symbol $\leapproxeq$, or ``approximately less than'', instead of
the usual $\leq$. Thus, when these bounds are close to 1 or greater,
they indicate that the computed answer may have no significant digits
at all, but do not otherwise bound the error.
}

\section{Error Bounds for Linear Equation Solving}\label{secAx=b}

\ignore{
Let $Ax=b$ be the system to be solved, and $\hat{x}$ the computed
solution. Let $n$ be the dimension of $A$.
An approximate error bound\index{error bounds!linear equations}
for $\hat{x}$ is provided in one of two ways, by using a simple
driver, or by using an expert driver:

\begin{enumerate}
\item Solve $Ax=b$ using a simple driver such as xGESV, xGBSV,
\indexR{SGESV}\indexR{CGESV}
\indexR{SGBSV}\indexR{CGBSV}
\indexR{SSYSV}\indexR{CSYSV}
\indexR{CHESV}
xSYSV, etc. (subsection~\ref{subsecdrivelineq}).
Then the approximate error bound
\[
\frac{\| \hat{x} - x \|_{\infty}}{\|x\|_{\infty}} \leq {\tt ERRBD}
\]
can be computed by the following code fragment.
(As discussed in
section~\ref{secnormnotation}, this approximate error bound
may underestimate the true error by a factor $p(n)$
which is a modestly growing function of the problem dimension $n$.
Often $p(n) \leq 10n$.)

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Get infinity-norm of A
      ANORM = SLANGE( 'I', N, N, A, LDA, WORK )
*     Solve system; The solution X overwrites B
      CALL SGESV( N, 1, A, LDA, IPIV, B, LDB, INFO )
      IF( INFO.GT.0 ) THEN
         PRINT *,'Singular Matrix'
      ELSE IF (N .GT. 0) THEN
*        Get reciprocal condition number RCOND of A
         CALL SGECON( 'I', N, A, LDA, ANORM, RCOND, WORK, IWORK, INFO )
         RCOND = MAX( RCOND, EPSMCH )
         ERRBD = EPSMCH / RCOND
      END IF
\end{verbatim}
\index{condition number}

For example, suppose
${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$,
\[
A = \bmat{ccc} 4 & 16000 & 17000 \\ 2 & 5 & 8 \\ 3 & 6 & 10 \emat
\; \; {\rm and} \; \;
b = \bmat{c} 100.1 \\ .1 \\ .01 \emat \; . \; \;
\]
Then (to 4 decimal places)
\[
x = \bmat{c} -.3974 \\ -.3349 \\ .3211 \emat \; , \; \;
\hat{x} = \bmat{c} -.3968 \\ -.3344 \\ .3207 \emat \; ,
\]
${\tt ANORM} = 3.300 \cdot 10^4$,
${\tt RCOND} = 3.907 \cdot 10^{-6}$,
the true reciprocal condition number $= 3.902 \cdot 10^{-6}$,
${\tt ERRBD} =  1.5 \cdot 10^{-2}$, and the true error
$= 1.5 \cdot 10^{-3}$.
\index{condition number}

\item Solve $Ax=b$ using an expert driver such as xGESVX, xGBSVX,
xSYSVX, etc. (subsection~\ref{subsecdrivelineq}),
\indexR{SGESVX}\indexR{CGESVX}
\indexR{SGBSVX}\indexR{CGBSVX}
\indexR{SSYSVX}\indexR{CSYSVX}
\indexR{CHESVX}
which provides an explicit error bound {\tt FERR}, measured
with the $\infty$-norm:
\index{arguments!FERR}
\[
\frac{\| \hat{x} - x \|_{\infty}}{\|x\|_{\infty}} \leq {\tt FERR}
\]
For example, the following code fragment solves
$Ax=b$ and computes an approximate error bound {\tt FERR}:

\begin{verbatim}
      CALL SGESVX( 'E', 'N', N, 1, A, LDA, AF, LDAF, IPIV,
     $             EQUED, R, C, B, LDB, X, LDX, RCOND, FERR, BERR,
     $             WORK, IWORK, INFO )
      IF( INFO.GT.0 ) PRINT *,'(Nearly) Singular Matrix'
\end{verbatim}

For the same {\tt A} and {\tt b} as above,
$\hat{x} = \bmat{c} -.3974 \\ -.3349 \\ .3211 \emat$,
${\tt FERR} = 3.0 \cdot 10^{-5}$,
and the actual error is $4.3 \cdot 10^{-7}$.

\end{enumerate}

This example illustrates
that the expert driver provides an error bound with less programming
effort than the simple driver, and also that it may produce a significantly
more accurate answer.
}

Let $Ax=b$ be the system to be solved, and $\hat{x}$ the computed
solution. Let $n$ be the dimension of $A$.
An approximate error bound\index{error bounds!linear equations}
for $\hat{x}$ may be obtained in one of the following two ways,
depending on whether the solution is computed by a simple driver or
an expert driver:

\begin{enumerate}
\item Suppose that $Ax=b$ is solved using the simple driver SGESV
\indexR{SGESV}
(subsection~\ref{subsecdrivelineq}).
Then the approximate error bound
\[
\frac{\| \hat{x} - x \|_{\infty}}{\|x\|_{\infty}} \leq {\tt ERRBD}
\]
can be computed by the following code fragment.
(As discussed in
section~\ref{secnormnotation}, this approximate error bound
may underestimate the true error by a factor $p(n)$
which is a modestly growing function of the problem dimension $n$.
Often $p(n) \leq 10n$.)

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Get infinity-norm of A
      ANORM = SLANGE( 'I', N, N, A, LDA, WORK )
*     Solve system; The solution X overwrites B
      CALL SGESV( N, 1, A, LDA, IPIV, B, LDB, INFO )
      IF( INFO.GT.0 ) THEN
         PRINT *,'Singular Matrix'
      ELSE IF (N .GT. 0) THEN
*        Get reciprocal condition number RCOND of A
         CALL SGECON( 'I', N, A, LDA, ANORM, RCOND, WORK, IWORK, INFO )
         RCOND = MAX( RCOND, EPSMCH )
         ERRBD = EPSMCH / RCOND
      END IF
\end{verbatim}
\index{condition number}

For example, suppose
${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$,
\[
A = \bmat{ccc} 4 & 16000 & 17000 \\ 2 & 5 & 8 \\ 3 & 6 & 10 \emat
\; \; {\rm and} \; \;
b = \bmat{c} 100.1 \\ .1 \\ .01 \emat \; . \; \;
\]
Then (to 4 decimal places)
\[
x = \bmat{c} -.3974 \\ -.3349 \\ .3211 \emat \; , \; \;
\hat{x} = \bmat{c} -.3968 \\ -.3344 \\ .3207 \emat \; ,
\]
${\tt ANORM} = 3.300 \cdot 10^4$,
${\tt RCOND} = 3.907 \cdot 10^{-6}$,
the true reciprocal condition number $= 3.902 \cdot 10^{-6}$,
${\tt ERRBD} =  1.5 \cdot 10^{-2}$, and the true error
$= 1.5 \cdot 10^{-3}$.
\index{condition number}

\item Suppose that $Ax=b$ is solved using the expert driver SGESVX
(subsection~\ref{subsecdrivelineq}).
\indexR{SGESVX}\indexR{CGESVX}
This routine provides an explicit error bound {\tt FERR}, measured
with the $\infty$-norm:
\index{arguments!FERR}
\[
\frac{\| \hat{x} - x \|_{\infty}}{\|x\|_{\infty}} \leq {\tt FERR}
\]
For example, the following code fragment solves
$Ax=b$ and computes an approximate error bound {\tt FERR}:

\begin{verbatim}
      CALL SGESVX( 'E', 'N', N, 1, A, LDA, AF, LDAF, IPIV,
     $             EQUED, R, C, B, LDB, X, LDX, RCOND, FERR, BERR,
     $             WORK, IWORK, INFO )
      IF( INFO.GT.0 ) PRINT *,'(Nearly) Singular Matrix'
\end{verbatim}

For the same {\tt A} and {\tt b} as above,
$\hat{x} = \bmat{c} -.3974 \\ -.3349 \\ .3211 \emat$,
${\tt FERR} = 3.0 \cdot 10^{-5}$,
and the actual error is $4.3 \cdot 10^{-7}$.

\end{enumerate}

This example illustrates
that the expert driver provides an error bound with less programming
effort than the simple driver, and also that it may produce a significantly
more accurate answer.

Similar code fragments, with obvious adaptations,
may be used with all the driver routines for linear
equations listed in Table~\ref{tabdrivelineq}.
For example, if a symmetric system is solved using the simple driver 
xSYSV\indexR{SSYSV}\indexR{CHESV}\indexR{CSYSV},
then xLANSY\indexR{SLANSY}\indexR{CLANSY}\indexR{CLANHE} 
must be used to compute {\tt ANORM}, and
xSYCON\indexR{SSYCON}\indexR{CSYCON}\indexR{CHECON} must be used
to compute {\tt RCOND}.

\subsection{Further Details:  Error Bounds for Linear Equation Solving}\label{secbackgroundAx=b}

The conventional error analysis of linear
equation\index{error bounds!linear equations} solving goes as follows.
Let $Ax=b$ be the system to be solved. Let $\hat{x}$ be the solution
computed by LAPACK using any of their linear equation solvers.
Let $r$ be
the residual $r = b - A \hat{x}$. In the absence of rounding error $r$
would be zero and $\hat{x}$ would equal $x$; with rounding error one can
only say the following:

\begin{quote}
The normwise backward error of the computed solution $\hat{x}$,
\index{backward error}
\index{error!backward}
with respect to the $\infty$-norm,
is the pair $E,f$ which minimizes
\[
\max \left( \frac{\| E \infnorm}{\| A \infnorm} ,
            \frac{\| f \infnorm}{\| b \infnorm} \right)
\]
subject to the constraint $(A+E) \hat{x} = b+f$.
The minimal value of
$\max \left( \frac{\| E \infnorm}{\| A \infnorm} ,
\frac{\| f \infnorm}{\| b \infnorm} \right)$
is given by
\[
\omegai =
\frac{\| r \infnorm }{\| A \infnorm \cdot \| \hat{x} \infnorm + \| b \infnorm } \; .
\]
One can show that the computed solution $\hat{x}$
satisfies $\omegai \leq p(n) \cdot \epsilon$,
where $p(n)$ is a modestly growing function of $n$.
The corresponding condition number is
$\kappai (A) \equiv \|A\infnorm \cdot \|A^{-1}\infnorm$.
\index{condition number}
The error $x-\hat{x}$ is bounded by
\[
\frac{\|x- \hat{x} \infnorm}{\| x \infnorm}
\leapproxeq 2 \cdot \omegai \cdot \kappai (A) = {\tt ERRBD} \; .
\]
In the first code fragment in the last section, $2 \cdot \omega_{\infty}$,
which is $4.504 \cdot 10^{-8}$ in the numerical example,
is approximated by $\epsilon  = 2^{-24} = 5.960 \cdot 10^{-8}$.
Approximations\index{condition number!estimate}
of  $\kappai (A)$ --- or, strictly speaking, its reciprocal {\tt RCOND} ---
are returned by computational routines
xyyCON (subsection~\ref{subseccomplineq}) or driver routines
xyySVX (subsection~\ref{subsecdrivelineq}). The code fragment
makes sure {\tt RCOND} is at least $\epsilon =$ {\tt EPSMCH} to
avoid overflow in computing
{\tt ERRBD}.\index{overflow}\index{floating-point arithmetic!overflow}
This limits
{\tt ERRBD} to a maximum of 1, which is no loss of generality since
a relative error of 1 or more indicates the same thing:
\index{error!relative}\index{relative error}
a complete loss of accuracy. \index{accuracy}
Note that the
value of {\tt RCOND} returned by xyySVX may apply to a linear
system obtained from $Ax=b$ by {\em equilibration}, i.e.
scaling the rows and columns of $A$ in order to make the
condition number smaller. This is the case in the second
code fragment in the last section, where the program
chose to scale the rows by the factors returned in
${\tt R} = (5.882 \cdot 10^{-5}, .125, .1 )$
and scale the columns by the factors returned in
${\tt C} = (3.333, 1.063, 1. )$,
resulting in ${\tt RCOND} = 3.454 \cdot 10^{-3}$.
\end{quote}

As stated in section~\ref{seccomponentwise},
this approach does not respect the presence
of zero or tiny entries in $A$. In contrast,
the LAPACK computational routines
xyyRFS (subsection~\ref{subseccomplineq}) or driver routines xyySVX
(subsection~\ref{subsecdrivelineq}) will (except in rare cases)
compute a solution $\hat{x}$ with the following properties:

\begin{quote}
The componentwise backward error
of the computed solution $\hat{x}$ is the pair $E,f$ which minimizes
\index{backward error}
\index{error!backward}
\[
\max_{i,j,k} \left( \frac{| e_{ij} |}{|a_{ij}|} ,
            \frac{| f_{k} |}{|b_{k}|} \right)
\]
(where we interpret $0/0$ as 0),
subject to the constraint $(A+E) \hat{x} = b+f$.
The minimal value of $\max_{i,j,k} \left( \frac{| e_{ij} |}{|a_{ij}|} ,
\frac{| f_{k} |}{|b_{k}|} \right)$
is given by
\[
\omegac = \max_i \frac{|r_i|}{ (|A| \cdot |\hat{x} | + |b|)_i} \; .
\]
One can show that for most problems the $\hat{x}$ computed by xyySVX
satisfies $\omegac \leq p(n) \cdot \epsilon$,
where $p(n)$ is a modestly growing function of $n$.
In other words, $\hat{x}$ is the exact solution of the
perturbed problem $(A+ E ) \hat{x} = b + f$
where $E$ and $f$ are small relative perturbations in each entry of $A$ and
$b$, respectively.
The corresponding condition number is
$\kappac (A,b,\hat{x}) \equiv {\| \, |A^{-1}| ( |A| \cdot | \hat{x} | + |b| )
\, \infnorm}/{\| \hat{x} \infnorm}$.
\index{condition number}
The error $x-\hat{x}$ is bounded by
\[
\frac{\| x- \hat{x} \infnorm}{\| \hat{x} \infnorm}
\leq \omegac \cdot \kappac (A,b,\hat{x})  .
\]

The routines xyyRFS and xyySVX return
\index{backward error}
\index{error!backward}
$\omegac$, which is called {\tt BERR}\index{arguments!BERR}
(for Backward ERRor),
and a bound on the the actual error
$\|x - \hat{x}\infnorm / \| \hat{x} \infnorm$, called {\tt FERR}
\index{arguments!FERR}
(for Forward ERRor), as
in the second code fragment in the last section.
{\tt FERR} is actually calculated by the following formula, which can
be smaller than the bound $\omegac \cdot \kappac (A,b,\hat{x})$ given above:
\[
\frac{\| x- \hat{x} \infnorm}{\| \hat{x} \infnorm} \leq {\tt FERR} =
\frac{\| \, |A^{-1}| ( |\hat{r}| + n \epsilon (|A| \cdot |\hat{x}| + |b|) )
\infnorm } {\| \hat{x} \infnorm}  \; \; .
\]
Here, $\hat{r}$ is the computed value of the residual $b-A \hat{x}$, and
the norm in the numerator is estimated using the same estimation
subroutine used for {\tt RCOND}.

The value of
{\tt BERR} for the example in the last section is $4.6 \cdot 10^{-8}$.

Even in the rare cases where xyyRFS fails to make
{\tt BERR} close to its minimum $\epsilon$, the error bound {\tt FERR}
may remain small. See \cite{ariolidemmelduff}
for details.
\end{quote}

\section{Error Bounds for Linear Least Squares Problems}\label{seclsq}

\ignore{
The linear least squares problem is to find $x$ that minimizes
$\| Ax-b \|_2$. Let $\hat{x}$ be the solution computed by the
simple driver xGELS (see section \ref{subsecdrivellsq}).
\indexR{SGELS}\indexR{CGELS}
We discuss error bounds for the most common case where $A$ is $m$-by-$n$
with $m > n$, and $A$ has full rank\index{error bounds!linear least squares};
this is called an {\em overdetermined least squares problem}
\index{linear least squares problem!overdetermined}
(the following code fragments deal with $m=n$ as well).
In this case, an approximate error
bound
\[
\frac{\| \hat{x} - x \|_2}{\| x \|_2} \leapproxeq {\tt ERRBD}
\]
may be  computed as follows:

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Get the 2-norm of the right hand side B
      BNORM = SNRM2( M, B, 1 )
*     Solve the least squares problem; The solution X overwrites B
      CALL SGELS( 'N', M, N, 1, A, LDA, B, LDB, WORK, LWORK, INFO )
      IF ( MIN(M,N) .GT. 0 ) THEN
*        Get the 2-norm of the residual A*X-B
         RNORM = SNRM2( M-N, B( N+1 ), 1 )
*        Get the reciprocal condition number RCOND of A
         CALL STRCON('I', 'U', 'N', N, A, LDA, RCOND, WORK, IWORK, INFO)
         RCOND = MAX( RCOND, EPSMCH )
         IF ( BNORM .GT. 0.0 ) THEN
            SINT = RNORM / BNORM
         ELSE
            SINT = 0.0
         ENDIF
         COST = MAX( SQRT( (1.0E0 - SINT)*(1.0E0 + SINT) ), EPSMCH )
         TANT = SINT / COST
         ERRBD = EPSMCH*( 2.0E0/(RCOND*COST) + TANT / RCOND**2 )
      ENDIF
\end{verbatim}
\index{condition number}

For example,
if ${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$,
\[
A = \bmat{ccc} 4 & 3 & 5 \\ 2 & 5 & 8 \\ 3 & 6 & 10 \\ 4 & 5 & 11 \emat
\; {\rm and} \;
b = \bmat{c} 100.1 \\ .1 \\ .01 \\ .01 \emat \; ,
\]
then, to 4 decimal places,
\[
x = \hat{x} = \bmat{c} 38.49 \\ 21.59 \\ -23.88 \emat \; \; ,
\]
${\tt BNORM} = 100.1$,
${\tt RNORM} = 8.843$,
${\tt RCOND} = 4.712 \cdot 10^{-2}$,
${\tt ERRBD} = 4.9 \cdot 10^{-6}$, and the true error
is $4.6 \cdot 10^{-7}$.

Slightly different code sequences are needed to use the expert
\indexR{SGELSX}\indexR{CGELSX}
\indexR{SGELSS}\indexR{CGELSS}
drivers xGELSX and xGELSS. Both use {\tt RCND} as an input parameter,
which is used to determine the rank of the input matrix (briefly,
\index{rank!numerical determination of}
the matrix is considered not to have full rank if its condition
number exceeds {\tt 1/RCND}).
\index{condition number}
When the matrix does not have full rank,
computing and interpreting error bounds is more complicated, and
the reader is referred to the next section.

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Get the 2-norm of the right hand side B
      BNORM = SNRM2( M, B, 1 )
*     Solve the least squares problem; The solution X overwrites B
      RCND = 0
      CALL SGELSX( M, N, 1, A, LDA, B, LDB, JPVT, RCND, RANK, WORK,
     $             INFO )
      IF ( RANK.LT.N ) THEN
         PRINT *,'Matrix less than full rank'
      ELSE IF ( MIN( M,N ) .GT. 0 ) THEN
*        Get the 2-norm of the residual A*X-B
         RNORM = SNRM2( M-N, B( N+1 ), 1 )
*        Get the reciprocal condition number RCOND of A
         CALL STRCON('I', 'U', 'N', N, A, LDA, RCOND, WORK, IWORK, INFO)
         RCOND = MAX( RCOND, EPSMCH )
         IF ( BNORM .GT. 0.0 ) THEN
            SINT = RNORM / BNORM
         ELSE
            SINT = 0.0
         ENDIF
         COST = MAX( SQRT( (1.0E0 - SINT)*(1.0E0 + SINT) ), EPSMCH )
         TANT = SINT / COST
         ERRBD = EPSMCH*( 2.0E0/(RCOND*COST) + TANT / RCOND**2 )
      END IF
\end{verbatim}
The numerical results of this code fragment on the above $A$ and $b$ are
the same as for the first code fragment.

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Get the 2-norm of the right hand side B
      BNORM = SNRM2( M, B, 1 )
*     Solve the least squares problem; The solution X overwrites B
      RCND = 0
      CALL SGELSS( M, N, 1, A, LDA, B, LDB, S, RCND, RANK, WORK, LWORK,
     $             INFO )
      IF ( RANK.LT.N ) THEN
         PRINT *,'Matrix less than full rank'
      ELSE IF ( MIN(M,N) .GT. 0 ) THEN
*        Get the 2-norm of the residual A*X-B
         RNORM = SNRM2( M-N, B( N+1 ), 1 )
*        Get the reciprocal condition number RCOND of A
         RCOND = MAX( S( N ) / S( 1 ), EPSMCH )
         IF ( BNORM .GT. 0.0 ) THEN
            SINT = RNORM / BNORM
         ELSE
            SINT = 0.0
         ENDIF
         COST = MAX( SQRT( (1.0E0 - SINT)*(1.0E0 + SINT) ), EPSMCH )
         TANT = SINT / COST
         ERRBD = EPSMCH*( 2.0E0/(RCOND*COST) + TANT / RCOND**2 )
      END IF
\end{verbatim}
\index{condition number}

Applied to the same $A$ and $b$ as above, the computed $\hat{x}$ is
nearly the same,
${\tt RCOND} = 5.428 \cdot 10^{-2}$,
${\tt ERRBD} = 4.0 \cdot 10^{-6}$, and the true error is
$6.6 \cdot 10^{-7}$.
}

The linear least squares problem is to find $x$ that minimizes
$\| Ax-b \|_2$.
We discuss error bounds for the most common case where $A$ is $m$-by-$n$
with $m > n$, and $A$ has full rank\index{error bounds!linear least squares};
this is called an {\em overdetermined least squares problem}
\index{linear least squares problem!overdetermined}
(the following code fragments deal with $m=n$ as well).

Let $\hat{x}$ be the solution computed by one of the driver routines
xGELS, xGELSY, xGELSS, or xGELSD (see section \ref{subsecdrivellsq}).
An approximate error
bound
\indexR{SGELS}\indexR{CGELS}
\indexR{SGELSY}\indexR{CGELSY}
\indexR{SGELSS}\indexR{CGELSS}
\indexR{SGELSD}\indexR{CGELSD}
\[
\frac{\| \hat{x} - x \|_2}{\| x \|_2} \leapproxeq {\tt ERRBD}
\]
may be  computed in one of the following ways, depending on which type
of driver routine is used:

\begin{enumerate}

\item Suppose the simple driver SGELS is used:

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Get the 2-norm of the right hand side B
      BNORM = SNRM2( M, B, 1 )
*     Solve the least squares problem; the solution X overwrites B
      CALL SGELS( 'N', M, N, 1, A, LDA, B, LDB, WORK, LWORK, INFO )
      IF ( MIN(M,N) .GT. 0 ) THEN
*        Get the 2-norm of the residual A*X-B
         RNORM = SNRM2( M-N, B( N+1 ), 1 )
*        Get the reciprocal condition number RCOND of A
         CALL STRCON('I', 'U', 'N', N, A, LDA, RCOND, WORK, IWORK, INFO)
         RCOND = MAX( RCOND, EPSMCH )
         IF ( BNORM .GT. 0.0 ) THEN
            SINT = RNORM / BNORM
         ELSE
            SINT = 0.0
         ENDIF
         COST = MAX( SQRT( (1.0E0 - SINT)*(1.0E0 + SINT) ), EPSMCH )
         TANT = SINT / COST
         ERRBD = EPSMCH*( 2.0E0/(RCOND*COST) + TANT / RCOND**2 )
      ENDIF
\end{verbatim}
\index{condition number}

For example,
if ${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$,
\[
A = \bmat{ccc} 4 & 3 & 5 \\ 2 & 5 & 8 \\ 3 & 6 & 10 \\ 4 & 5 & 11 \emat
\; {\rm and} \;
b = \bmat{c} 100.1 \\ .1 \\ .01 \\ .01 \emat \; ,
\]
then, to 4 decimal places,
\[
x = \hat{x} = \bmat{c} 38.49 \\ 21.59 \\ -23.88 \emat \; \; ,
\]
${\tt BNORM} = 100.1$,
${\tt RNORM} = 8.843$,
${\tt RCOND} = 4.712 \cdot 10^{-2}$,
${\tt ERRBD} = 4.9 \cdot 10^{-6}$, and the true error
is $4.6 \cdot 10^{-7}$.

\item Suppose the expert driver SGELSY is used.
\indexR{SGELSY}
This routine has an input argument {\tt RCND},
which is used to determine the rank of the input matrix (briefly,
\index{rank!numerical determination of}
the matrix is considered not to have full rank if its condition
number exceeds {\tt 1/RCND}).
\index{condition number}
The code fragment below only computes error bounds
if the matrix has been determined to have full rank.
When the matrix does not have full rank,
computing and interpreting error bounds is more complicated, and
the reader is referred to the next section.

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Get the 2-norm of the right hand side B
      BNORM = SNRM2( M, B, 1 )
*     Solve the least squares problem; the solution X overwrites B
      RCND = 0
      CALL SGELSY( M, N, 1, A, LDA, B, LDB, JPVT, RCND, RANK, WORK,
     $             LWORK, INFO )
      IF ( RANK.LT.N ) THEN
         PRINT *,'Matrix less than full rank'
      ELSE IF ( MIN( M,N ) .GT. 0 ) THEN
*        Get the 2-norm of the residual A*X-B
         RNORM = SNRM2( M-N, B( N+1 ), 1 )
*        Get the reciprocal condition number RCOND of A
         CALL STRCON('I', 'U', 'N', N, A, LDA, RCOND, WORK, IWORK, INFO)
         RCOND = MAX( RCOND, EPSMCH )
         IF ( BNORM .GT. 0.0 ) THEN
            SINT = RNORM / BNORM
         ELSE
            SINT = 0.0
         ENDIF
         COST = MAX( SQRT( (1.0E0 - SINT)*(1.0E0 + SINT) ), EPSMCH )
         TANT = SINT / COST
         ERRBD = EPSMCH*( 2.0E0/(RCOND*COST) + TANT / RCOND**2 )
      END IF
\end{verbatim}
The numerical results of this code fragment on the above $A$ and $b$ are
the same as for the first code fragment.

\item Suppose the other type of expert driver SGELSS or SGELSD is
used\indexR{SGELSS}\indexR{SGELSD}.
This routine also has an input argument {\tt RCND}, which is used to
determine the rank of the matrix $A$. The same code fragment can be used
to compute error bounds as for SGELSY,
except that the call to SGELSY must
be replaced by:

\begin{verbatim}
      CALL SGELSD( M, N, 1, A, LDA, B, LDB, S, RCND, RANK, WORK, LWORK,
     $             IWORK, INFO )
\end{verbatim}

and the call to STRCON must be replaced by:

\begin{verbatim}
         RCOND = S( N ) / S( 1 )
\end{verbatim}
\index{condition number}

Applied to the same $A$ and $b$ as above, the computed $\hat{x}$ is
nearly the same,
${\tt RCOND} = 5.428 \cdot 10^{-2}$,
${\tt ERRBD} = 4.0 \cdot 10^{-6}$, and the true error is
$6.6 \cdot 10^{-7}$.

\end{enumerate}

\subsection{Further Details:  Error Bounds for Linear Least Squares
Problems}\label{secbackgroundlsq}

The conventional error analysis of linear least squares problems goes
as follows\index{error bounds!linear least squares}.
As above, let $\xhat$ be the solution to minimizing
$\| Ax-b \|_2$ computed by
LAPACK using one of the least squares drivers
xGELS, xGELSY, xGELSS or xGELSD
(see subsection \ref{subsecdrivellsq}).
We discuss the most common case, where $A$ is
overdetermined\index{overdetermined system}
(i.e., has more rows than columns) and has full rank
\cite{bjorck3,demmelMA221,GVL2,higham02}:
\indexR{SGELS}\indexR{CGELS}
\indexR{SGELSY}\indexR{CGELSY}
\indexR{SGELSS}\indexR{CGELSS}
\indexR{SGELSD}\indexR{CGELSD}

\begin{quote}
The computed solution $\xhat$ has a small normwise backward error.
In other words $\xhat$ minimizes $\|(A+E) \xhat - (b+f)\|_2$, where
$E$ and $f$ satisfy
\index{backward error}
\index{error!backward}
\[
\max \left( \frac{\| E \|_2}{\| A \|_2} ,
            \frac{\| f \|_2}{\| b \|_2} \right) \leq p(n) \epsilon
\]
and $p(n)$ is a modestly growing function of $n$. We take $p(n)=1$ in
the code fragments above.
Let $\kappa_2 (A) = \sigma_{\max} (A)/\sigma_{\min} (A)$ (approximated by
1/{\tt RCOND} in the above code fragments),
$\rho = \|A \hat{x} -b\|_2$ (= {\tt RNORM} above), and $\sin ( \theta ) = \rho / \|b\|_2$
({\tt SINT = RNORM / BNORM} above). Here, $\theta$ is the acute angle between
the vectors $A \hat{x}$ and $b$.
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
Then when $p(n) \epsilon$ is small, the error $\xhat - x$ is bounded by
\[
\frac{\|x-\xhat\|_2}{\|x\|_2} \leapproxeq p(n) \epsilon
\left\{ \frac{2 \kappa_2 (A)}{\cos ( \theta )} + \tan ( \theta ) \kappa_2^2 (A)
\right\},
\]
where $\cos ( \theta ) $ = {\tt COST} and $\tan ( \theta )$ = {\tt TANT} in the code fragments
above.
\end{quote}

We avoid overflow by making sure {\tt RCOND} and {\tt COST} are both at least
$\epsilon =$ {\tt EPSMCH}, and by handling the case of a zero {\tt B} matrix
separately ({\tt BNORM = 0}).
\index{overflow}
\index{floating-point arithmetic!overflow}

$\kappa_2 (A) = \sigma_{\max} (A) / \sigma_{\min} (A)$ may be computed directly
from the singular values of $A$ returned by xGELSS or xGELSD (as in the code fragment)
or by xGESVD or xGESDD. It may also be approximated by using xTRCON following calls to
xGELS or xGELSY.  xTRCON estimates $\kappa_{\infty}$ or $\kappa_1$ instead
of $\kappa_2$, but these can differ from $\kappa_2$ by at most a factor of $n$.
\indexR{SGESVD}\indexR{CGESVD}
\indexR{SGESDD}\indexR{CGESDD}
\indexR{STRCON}\indexR{CTRCON}

If $A$ is rank-deficient, xGELSS, xGELSD and xGELSY can be used to
{\bf regularize} the
problem\index{regularization}\index{linear least squares problem!regularization}
by treating all singular values
less than a user-specified threshold
(${\tt RCND} \cdot \sigma_{\max} (A)$) as
exactly zero.  The number of singular values treated as nonzero is returned
in {\tt RANK}.  See \cite{bjorck3,GVL2,higham02}
for error bounds in this case, as well as
\index{arguments!RANK}
\cite{demmelhigham1} for the
underdetermined\index{linear least squares problem!underdetermined}
\index{underdetermined system} case.
The ability to deal with rank-deficient matrices is the principal attraction
of these three drivers, which are more expensive than the simple driver xGELS.

The solution of the overdetermined,
\index{overdetermined system}
\index{linear least squares problem!overdetermined system}
full-rank problem may also be
characterized as the solution of the linear system of equations
\[
\bmat{cc} I & A \\ A^T & 0 \emat \bmat{c} r \\ x \emat =
\bmat{c} b \\ 0 \emat.
\]
By solving this linear system using xyyRFS or xyySVX (see section
\ref{secAx=b}) componentwise error bounds can also be obtained
\cite{arioliduffderijk}.

\section{Error Bounds for Generalized Least Squares Problems}\label{secEbndGLSQ}
\label{sec_lseglm_drivers}

There are two kinds of generalized least squares problems that are
discussed in section~\ref{sec_lseglm_drivers}:
the {\em linear equality constrained
least squares problem} and the {\em general linear model problem}.

\subsection{Linear Equality Constrained Least Squares Problem}

The linear equality constrained least squares (LSE) problem is
$$      \min_x \| Ax - b \| \quad \mbox{subject to} \quad Bx = d   $$
where $A$ is an $m$-by-$n$ matrix, $B$ is a $p$-by-$n$ matrix,
$b$ is an $m$ vector, and $d$ is a $p$ vector, with $p \leq n \leq p+m$. \\

\noindent
The LSE problem is solved by the driver routine xGGLSE\indexR{SGGLSE}\indexR{CGGLSE}
(see section \ref{sec_lseglm_drivers}).
Let $\ha{x}$ be the value of $x$ computed by xGGLSE.
The approximate error bound
\begin{eqnarray*}
     \frac{ \| x - \ha{x} \|_2 }{ \| x \|_2 } & \lapproxeq & {\tt ERRBD} . \\
\end{eqnarray*}
can be computed by the following code fragment:

{\small
\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Get the 2-norm of the right hand side C
      CNORM = SNRM2( M, C, 1 )
      print*,'CNORM = ',CNORM
*     Solve the least squares problem with equality constraints
      CALL SGGLSE( M, N, P, A, LDA, B, LDA, C, D, Xc, WORK, LWORK, IWORK, INFO )
*     Get the Frobenius norm of A and B
      ANORM = SLANTR( 'F', 'U', 'N', N, N, A, LDA, WORK )
      BNORM = SLANTR( 'F', 'U', 'N', P, P, B( 1, N-P+1 ), LDA, WORK )
      MN = MIN( M, N )
      IF( N.EQ.P ) THEN
         APPSNM = ZERO
         RNORM = SLANTR( '1', 'U', 'N', N, N, B, LDB, WORK(P+MN+N+1) )
         CALL STRCON( '1', 'U', 'N', N, B, LDB, RCOND, WORK( P+MN+N+1 ),
     $                IWORK, INFO )
         BAPSNM = ONE/ (RCOND * RNORM )
      ELSE
*        Estimate norm of (AP)^+
         RNORM = SLANTR( '1', 'U', 'N', N-P, N-P, A, LDA, WORK(P+MN+1) )
         CALL STRCON( '1', 'U', 'N', N-P, A, LDA, RCOND, WORK( P+MN+1 ),
     $                IWORK, INFO )
         APPSNM = ONE/ (RCOND * RNORM )
*        Estimate norm of B^+_A
         KASE = 0
         CALL SLACN2( P, WORK( P+MN+1 ), WORK( P+MN+N+1 ), IWORK, EST, KASE,
     $                ISAVE )
   30    CONTINUE
            CALL STRSV( 'Upper', 'No trans', 'Non unit', P, B( 1, N-P+1 ),
     $                  LDB, WORK( P+MN+N+1 ), 1 )
            CALL SGEMV( 'No trans', N-P, P, -ONE, A( 1, N-P+1 ), LDA,
     $                  WORK( P+MN+N+1 ), 1, ZERO, WORK( P+MN+P+1 ), 1 )
            CALL STRSV( 'Upper', 'No transpose', 'Non unit', N-P, A, LDA,
     $                  WORK( P+MN+P+1 ), 1 )
            DO I = 1, P
               WORK( P+MN+I ) = WORK( P+MN+N+I )
            END DO
            CALL SLACN2( N, WORK( P+MN+N+1 ), WORK( P+MN+1 ), IWORK, EST, KASE,
     $                   ISAVE )
*
            IF( KASE.EQ.0 ) GOTO 40
            DO I = 1, P
               WORK( P+MN+N+I ) = WORK( MN+N+I )
            END DO
            CALL STRSV( 'Upper', 'Trans', 'Non unit', N-P, A, LDA,
     $                  WORK( P+MN+1 ), 1 )
            CALL SGEMV( 'Trans', N-P, P, -ONE, A( 1, N-P+1 ), LDA,
     $                  WORK( P+MN+1 ), 1, ONE, WORK( P+MN+N+1 ), 1 )
            CALL STRSV( 'Upper', 'Trans', 'Non unit', P, B( 1, N-P+1 ),
     $                  LDB, WORK( P+MN+N+1 ), 1 )
            CALL SLACN2( P, WORK( P+MN+1 ), WORK( P+MN+N+1 ), IWORK, EST, KASE,
     $                   ISAVE )
*
            IF( KASE.EQ.0 ) GOTO 40
         GOTO 30
   40    CONTINUE
         BAPSNM = EST
*
      END IF
*     Estimate norm of A*B^+_A
      IF( P+M.EQ.N ) THEN
         EST = ZERO
      ELSE
         R22RS = MIN( P, M-N+P )
         KASE = 0
         CALL SLACN2( P, WORK( P+MN+P+1 ), WORK( P+MN+1 ), IWORK, EST, KASE, 
     $                ISAVE )
   50    CONTINUE
            CALL STRSV( 'Upper', 'No trans', 'Non unit', P, B( 1, N-P+1 ),
     $                  LDB, WORK( P+MN+1 ), 1 )
            DO I = 1, R22RS
               WORK( P+MN+P+I ) = WORK( P+MN+I )
            END DO
            CALL STRMV( 'Upper', 'No trans', 'Non unit', R22RS,
     $                  A( N-P+1, N-P+1 ), LDA, WORK( P+MN+P+1 ), 1 )
            IF( M.LT.N ) THEN
               CALL SGEMV( 'No trans', R22RS, N-M, ONE, A( N-P+1, M+1 ), LDA,
     $                     WORK( P+MN+R22RS+1 ), 1, ONE, WORK( P+MN+P+1 ), 1 )
            END IF
            CALL SLACN2( R22RS, WORK( P+MN+1 ), WORK( P+MN+P+1 ), IWORK, EST,
     $                   KASE, ISAVE )
*
            IF( KASE.EQ.0 ) GOTO 60
            DO I = 1, R22RS
               WORK( P+MN+I ) = WORK( P+MN+P+I )
            END DO
            CALL STRMV( 'Upper', 'Trans', 'Non Unit', R22RS,
     $                  A( N-P+1, N-P+1 ), LDA, WORK( P+MN+1 ), 1 )
            IF( M.LT.N ) THEN
               CALL SGEMV( 'Trans', R22RS, N-M, ONE, A( N-P+1, M+1 ), LDA,
     $                     WORK( P+MN+P+1 ), 1, ZERO, WORK( P+MN+R22RS+1 ), 1 )
            END IF
            CALL STRSV( 'Upper', 'Trans', 'Non unit', P, B( 1, N-P+1 ), LDB,
     $                  WORK( P+MN+1 ), 1 )
            CALL SLACN2( P, WORK( P+MN+P+1 ), WORK( P+MN+1 ), IWORK, EST, KASE,
     $                   ISAVE )
*
            IF( KASE.EQ.0 ) GOTO 60
            GOTO 50
   60    CONTINUE
      END IF
      ABAPSN = EST
*     Get the 2-norm of Xc
      XNORM = SNRM2( N, Xc, 1 )
      IF( APPSNM.EQ.0.0E0 ) THEN
*        B is square and nonsingular
         ERRBD = EPSMCH*BNORM*BAPSNM
      ELSE
*        Get the 2-norm of the residual A*Xc - C
         RNORM = SNRM2( M-N+P, C( N-P+1 ), 1 )
*        Get the 2-norm of Xc
         XNORM = SNRM2( N, Xc, 1 )
*        Get the condition numbers
         CNDBA = BNORM*BAPSNM
         CNDAB = ANORM*APPSNM
*        Get the approximate error bound
         ERRBD = EPSMCH*( (1.0E0 + CNORM/(ANORM*XNORM))*CNDAB +
     $               RNORM/(ANORM*XNORM)*(1.0E0 + BNORM*ABAPSN/ANORM)*
     $               (CNDAB*CNDAB) + 2.0E0*CNDBA )
      END IF
\end{verbatim}
}

\noindent
For example, if ${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$,
$$  A = \left( \begin{tabular}{rrrr}
                  1 &  1 &  1 &  1 \\
                  1 &  3 &  1 &  1 \\
                  1 & -1 &  3 &  1 \\
                  1 &  1 &  1 &  3 \\
                  1 &  1 &  1 & -1
               \end{tabular} \right),
    b = \left( \begin{array}{r}
                   2 \\
                   1 \\
                   6 \\
                   3 \\
                   1
               \end{array} \right),
    B = \left( \begin{tabular}{rrrr}
                  1 &  1 &  1 & -1 \\
                  1 & -1 &  1 &  1\\
                  1 &  1 & -1 &  1
               \end{tabular} \right)
    \quad \mbox{and} \quad
    d = \left( \begin{array}{r}
                   1 \\
                   3 \\
                  -1
               \end{array} \right),
$$
then (to 7 decimal places),
$$  \ha{x} = \left( \begin{array}{r}
                             0.5000000 \\
                            -0.5000001 \\
                             1.4999999 \\
                             0.4999998
                        \end{array} \right).$$
The computed error bound ${\tt ERRBD} = 5.7\cdot10^{-7}$,
where ${\tt CNDAB} = 2.09$, ${\tt CNDBA} = 3.12$.
The true error $= 1.2\cdot10^{-7}$.
The exact solution is $x = [0.5, -0.5, 1.5, 0.5]^T$.

\subsubsection{Further Details:
Error Bounds for Linear Equality Constrained Least Squares Problems}
\label{sec_lseglm_lsedetails}

In this subsection, we will summarize the available error bounds.
The reader may also refer to \cite{lawn31,coxhigham,elden} for
further details.  \\

\noindent
Let $\ha{x}$ be the solution computed by the driver 
xGGLSE\indexR{SGGLSE}\indexR{CGGLSE} (see subsection
\ref{sec_lseglm_drivers}).  It is normwise
stable in a mixed forward/backward sense \cite{coxhigham}.
Specifically, $\ha{x} = \ba{x} + \Delta \ba{x}$, where $\ba{x}$ solves
$\min\{\| b + \Delta b - (A + \Delta A)x \|_2: \; (B + \Delta B)x = d \} $,
and
\begin{center}
\parbox{2in}{
  \begin{eqnarray*}
  \| \Delta \ba{x} \|_2 & \leq & q(m,n,p) \epsilon\|\ba{x}\|_2, \\
  \| \Delta A \|_F & \leq & q(m,n,p)\epsilon\|A\|_F,
  \end{eqnarray*} }\quad
\parbox{2in}{
  \begin{eqnarray*}
  \| \Delta b \|_2 & \leq & q(m,n,p)\epsilon\|b\|_2, \\
  \| \Delta B \|_F & \leq & q(m,n,p)\epsilon\|B\|_F,
  \end{eqnarray*}
}
\end{center}
$q(m,n,p)$ is a modestly growing function of $m$, $n$, and $p$.
We take $q(m,n,p) = 1$ in the code fragment above.
Let $X^{\dagger}$ denote the Moore-Penrose pseudo-inverse of $X$.
Let $\kappa_B(A) = \nrmfro{A}\nrmtwo{(AP)^\dagger}$( = {\tt CNDAB} above) and
    $\kappa_A(B) = \nrmfro{B}\nrmtwo{B^\dagger_A}$( = {\tt CNDBA} above)
where $P = I - B^\dagger B$ and $B^\dagger_A = (I - (AP)^\dagger A)B^\dagger$.
When $q(m,n,p)\epsilon$ is small, the error $x-\hat{x}$ is bounded by
$$
 \frac{\norma{x-\hat{x}}}{\norma{x}} \lapproxeq q(m,n,p)\epsilon\left\{
      \kappa_A(B) +
      \kappa_B(A)\left(\frac{\nrmtwo{b}}{\nrmfro{A}\nrmtwo{x}} + 1 \right) +
      \kappa^2_B(A)\left( \frac{\nrmfro{B}}{\nrmfro{A}}\nrmtwo{AB^\dagger_A} +
                     1\right)\frac{\nrmtwo{r}}{\nrmfro{A}\nrmtwo{x}} \right\}.
$$

\vspace{.2in}

\noindent
When $B = 0$ and $d = 0$, we essentially recover error bounds for the
linear least squares (LS) problem:
$$
 \frac{\nrmtwo{x-\ha{x}}}{\nrmtwo{x}} \leq q(m,n)\epsilon
      \left\{\kappa(A)\left(
          \frac{\nrmtwo{b}}{\nrmfro{A}\nrmtwo{x}} + 1 +
          \kappa(A)\frac{\nrmtwo{r}}{\nrmfro{A}\nrmtwo{x}} \right) \right\},
$$
where $\kappa(A) = \nrmfro{A}\nrmtwo{A^\dagger}$. Note that
the error in the standard least squares problem provided in section~\ref{secbackgroundlsq} is
\[
 \frac{\nrmtwo{x-\ha{x}}}{\nrmtwo{x}} \lapproxeq p(n)\epsilon
      \left\{\frac{2\kappa(A)}{\cos(\theta)} + \tan(\theta)\kappa^2(A)\right\}
      =  p(n)\epsilon \left\{\frac{2\kappa(A)\nrmtwo{b}}{\nrmtwo{Ax}} +
                  \kappa^2(A)\frac{\nrmtwo{r}}{\nrmtwo{Ax}}\right\}\\
\]
since $\sin(\theta) = \frac{\nrmtwo{A\ha{x}-b}}{\nrmtwo{b}}$.  If one assumes
that $q(m,n) = p(n) = 1$, then the bounds are essentially the same. \\

\subsection{General Linear Model Problem}

The general linear model (GLM) problem is
$$      \min_{x,y} \|y\| \quad \mbox{subject to} \quad d = Ax + By     $$
where $A$ is an $n$-by-$m$ matrix, $B$ is an $n$-by-$p$ matrix, and $d$
is a given $n$-vector, with $m \leq n \leq m+p$.   \\

\noindent
The GLM problem is solved by the driver routine xGGGLM\indexR{SGGGLM}\indexR{CGGGLM}
(see section \ref{sec_lseglm_drivers}).
Let $\ha{x}$ and $\ha{y}$ be the computed values of $x$ and $y$, respectively.
The approximate error bounds
\begin{eqnarray*}
     \frac{ \| x - \ha{x} \|_2 }{ \| x \|_2 } & \lapproxeq & {\tt XERRBD},  \\
     \frac{ \| y - \ha{y} \|_2 }{ \| y \|_2 } & \lapproxeq & {\tt YERRBD}
\end{eqnarray*}
can be computed by the following code fragment:

{\small
\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Compute the 2-norm of the left hand side D
      DNORM = SNRM2( N, D, 1 )
*     Solve the generalized linear model problem
      CALL SGGGLM( N, M, P, A, LDA, B, LDB, D, Xc, Yc, WORK,
     $            LWORK, IWORK, INFO )
*     Compute the F-norm of A and B
      ANORM = SLANTR( 'F', 'U', 'N', M, M, A, LDA, WORK( M+NP+1 ) )
      BNORM = SLANTR( 'F', 'U', 'N', N, P, B( 1, MAX( 1, P-N+1 ) ),
     $                LDB, WORK( M+NP+1 ) )
*     Compute the 2-norm of Xc
      XNORM = SNRM2( M, Xc, 1 )
*     Condition estimation
      IF( N.EQ.M ) THEN
         PBPSNM = ZERO
         TNORM = SLANTR( '1', 'U', 'N', N, N, A, LDA, WORK( M+NP+M+1 ) )
         CALL STRCON( '1', 'U', 'N', N, A, LDA, RCOND, WORK( M+NP+M+1 ),
     $                IWORK, INFO )
         ABPSNM = ONE / (RCOND * TNORM )
      ELSE
*        Compute norm of (PB)^+
         TNORM = SLANTR( '1', 'U', 'N', N-M, N-M, B( M+1, P-N+M+1 ), LDB,
     $                   WORK( M+NP+1 ) )
         CALL STRCON( '1', 'U', 'N', N-M, B( M+1, P-N+M+1 ), LDB, RCOND,
     $                WORK( M+NP+1 ), IWORK, INFO )
         PBPSNM = ONE / (RCOND * TNORM )
*        Estimate norm of A^+_B
         KASE = 0
         CALL SLACN2( N, WORK( M+NP+1 ), WORK( M+NP+N+1 ), IWORK, EST, KASE,
     $                ISAVE )
   30    CONTINUE
            CALL STRSV( 'Upper', 'No transpose', 'Non unit', N-M,
     $                  B( M+1, P-N+M+1 ), LDB, WORK( M+NP+N+M+1 ), 1 )
            CALL SGEMV( 'No transpose', M, N-M, -ONE, B( 1, P-N+M+1 ),
     $                  LDB, WORK( M+NP+N+M+1 ), 1, ONE,
     $                  WORK( M+NP+N+1 ), 1 )
            CALL STRSV( 'Upper', 'No transpose', 'Non unit', M, A, LDA,
     $                  WORK( M+NP+N+1 ), 1 )
            DO I = 1, P
               WORK( M+NP+I ) = WORK( M+NP+N+I )
            END DO
            CALL SLACN2( M, WORK( M+NP+N+1 ), WORK( M+NP+1 ), IWORK, EST, KASE,
     $                   ISAVE )
            IF( KASE.EQ.0 ) GOTO 40
            CALL STRSV( 'Upper', 'Transpose', 'Non unit', M, A, LDA,
     $                  WORK( M+NP+1 ), 1 )
            CALL SGEMV( 'Transpose', M, N-M, -ONE, B( 1, P-N+M+1 ), LDB,
     $                  WORK( M+NP+1 ), 1, ZERO, WORK( M+NP+M+1 ), 1 )
            CALL STRSV( 'Upper', 'Transpose', 'Non unit', N-M,
     $                  B( M+1, P-N+M+1 ), LDB, WORK( M+NP+M+1 ), 1 )
            DO I = 1, N
               WORK( M+NP+N+I ) = WORK( M+NP+I )
            END DO
            CALL SLACN2( N, WORK( M+NP+1 ), WORK( M+NP+N+1 ), IWORK, EST, KASE,
     $                   ISAVE )
            IF( KASE.EQ.0 ) GOTO 40
         GOTO 30
   40    CONTINUE
         ABPSNM = EST
      END IF
*     Estimate norm of (A^+_B)*B
      IF( P+M.EQ.N ) THEN
         EST = ZERO
      ELSE
         KASE = 0
         CALL SLACN2( P-N+M, WORK( M+NP+1 ), WORK( M+NP+M+1 ), IWORK, EST, KASE,
     $                ISAVE )
   50    CONTINUE
*
            IF( P.GE.N ) THEN
               CALL STRMV( 'Upper', 'No trans', 'Non Unit', M,
     $                     B( 1, P-N+1 ), LDB, WORK( M+NP+M+P-N+1 ), 1 )
               DO I = 1, M
                  WORK( M+NP+I ) = WORK( M+NP+M+P-N+I )
               END DO
            ELSE
               CALL SGEMV( 'No transpose', N-P, P-N+M, ONE, B, LDB,
     $                  WORK( M+NP+M+1 ), 1, ZERO, WORK( M+NP+1 ), 1 )
               CALL STRMV( 'Upper', 'No trans', 'Non Unit', P-N+M,
     $                     B( N-P+1, 1 ), LDB, WORK( M+NP+M+1 ), 1 )
               DO I = N-P+1, M
                  WORK( M+NP+I ) = WORK( M+NP+M-N+P+I )
               END DO
            END IF
            CALL STRSV( 'Upper', 'No transpose', 'Non unit', M, A, LDA,
     $                  WORK( M+NP+1 ), 1 )
            CALL SLACN2( M, WORK( M+NP+M+1 ), WORK( M+NP+1 ), IWORK, EST, KASE,
     $                   ISAVE )
*
            IF( KASE.EQ.0 ) GOTO 60
*
            CALL STRSV( 'Upper', 'Transpose', 'Non unit', M, A, LDA,
     $                  WORK( M+NP+1 ), 1 )
            IF( P.GE.N ) THEN
               CALL STRMV( 'Upper', 'Trans', 'Non Unit', M,
     $                     B( 1, P-N+1 ), LDB, WORK( M+NP+1 ), 1 )
               DO I = 1, M
                  WORK( M+NP+M+P-N+I ) = WORK( M+NP+I )
               END DO
               DO I = 1, P-N
                  WORK( M+NP+M+I ) = ZERO
               END DO
            ELSE
               CALL STRMV( 'Upper', 'Trans', 'Non Unit', P-N+M,
     $                     B( N-P+1, 1 ), LDB, WORK( M+NP+N-P+1 ), 1 )
               DO I = 1, P-N+M
                  WORK( M+NP+M+I ) = WORK( M+NP+N-P+I )
               END DO
               CALL SGEMV( 'Transpose', N-P, P-N+M, ONE, B, LDB,
     $                  WORK( M+NP+1 ), 1, ONE, WORK( M+NP+M+1 ), 1 )
            END IF
            CALL SLACN2( P-N+M, WORK( M+NP+1 ), WORK( M+NP+M+1 ), IWORK, EST,
     $                   KASE, ISAVE )
*
            IF( KASE.EQ.0 ) GOTO 60
         GOTO 50
   60    CONTINUE
      END IF
      ABPSBN = EST
*     Get condition numbers and approximate error bounds
      CNDAB = ANORM*ABPSNM
      CNDBA = BNORM*PBPSNM
      IF( PBPSNM.EQ.0.0E+0 ) THEN
*        Then A is square and nonsingular
         XERRBD = EPSMCH*( CNDAB*( ONE+DNORM/(ANORM*XNORM) ) )
         YERRBD = 0.0E+0
      ELSE
         XERRBD = EPSMCH*( CNDAB*( ONE+DNORM/(ANORM*XNORM) ) +
     $                2.0E0*CNDAB*CNDBA*CNDBA*DNORM/(ANORM*XNORM) +
     $                ABPSBN*ABPSBN*PBPSNM*PBPSNM*ANORM*DNORM/XNORM )
         YERRBD = EPSMCH*( ABPSBN*ANORM*PBPSNM*PBPSNM +
     $                PBPSNM*(ANORM*XNORM/DNORM + 2.0E0*CNDBA*CNDBA +
     $                ONE) + CNDBA*PBPSNM )
      END IF
\end{verbatim}
}

\noindent
For example, if ${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$,
$$  A = \left( \begin{tabular}{rrrr}
                  1 &  2 &  1 & 4 \\
                  1 &  3 &  2 & 1 \\
                 -1 & -2 & -1 & 1 \\
                 -1 &  2 & -1 & 5 \\
                  1 &  0 &  0 & 1
               \end{tabular} \right), \quad
    B = \left( \begin{tabular}{rrr}
                  1 &  2 &  2 \\
                 -1 &  1 & -2\\
                  3 &  1 &  6 \\
                  1 & -1 &  2 \\
                  2 & -2 &  4
               \end{tabular} \right)
    \quad \mbox{and} \quad
    d = \left( \begin{array}{r}
                   1 \\
                   1 \\
                   1 \\
                   1 \\
                   1
               \end{array} \right). $$
Then (to 7 decimal places)
$$  \ha{x} = \left( \begin{array}{r}
                            -0.5466667  \\
                             0.3200001  \\
                             0.7200000  \\
                            -0.0533334
                        \end{array} \right)
    \quad \mbox{and} \quad
    \ha{y} = \left( \begin{array}{r}
                         0.1333334 \\
                        -0.1333334 \\
                         0.2666667
                    \end{array} \right).$$
The computed error bounds
${\tt XERRBD} = 1.2\cdot10^{-5}$ and ${\tt YERRBD} = 6.4\cdot10^{-7}$,
where ${\tt CNDAB} = 26.97$, ${\tt CNDBA} = 1.78$,
The true errors in $x$ and $y$ are $1.2\cdot10^{-7}$ and
$1.3\cdot10^{-7}$, respectively.  Note that the exact solutions are
$x = \frac{1}{75}[-41,24,54,-4]^T$ and
$y = \frac{1}{15}[2,-2,4]^T$.

\subsubsection{Further Details:  Error Bounds for General Linear Model Problems}
\label{sec_lseglm_glmdetails}

In this subsection, we will summarize the available error bounds.
The reader may also refer to \cite{lawn31,elden,paige79b}
for further details.  \\

\noindent
Let $\ha{x}$ and $\ha{y}$ be the solutions
by the driver routine xGGGLM\indexR{SGGGLM}\indexR{CGGGLM} (see subsection
\ref{sec_lseglm_drivers}). Then $\ha{x}$ is normwise
backward stable and $\ha{y}$ is stable
in a mixed forward/backward sense. Specifically,
we have $\ha{x}$ and $\ha{y} = \ba{y} + \Delta \ba{y}$,
where $\ha{x}$ and $\ba{y}$ solve
  $\min\{\| y \|_2: \; (A + \Delta A)x + (B + \Delta B)y= d + \Delta d \} $,
and
\begin{center}
\parbox{2in}{
  \begin{eqnarray*}
  \| \Delta \ba{y} \|_2 & \leq & q(m,n,p)\epsilon\|\ba{y}\|_2, \\
  \| \Delta A \|_F & \leq & q(m,n,p)\epsilon\|A\|_F,
  \end{eqnarray*} }\quad
\parbox{2in}{
  \begin{eqnarray*}
  \| \Delta d \|_2 & \leq & q(m,n,p)\epsilon\|d\|_2, \\
  \| \Delta B \|_F & \leq & q(m,n,p)\epsilon\|B\|_F,
  \end{eqnarray*}
}
\end{center}
and $q(m,n,p)$ is a modestly growing function of $m$, $n$, and $p$.
We take $q(m,n,p) = 1$ in the code fragment above.
Let $X^{\dagger}$ denote the Moore-Penrose pseudo-inverse of $X$.
Let $\kappa_B(A) = \nrmfro{A}\nrmtwo{(A^\dagger_B)}$( = {\tt CNDAB} above) and
    $\kappa_A(B) = \nrmfro{B}\nrmtwo{(GB)^\dagger}$( = {\tt CNDBA} above)
where $G = I - AA^\dagger$ and $A^\dagger_B = A^\dagger[I-B(GB)^\dagger]$.
When $q(m,n,p) \epsilon$ is small, the errors $x-\ha{x}$ and $y - \ha{y}$
are bounded by
\begin{eqnarray*}
 \frac{\nrmtwo{x-\ha{x}}}{\nrmtwo{x}} & \leq & q(m,n,p)\epsilon
      \Bigg[ \kappa_B(A)\left( 1 + \frac{\nrmtwo{d}}{\nrmfro{A}\nrmtwo{x}}
         + 2\kappa^2_A(B)
              \frac{\nrmtwo{d}}{\nrmfro{A}\nrmtwo{x}} \right)  \\
    &   & {}  + \nrmtwo{A^\dagger_BB}^2
             \nrmtwo{(GB)^\dagger}^2\nrmfro{A}\frac{\nrmtwo{d}}{\nrmtwo{x}}
       \Bigg],  \\
\mbox{and} \hspace{1in} {} & & \\
  \frac{\nrmtwo{y - \ha{y}}}{\nrmtwo{d}}
    & \leq & q(m,n,p)\epsilon
         \bigg[ \nrmtwo{A^\dagger_BB}
                    \nrmfro{A}\nrmtwo{(GB)^\dagger}^2 \\
     & & {} + \nrmtwo{(GB)^\dagger}
             \left(\nrmfro{A}\frac{\nrmtwo{x}}{\nrmtwo{d}}
             + 2\kappa^2_A(B) + 1\right)
             + \nrmfro{B}\nrmtwo{(GB)^\dagger}^2 \bigg] .
\end{eqnarray*}


\vspace*{.2in}

\noindent
When $B = I$, the GLM problem is the standard LS problem.
$y$ is the residual vector $y = d - Ax$, and we have
$$
 \frac{\nrmtwo{x-\ha{x}}}{\nrmtwo{x}} \leq q(m,n)\epsilon
      \kappa(A)\Bigg( 1 + \frac{\nrmtwo{d}}{\nrmfro{A}\nrmtwo{x}}
         + \kappa(A)\frac{\nrmtwo{y}}{\nrmfro{A}\nrmtwo{x}} \Bigg),
$$
and
$$
  \frac{\nrmtwo{y - \ha{y}}}{\nrmtwo{y}} \leq q(m,n)\epsilon
         \kappa(A)\frac{\nrmtwo{y}}{\nrmtwo{d}} .
$$
where $\kappa(A) = \kappa_B(A) = \nrmfro{A}\nrmtwo{A^\dagger}$ and
$\kappa_A(B) = 1$.  The error bound of $x-\ha{x}$ is the same
as in the LSE problem (see section~\ref{sec_lseglm_lsedetails}),
which is essentially the same as given in section~\ref{secbackgroundlsq}.
The bound on the error in $\ha{y}$ is the same as that provided
in \cite[section 5.3.7]{GVL2}. \\

\section{Error Bounds for the Symmetric Eigenproblem }\label{secsym}

The eigendecomposition\index{eigendecomposition!symmetric} of
an $n$-by-$n$ real symmetric matrix is the
factorization $A=Z \Lambda Z^T$ ($A= Z \Lambda Z^H$ in the complex Hermitian
case), where $Z$ is orthogonal (unitary) and
$\Lambda = \diag ( \lambda_1 , \ldots , \lambda_n )$ is real and diagonal,
with $\lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$.
The $\lambda_i$ are the {\bf eigenvalues}\index{eigenvalue} of $A$ and the columns $z_i$ of
$Z$ are the {\bf eigenvectors}\index{eigenvector}. This is also often written
$A z_i = \lambda_i z_i$. The eigendecomposition of a symmetric matrix is
computed
by the driver routines
xSYEV, xSYEVX, xSYEVD, xSYEVR,
xSPEV, xSPEVX, xSPEVD,
xSBEV, xSBEVX, xSBEVD,
xSTEV, xSTEVX, xSTEVD and xSTEVR.
The complex counterparts of these routines, which compute the
eigendecomposition of complex Hermitian matrices, are
the driver routines
xHEEV, xHEEVX, xHEEVD, xHEEVR,
xHPEV, xHPEVX, xHPEVD,
xHBEV, xHBEVX and xHBEVD
(see subsection~\ref{subsecdriveeigSEP}).
\indexR{SSYEV}
\indexR{SSYEVX}
\indexR{SSYEVD}
\indexR{SSYEVR}
\indexR{SSPEV}
\indexR{SSPEVX}
\indexR{SSPEVD}
\indexR{SSBEV}
\indexR{SSBEVX}
\indexR{SSBEVD}
\indexR{SSTEV}
\indexR{SSTEVX}
\indexR{SSTEVD}
\indexR{SSTEVR}
\indexR{CHEEV}
\indexR{CHEEVX}
\indexR{CHEEVD}
\indexR{CHEEVR}
\indexR{CHPEV}
\indexR{CHPEVX}
\indexR{CHPEVD}
\indexR{CHBEV}
\indexR{CHBEVX}
\indexR{CHBEVD}

The approximate error bounds
\index{error bounds!symmetric eigenproblem}
\index{eigenvalue!error bound}
\index{eigenvector!error bound}
for the computed eigenvalues
$\hat{\lambda}_1 \leq \cdots \leq \hat{\lambda}_n$ are
\[
| \hat{\lambda}_i - \lambda_i | \leq {\tt EERRBD} \; \; .
\]
The approximate error bounds for the computed eigenvectors
$\hat{z}_i$, which bound the acute angles
between the computed eigenvectors and true
eigenvectors $z_i$, are:
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
\[
\theta ( \hat{z}_i , z_i ) \leq {\tt ZERRBD} (i) \; .
\]
These bounds can be computed by the following code fragment:

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Compute eigenvalues and eigenvectors of A
*     The eigenvalues are returned in W
*     The eigenvector matrix Z overwrites A
      CALL SSYEV( 'V', UPLO, N, A, LDA, W, WORK, LWORK, INFO )
      IF( INFO.GT.0 ) THEN
         PRINT *,'SSYEV did not converge'
      ELSE IF ( N.GT.0 ) THEN
*        Compute the norm of A
         ANORM = MAX( ABS( W(1) ), ABS( W(N) ) )
         EERRBD = EPSMCH * ANORM
*        Compute reciprocal condition numbers for eigenvectors
         CALL SDISNA( 'Eigenvectors', N, N, W, RCONDZ, INFO )
         DO 10 I = 1, N
            ZERRBD( I ) = EPSMCH * ( ANORM / RCONDZ( I ) )
10       CONTINUE
      ENDIF
\end{verbatim}

For example,
if ${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$ and
\[
A = \bmat{ccc} 1 & 2 & 3 \\ 2 & 4 & 5 \\ 3 & 5 & 6 \emat \; ,
\]
then the eigenvalues, approximate error bounds, and true errors are

\begin{center}
\begin{tabular}{||c|c||c|c||c|c||}
\hline
i & $\hat{\lambda}_i$ & {\tt EERRBD} & true $| \hat{\lambda}_i - \lambda_i |$ &
{\tt ZERRBD}(i) & true $\theta ( \hat{z}_i , z_i )$ \\ \hline
1 & $-.5157$ & $6.7 \cdot 10^{-7}$ & $1.6 \cdot 10^{-7}$ &
$9.8 \cdot 10^{-7}$ & $1.2 \cdot 10^{-7}$ \\
2 & $.1709$  & $6.7 \cdot 10^{-7}$ & $3.2 \cdot 10^{-7}$ &
$9.8 \cdot 10^{-7}$ & $7.0 \cdot 10^{-8}$ \\
3 & $11.34$  & $6.7 \cdot 10^{-7}$ & $2.8 \cdot 10^{-6}$ &
$6.1 \cdot 10^{-8}$ & $9.7 \cdot 10^{-8}$ \\ \hline
\end{tabular}
\end{center}

\subsection{Further Details:  Error Bounds for the Symmetric Eigenproblem }\label{secDetailsym}

The usual error analysis of the
symmetric\index{error bounds!symmetric eigenproblem}
eigenproblem (using any LAPACK
routine in subsection~\ref{subsecdriveeigSEP}) is as follows \cite{parlett}:

\begin{quote}
The computed eigendecomposition $\hat{Z} \hat{\Lambda} \hat{Z}^T$ is nearly
the exact
eigendecomposition of $A+E$, i.e.,
$A+E = (\hat{Z}+ \delta \hat{Z}) \hat{\Lambda} (\hat{Z}+ \delta \hat{Z})^T$
is a true eigendecomposition so that $\hat{Z}+ \delta \hat{Z}$ is
orthogonal,
where $\|E\|_2 / \|A\|_2 \leq p(n) \epsilon$ and
$\| \delta \hat{Z} \|_2 \leq p(n) \epsilon$. Here $p(n)$ is a modestly growing
function of $n$. We take $p(n)=1$ in the above code fragment.
Each computed eigenvalue $\lambdahat_i$ differs from a
true $\lambda_i$ by at most
\[
| \lambdahat_i - \lambda_i |  \leq p(n) \cdot \epsilon \cdot \|A\|_2
= {\tt EERRBD} \; .
\]
Thus large eigenvalues (those near $\max_i | \lambda_i | = \|A\|_2$)
are computed to high relative accuracy \index{accuracy!high} and small ones may not be.
\index{eigenvalue!error bound}

There are two questions to ask about the computed eigenvectors:
``Are they orthogonal?'' and ``How much do they differ from the
true eigenvectors?''
The answer to the first question is that, except for eigenvectors
computed by computational routine xSTEIN\indexR{SSTEIN}\indexR{CSTEIN}
(which is called by drivers
with names ending in -EVX when only a subset of the eigenvalues and
eigenvectors are requested),
the computed eigenvectors
are always nearly orthogonal to working precision, independent of
how much they differ from the true eigenvectors. In other words
\[
|\hat{z}_i^T \hat{z}_j | = O( \epsilon )
\]
for $i \neq j$.
xSTEIN almost
always returns orthogonal eigenvectors, but can occasionally fail when
eigenvalues are tightly clustered.

Here is the answer to the second question about eigenvectors.
The angular difference between the computed unit eigenvector
$\zhat_i$ and a true unit eigenvector $z_i$ satisfies the approximate bound
\index{eigenvector!error bound}
\[
\theta ( \zhat_i , z_i ) \leapproxeq \frac{p(n) \epsilon \|A\|_2}{\gap_i}
= {\tt ZERRBD}(i)
\]
if $p(n) \epsilon$ is small enough.
Here $\gap_i = \min_{j \neq i} | \lambda_i - \lambda_j |$ is the
{\bf absolute gap}\index{absolute gap}\index{gap}
between $\lambda_i$ and the nearest other eigenvalue.  Thus, if $\lambda_i$
is close to other eigenvalues, its corresponding eigenvector $z_i$
may be inaccurate.
The gaps may be easily computed from the array of computed eigenvalues
using subroutine SDISNA\indexR{SDISNA}.
The gaps computed by SDISNA are ensured not to be so small as
to cause overflow when used as divisors.
\index{overflow}
\index{floating-point arithmetic!overflow}

Let $\hatcalS$ be the invariant subspace spanned by a collection of eigenvectors
$\{\hat{z}_i \, , \, i \in {\cal I}\}$, where $\cal I$ is a subset of the
integers from 1 to $n$. Let $\calS$ be the corresponding true subspace. Then
\[
\theta ( \hatcalS , \calS ) \leapproxeq \frac{p(n) \epsilon \|A\|_2}
{\gap_{\cal I}}
\]
\index{invariant subspaces!error bound}
where
\[
\gap_{\cal I} = \min_{i \in {\cal I} \atop j \not\in {\cal I}}
| \lambda_i - \lambda_j |
\]
is the absolute gap between the eigenvalues in $\cal I$ and the nearest
other eigenvalue. Thus, a cluster\index{cluster!eigenvalues} of
close eigenvalues which is
far away from any other eigenvalue may have a well determined
invariant subspace $\hatcalS$ even if its individual eigenvectors are
ill-conditioned. (These bounds are special
cases of those in section~\ref{secnonsym}.)
\end{quote}

In the special case of a real symmetric tridiagonal matrix $T$, the eigenvalues
and eigenvectors can be computed much more accurately. xSYEV (and the other
symmetric eigenproblem drivers) computes the eigenvalues and eigenvectors of
a dense symmetric matrix by first reducing it to tridiagonal form\index{tridiagonal form} $T$, and then
finding the eigenvalues and eigenvectors of $T$.
Reduction of a dense matrix to tridiagonal form\index{tridiagonal form} $T$ can introduce
additional errors, so the following bounds for the tridiagonal case do not
apply to the dense case.

\begin{quote}
The eigenvalues of $T$ may be computed with small componentwise relative
backward error
\index{backward error}
\index{error!backward}
($O( \epsilon )$) by using subroutine xSTEBZ (subsection
\indexR{SSTEBZ}
\ref{subseccompsep})
\indexR{SSTEVX}
or driver xSTEVX (subsection \ref{subsecdriveeigSEP}). If $T$ is also positive definite,
they may also be computed at least as accurately by
xPTEQR\indexR{SPTEQR}\indexR{CPTEQR}
or
xSTEMR\indexR{SSTEMR}\indexR{CSTEMR}
(subsection \ref{subseccompsep}).
To compute error bounds
for the computed eigenvalues $\lambdahat_i$ we must make some assumptions
about $T$. The bounds discussed here are from \cite{barlowdemmel}.
Suppose $T$ is positive definite, and
write $T=DHD$ where $D = \diag( t_{11}^{1/2} , \ldots , t_{nn}^{1/2})$
and $h_{ii}= 1$.
Then the computed eigenvalues
$\lambdahat_i$ can differ from true eigenvalues $\lambda_i$ by
\[
| \lambdahat_i - \lambda_i | \leq p(n) \cdot \epsilon \cdot \kappa_2(H)
\cdot \lambda_i
\]
where $p(n)$ is a modestly growing function of $n$.
Thus if $\kappa_2 (H)$ is moderate, each eigenvalue will be computed
to high relative accuracy, \index{accuracy!high} no matter how tiny it is.
The eigenvectors $\zhat_i$ computed by xPTEQR
can differ from true eigenvectors $z_i$ by
at most about
\[
\theta ( \zhat_i , z_i ) \leapproxeq \frac{p(n) \cdot \epsilon \cdot \kappa_2 (H)}
{\relgap_i}
\]
if $p(n) \epsilon$ is small enough, where
$\relgap_i = \min_{j \neq i} | \lambda_i - \lambda_j |/ ( \lambda_i + \lambda_j )$
is the {\bf relative gap} between $\lambda_i$ and the nearest other eigenvalue.
\index{relative gap}\index{gap}
Since the relative gap may be much larger than the absolute gap, this
error bound may be much smaller than the previous one.
Independent of the difference between the computed and true eigenvectors,
the computed eigenvectors are orthogonal to nearly full precision,
i.e. $| \hat{z}_i^T \hat{z}_j | = O( \epsilon )$ for $i \neq j$.

$\kappa_2 (H)$ could be computed by applying
xPTCON\indexR{SPTCON}\indexR{CPTCON}
(subsection \ref{subseccomplineq}) to $H$.
The relative gaps are easily computed from the
array of computed eigenvalues.

\end{quote}

\section{Error Bounds for the Nonsymmetric Eigenproblem }\label{secnonsym}

The nonsymmetric eigenvalue
problem\index{error bounds!nonsymmetric eigenproblem} is more
complicated than the
symmetric eigenvalue problem. In this subsection,
we state the simplest bounds and leave the more complicated ones to
subsequent subsections.

Let $A$ be an $n$-by-$n$ nonsymmetric matrix, with eigenvalues
$\lambda_1, \ldots , \lambda_n$. Let $v_i$ be a right eigenvector
corresponding to $\lambda_i$: $A v_i = \lambda_i v_i$.
Let $\hat{\lambda}_i$ and $\hat{v}_i$ be the corresponding
computed eigenvalues and eigenvectors, computed by expert driver routine
xGEEVX (see subsection~\ref{subsecdriveeigNEP}).
\indexR{SGEEVX}\indexR{CGEEVX}

The approximate error bounds for the computed eigenvalues are
\[
| \hat{\lambda}_i - \lambda_i | \leq {\tt EERRBD}(i) \; \; .
\]
The approximate error
bounds\index{eigenvector!error bound}\index{eigenvalue!error bound}
for the computed eigenvectors $\hat{v}_i$,
which bound the acute angles between the computed eigenvectors and true
eigenvectors $v_i$, are
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
\[
\theta ( \hat{v}_i , v_i ) \leq {\tt VERRBD}(i) \; .
\]
These bounds can be computed by the following code fragment:

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Compute the eigenvalues and eigenvectors of A
*     WR contains the real parts of the eigenvalues
*     WI contains the real parts of the eigenvalues
*     VL contains the left eigenvectors
*     VR contains the right eigenvectors
      CALL SGEEVX( 'P', 'V', 'V', 'B', N, A, LDA, WR, WI,
     $             VL, LDVL, VR, LDVR, ILO, IHI, SCALE, ABNRM,
     $             RCONDE, RCONDV, WORK, LWORK, IWORK, INFO )
      IF( INFO.GT.0 ) THEN
         PRINT *,'SGEEVX did not converge'
      ELSE IF ( N.GT.0 ) THEN
         DO 10 I = 1, N
            EERRBD(I) = EPSMCH*ABNRM/RCONDE(I)
            VERRBD(I) = EPSMCH*ABNRM/RCONDV(I)
10       CONTINUE
      ENDIF
\end{verbatim}

For example, if
${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$ and
\[
A = \bmat{ccc} 58 & 9 & 2 \\ 186 & 383 & 96 \\ -912 & -1551 & -388 \emat
\]
then true eigenvalues, approximate eigenvalues, approximate error bounds,
and true errors are
\begin{center}
\begin{tabular}{||c|c|c||c|c||c|c||}
\hline
$i$ & $\lambda_i$ & $\hat{\lambda}_i$ &
{\tt EERRBD}$(i)$ & true $| \hat{\lambda}_i - \lambda_i |$ &
{\tt VERRBD}$(i)$ & true $\theta ( \hat{v}_i , v_i )$ \\ \hline
1 & $50$ & $50.00$ & $8.5 \cdot 10^{-4}$ & $2.7 \cdot 10^{-4}$ &
$2.6 \cdot 10^{-5}$ & $8.5 \cdot 10^{-6}$ \\
2 & $2$ & $1.899$ & $3.1 \cdot 10^{-1}$ & $1.0 \cdot 10^{-1}$ &
$1.9 \cdot 10^{-4}$ & $7.7 \cdot 10^{-5}$ \\
3 & $1$ & $1.101$ & $3.1 \cdot 10^{-1}$ & $1.0 \cdot 10^{-1}$ &
$1.8 \cdot 10^{-4}$ & $7.5 \cdot 10^{-5}$ \\ \hline
\end{tabular}
\end{center}

\subsection{Further Details:  Error Bounds for the Nonsymmetric Eigenproblem}
\subsubsection{Overview}\label{secnepsummary}

In this subsection, we will summarize all the available error bounds.
Later subsections will provide further details. The reader may also
refer to \cite{baidemmelmckenney,stewartsun90}.

Bounds for individual eigenvalues and eigenvectors are provided by
driver xGEEVX (subsection~\ref{subsecdriveeigNEP}) or computational
routine xTRSNA (subsection~\ref{subseccompnep}).
\indexR{SGEEVX}\indexR{CGEEVX}
\indexR{STRSNA}\indexR{CTRSNA}
Bounds for
clusters\index{error bounds!clustered eigenvalues} of eigenvalues
and their associated invariant subspace are
provided by driver xGEESX (subsection~\ref{subsecdriveeigNEP}) or
\indexR{SGEESX}\indexR{CGEESX}
\indexR{STRSEN}\indexR{CTRSEN}
computational routine xTRSEN (subsection~\ref{subseccompnep}).
\index{error bounds!nonsymmetric eigenproblem}

\newcommand{\calSI}{{\cal S}_{\cal I}}
\newcommand{\hatcalSI}{{\hat{\cal S}}_{\cal I}}
Let $\hat{\lambda}_i$ be the $i^{th}$ computed eigenvalue and
$\lambda_i$ an $i^{th}$ true eigenvalue. Although such a one-to-one
correspondence between computed and true eigenvalues exists, it is not as
simple to describe as in the symmetric case. In the symmetric case the eigenvalues
are real and simply sorting provides the one-to-one correspondence,
so that $\hat{\lambda}_1 \leq \cdots \leq \hat{\lambda}_n$
and $\lambda_1 \leq \cdots \leq \lambda_n$. With nonsymmetric matrices
$\hat{\lambda}_i$ is usually just the computed eigenvalue closest to
$\lambda_i$, but in very ill-conditioned problems this is not always true.
In the most general case, the one-to-one correspondence may be described
in the following nonconstructive way. Let $\lambda_i$ be the eigenvalues
of $A$ and $\hat{\lambda}_i$ be the eigenvalues of $A+E$. Let
$\lambda_i(t)$ be the eigenvalues of $A+tE$, where $t$ is a parameter,
which is initially zero, so that we may set $\lambda_i (0) = \lambda_i$.
As $t$ increase from 0 to 1, $\lambda_i (t)$ traces out a curve from
$\lambda_i$ to $\hat{\lambda}_i$, providing the correspondence.
Care must be taken when the curves intersect, and the correspondence
may not be unique.

Let $\hat{v}_i$ be the
corresponding computed right eigenvector, and $v_i$ a true right
eigenvector (so $Av_i = \lambda_i v_i$).
If $\cal I$ is a subset of the
integers from 1 to $n$, we let $\lambda_{\cal I}$ denote the average of
the selected eigenvalues:
$\lambda_{\cal I} = (\sum_{i \in \cal I} \lambda_i)/(\sum_{i \in \cal I} 1)$,
and similarly for $\hat{\lambda}_{\cal I}$. We also let $\calSI$
denote the subspace spanned by $\{ v_i \, , \, i \in \cal I \}$; it is
called a right invariant subspace because if $v$ is any vector in $\calSI$ then
$Av$ is also in $\calSI$. $\hatcalSI$ is the corresponding computed subspace.

The algorithms for the nonsymmetric eigenproblem are normwise backward stable:
\index{backward stability}
\index{stability!backward}
they compute the exact eigenvalues, eigenvectors and invariant subspaces
of slightly perturbed matrices $A+E$, where $\|E\| \leq p(n) \epsilon \|A\|$.
Some of the bounds are stated in terms of $\|E\|_2$ and others in
terms of $\|E\|_F$; one may use $p(n) \epsilon \|A\|_1$ to approximate
either quantity.
The code fragment in the previous subsection approximates
$\|E\|$ by $\epsilon \cdot {\tt ABNRM}$, where ${\tt ABNRM} = \|A\|_1$
is returned by xGEEVX.

xGEEVX (or xTRSNA) returns two quantities for each
$\hat{\lambda_i}$, $\hat{v}_i$ pair: $s_i$ and $\sep_i$.
xGEESX (or xTRSEN) returns two quantities for a selected subset
$\cal I$ of eigenvalues: $s_{\cal I}$ and $\sep_{\cal I}$.
$s_i$ (or $s_{\cal I}$) is a reciprocal condition number for the
computed eigenvalue $\hat{\lambda}_i$ (or $\hat{\lambda}_{\cal I}$),
and is referred to as {\tt RCONDE} by xGEEVX (or xGEESX).
\index{condition number}
\index{arguments!RCONDE}
$\sep_i$ (or $\sep_{\cal I}$) is a reciprocal condition number for
the right eigenvector $\hat{v}_i$ (or ${\cal S}_{\cal I}$), and
is referred to as {\tt RCONDV} by xGEEVX (or xGEESX).
\index{arguments!RCONDV}
The approximate error bounds for eigenvalues, averages of eigenvalues,
eigenvectors, and invariant subspaces
provided in Table~\ref{tabasympnepbounds} are
true for sufficiently small $\|E\|$, which is why they are called asymptotic.

\begin{table}[h]
\caption{Asymptotic error bounds for the nonsymmetric eigenproblem}
\label{tabasympnepbounds}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Simple eigenvalue &
$|\hat{\lambda}_i - \lambda_i | \leapproxeq \|E\|_2 / s_i$ \\ \hline
Eigenvalue cluster &
$|\hat{\lambda}_{\cal I} - \lambda_{\cal I} | \leapproxeq \|E\|_2 / s_{\cal I}$ \\ \hline
Eigenvector &
$\theta ( \hat{v}_i , v_i ) \leapproxeq \|E\|_F / \sep_i$ \\ \hline
Invariant subspace &
$\theta ( \hatcalSI , \calSI ) \leapproxeq \|E\|_F / \sep_{\cal I}$ \\ \hline
\end{tabular}
\end{center}
\end{table}
\index{cluster!eigenvalues!error bound}
\index{eigenvector!error bound}
\index{eigenvalue!error bound}
\index{invariant subspaces!error bound}

If the problem is ill-conditioned, the asymptotic bounds may only hold
for extremely small $\|E\|$. Therefore, in Table~\ref{tabglobalnepbounds}
we also provide global bounds
which are guaranteed to hold for all $\|E\|_F < s \cdot \sep / 4$.

\begin{table}[h]
\caption{Global error bounds for the nonsymmetric eigenproblem
assuming $\|E\|_F < s_i \cdot \sep_i / 4$}
\label{tabglobalnepbounds}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Eigenvalue cluster &
$|\hat{\lambda}_{\cal I} - \lambda_{\cal I} | \leq 2 \|E\|_2 / s_{\cal I}$
\\ \hline
Eigenvector &
$\theta ( \hat{v}_i , v_i ) \leq \arctan (2 \|E\|_F/(\sep_i - 4 \|E\|_F/s_i))$
\\ \hline
Invariant subspace &
$\theta ( \hatcalSI , \calSI ) \leq \arctan (2 \|E\|_F/(\sep_{\cal I} - 4 \|E\|_F/s_{\cal I}))$
\\ \hline
\end{tabular}
\end{center}
\end{table}

We also have the following bound, which is true for all $E$:
all the $\lambda_i$ lie in the union of $n$ disks,
where the $i$-th disk is centered at $\hat{\lambda}_i$ and has
radius $n \|E\|_2 / s_i$. If $k$ of these disks overlap,
so that any two points inside the $k$ disks can be connected
by a continuous curve lying entirely inside the $k$ disks,
and if no larger set of $k+1$ disks has this property,
then exactly $k$ of the $\lambda_i$ lie inside the
union of these $k$ disks. 

Finally, the quantities $s$ and $\sep$ tell use how we can best
(block) diagonalize a matrix $A$ by a similarity,
$V^{-1}AV = \diag (A_{11} , \ldots , A_{bb})$, where each diagonal block
$A_{ii}$ has a selected subset of the eigenvalues of $A$. Such a decomposition
may be useful in computing functions of matrices, for example.
The goal is to choose a $V$ with a nearly minimum condition number
$\kappa_2 (V)$
\index{condition number}
which performs this decomposition, since this generally minimizes the error
in the decomposition.
This may be done as follows. Let $A_{ii}$ be
$n_i$-by-$n_i$. Then columns $1+\sum_{j=1}^{i-1} n_j$ through
$\sum_{j=1}^{i} n_j$ of $V$ span the invariant
subspace\index{invariant subspaces} of $A$ corresponding
to the eigenvalues of $A_{ii}$; these columns should be chosen to be any
orthonormal basis of this space (as computed by xGEESX, for example).
Let $s_{{\cal I}_i}$ be the value corresponding to the
cluster of
eigenvalues of $A_{ii}$, as computed by xGEESX or xTRSEN. Then
$\kappa_2 (V) \leq b/ \min_i (s_{{\cal I}_i})$, and no other choice of $V$ can make
its condition number smaller than $1/ \min_i (s_{{\cal I}_i})$ \cite{demmel83}.
Thus choosing orthonormal
subblocks of $V$ gets $\kappa_2 (V)$ to within a factor $b$ of its minimum
value.

In the case of a real symmetric (or complex Hermitian) matrix,
$s=1$ and $\sep$ is the absolute gap, as defined in subsection~\ref{secsym}.
The bounds in Table~\ref{tabasympnepbounds} then reduce to the
bounds in subsection~\ref{secsym}.

\subsubsection{Balancing and Conditioning}\label{secbalance}

There are two preprocessing
steps\index{balancing of eigenproblems} one may perform
on a matrix $A$ in order
to make its eigenproblem easier. The first is {\bf permutation}, or
reordering the rows and columns to make $A$ more nearly upper triangular
(closer to Schur form): $A' = PAP^T$, where $P$ is a permutation matrix.
If $A'$ is permutable to upper triangular form (or close to it), then
no floating-point operations (or very few) are needed to reduce it to
Schur form.
The second is {\bf scaling}\index{scaling} by a diagonal matrix $D$ to make the rows and
columns of $A'$ more nearly equal in norm: $A''= DA'D^{-1}$. Scaling
can make the matrix norm smaller with respect to the eigenvalues, and so
possibly reduce the inaccuracy contributed by roundoff
\cite[Chap. II/11]{wilkinson3}. We refer to these two operations as
{\bf balancing}.

Balancing is performed by driver xGEEVX, which calls
computational routine xGEBAL. The user may tell xGEEVX to optionally
\indexR{SGEBAL}\indexR{CGEBAL}
permute, scale, do both, or do neither; this is specified by input
parameter {\tt BALANC}. Permuting has no effect on
\index{arguments!BALANC}
the condition numbers
\index{condition number}
or their interpretation as described in previous
subsections. Scaling, however, does change their interpretation,
as we now describe.

The output parameters of xGEEVX --- {\tt SCALE} (real array of length N),
\index{arguments!SCALE}
\index{arguments!ILO and IHI}
\index{arguments!ABNRM}
{\tt ILO} (integer), {\tt IHI} (integer) and {\tt ABNRM} (real) --- describe
the result of
balancing a matrix $A$ into $A''$, where N is the dimension of $A$.
The matrix $A''$ is block upper triangular, with at most three blocks:
from $1$ to {\tt ILO}$-1$, from {\tt ILO} to {\tt IHI}, and from {\tt IHI}$+1$ to N.
The first and last blocks are upper triangular, and so already in Schur
form. These are not scaled; only the block from {\tt ILO} to {\tt IHI} is scaled.
Details of the scaling and permutation are described in {\tt SCALE} (see the
specification of xGEEVX or xGEBAL for details)\index{scaling}. The 1-norm of
$A''$ is returned in {\tt ABNRM}.

The condition numbers
\index{condition number}
described in earlier subsections are computed for
the balanced matrix $A''$, and so some interpretation is needed to
apply them to the eigenvalues and eigenvectors of the original matrix $A$.
To use the bounds for eigenvalues in Tables \ref{tabasympnepbounds} and
\ref{tabglobalnepbounds},
we must replace $\|E\|_2$ and $\|E\|_F$
by $O(\epsilon) \|A''\| = O(\epsilon) \cdot {\tt ABNRM}$. To use the
bounds for eigenvectors, we also need to take into account that bounds
on rotations of eigenvectors are for the eigenvectors $x''$ of
$A''$, which are related to the eigenvectors $x$ of $A$ by
$DPx=x''$, or $x=P^T D^{-1}x''$. One coarse but simple way to do this is
as follows: let $\theta''$ be the bound on rotations of $x''$ from
Table~\ref{tabasympnepbounds} or Table~\ref{tabglobalnepbounds}
and let $\theta$ be the desired bound on rotation of $x$. Let
\[
\kappa (D) =
\frac{{\rule[-.25cm]{0cm}{.5cm} \max_{{\tt ILO} \leq i \leq {\tt IHI}}
{\tt SCALE}(i)}}
{\min_{{\tt ILO} \leq i \leq {\tt IHI}} {\tt SCALE}(i)}
\]
be the condition number of $D$.
\index{condition number}
Then
\[
\sin \theta \leq \kappa(D) \cdot \sin \theta'' \; \; .
\]
\index{eigenvector!error bound}
\index{eigenvalue!error bound}

The numerical example in subsection~\ref{secnonsym} does no scaling,
just permutation.

\subsubsection{Computing $s$ and $\sep$}\label{secspec}

To explain $s$ and $\sep$\index{s and sep}, we need to
introduce\index{spectral projector}
the {\bf spectral projector} $P$ \cite{stewart73,kato}, and the
{\bf separation of two matrices}\index{separation of matrices}
$A$ and $B$, $\sep (A,B)$ \cite{stewart73,varah}.

We may assume the matrix $A$ is in Schur form, because reducing it
to this form does not change the values of $s$ and $\sep$.
Consider a cluster of $m \geq 1$ eigenvalues, counting multiplicities.
Further assume the $n$-by-$n$ matrix $A$ is
\bq\label{eq2.1}
A  = \bmat{cc} A_{11} & A_{12} \\ 0 & A_{22} \emat
\eq
where the eigenvalues of the $m$-by-$m$ matrix
$A_{11}$ are exactly those in which we are
interested. In practice, if the eigenvalues on the diagonal of $A$
are in the wrong order, routine xTREXC
\indexR{STREXC}\indexR{CTREXC}
can be used to put the desired ones in the upper left corner
as shown.

We define the {\bf spectral projector}, or simply projector $P$ belonging
to the eigenvalues of $A_{11}$ as
\bq\label{eq2.2}
P = \bmat{cc} I_m & R \\ 0 & 0 \emat
\eq
where $R$ satisfies the system of linear equations
\bq\label{eq2.3}
A_{11}R - RA_{22} = A_{12}.
\eq
Equation (\ref{eq2.3}) is called a Sylvester equation\index{Sylvester
equation}.
Given the Schur form (\ref{eq2.1}), we solve equation
(\ref{eq2.3}) for $R$ using the subroutine xTRSYL.
\indexR{STRSYL}\indexR{CTRSYL}

We can now define $s$ for the eigenvalues of $A_{11}$:
\bq
s = \frac{1}{\|P\|_2} = \frac{1}{\sqrt{1+\|R\|_2^2}}.
\eq
In practice we do not use this expression since $\|R\|_2$ is hard to
compute. Instead we use the more easily computed underestimate
\bq
\frac{1}{\sqrt{1+\|R\|_F^2}}
\eq
which can underestimate the true value of $s$ by no more than a factor
$\sqrt { \min ( m,n-m ) }$.
This underestimation makes our error bounds more conservative.
This approximation of $s$ is called {\tt RCONDE} in xGEEVX and xGEESX.
\index{arguments!RCONDE}

The {\bf separation} $\sep (A_{11},A_{22})$ of the matrices $A_{11}$ and
$A_{22}$ is defined as the smallest singular value of the linear
map in (\ref{eq2.3}) which takes $X$ to $A_{11}X - XA_{22}$, i.e.,
\bq\label{eq2.4}
\sep (A_{11},A_{22}) = \min_{X \neq 0} \frac{\|A_{11}X - XA_{22}\|_F}
{\| X \|_F}.
\eq
This formulation lets us estimate $\sep (A_{11},A_{22})$
using the condition estimator
\index{condition number!estimate}
xLACN2 \cite{hager84,higham1,nick2}, which estimates the norm of
a linear operator $\| T \|_1$ given the ability to compute $Tx$ and
$T^Tx$ quickly for arbitrary $x$.
In our case, multiplying an
arbitrary vector by $T$
means solving the Sylvester equation (\ref{eq2.3})\index{Sylvester
equation}
with an arbitrary right hand side using xTRSYL, and multiplying by
$T^T$ means solving the same equation with $A_{11}$ replaced by
$A_{11}^T$ and $A_{22}$ replaced by $A_{22}^T$. Solving either equation
costs at most $O(n^3)$ operations, or as few as $O(n^2)$ if $m \ll n$.
Since the true value of $\sep$ is $\|T\|_2$ but we use $\|T\|_1$,
our estimate of $\sep$ may differ from the true value by as much as
$\sqrt{m(n-m)}$. This approximation to $\sep$ is called
{\tt RCONDV} by xGEEVX and xGEESX.
\index{arguments!RCONDV}

Another formulation which in principle permits an exact evaluation of
$\sep ( A_{11},A_{22})$ is
\bq\label{eq2.5}
\sep (A_{11},A_{22}) = \sigma_{\min} ( I_{n-m} \otimes A_{11} -
A_{22}^T \otimes I_m )
\eq
where $X \otimes Y \equiv [ x_{ij} Y]$ is the Kronecker product of $X$ and $Y$.
This method is
generally impractical, however, because the matrix whose smallest singular
value we need is $m(n-m)$ dimensional, which can be as large as
$n^2/4$. Thus we would require as much as $O( n^4 )$ extra workspace and
$O(n^6)$ operations, much more than the estimation method of the last
paragraph.

The expression $\sep ( A_{11},A_{22})$ measures the ``separation'' of
the spectra
of $A_{11}$ and $A_{22}$ in the following sense. It is zero if and only if
$A_{11}$ and $A_{22}$ have a common eigenvalue, and small if there is a small
perturbation of either one that makes them have a common eigenvalue. If
$A_{11}$ and $A_{22}$ are both Hermitian matrices, then $\sep ( A_{11},A_{22})$
is just the gap, or minimum distance between an eigenvalue of $A_{11}$ and an
eigenvalue of $A_{22}$. On the other hand, if $A_{11}$ and $A_{22}$ are
non-Hermitian, $\sep ( A_{11},A_{22})$ may be much smaller than
this gap.

\section{Error Bounds for the Singular Value Decomposition}\label{secsvd}

The singular\index{error bounds!singular value decomposition} value decomposition (SVD) of a
real $m$-by-$n$ matrix $A$ is defined as follows. Let $r = \min (m,n)$.
The SVD of $A$ is $A=U \Sigma V^T$ ($A=U \Sigma V^H$ in the complex case),
where
$U$ and $V$ are orthogonal (unitary) matrices and
$\Sigma = \diag ( \sigma_1 , \ldots , \sigma_{r} )$ is diagonal,
with $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{r} \geq 0$.
The $\sigma_i$ are the {\bf singular values} of $A$ and the leading
$r$ columns $u_i$ of $U$ and $v_i$ of $V$ the
{\bf left and right singular vectors,} respectively.
The SVD of a general matrix is computed by xGESVD, xGESDD or SGEJSV
(see subsection \ref{subsecdriveeigSVD}).
\indexR{SGESVD}\indexR{CGESVD}
\indexR{SGESDD}\indexR{CGESDD}
\indexR{SGEJSV}

The approximate error bounds
for the computed singular values
$\hat{\sigma}_1 \geq \cdots \geq \hat{\sigma}_{r}$ are
\[
| \hat{\sigma}_i - \sigma_i | \leq {\tt SERRBD} \; \; .
\]
The approximate error bounds for the computed singular vectors
$\hat{v}_i$ and $\hat{u}_i$,
which bound the acute angles between the computed singular vectors and true
singular vectors $v_i$ and $u_i$, are
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
\begin{eqnarray*}
\theta ( \hat{v}_i , v_i ) & \leq & {\tt VERRBD}(i) \\
\theta ( \hat{u}_i , u_i ) & \leq & {\tt UERRBD}(i) \; \; .
\end{eqnarray*}
These bounds can be found by computing by the following code fragment.
\index{singular vector!error bound}
\index{singular value!error bound}

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Compute singular value decomposition of A
*     The singular values are returned in S
*     The left singular vectors are returned in U
*     The transposed right singular vectors are returned in VT
      CALL  SGESVD( 'S', 'S', M, N, A, LDA, S, U, LDU, VT, LDVT,
     $              WORK, LWORK, INFO )
      IF( INFO.GT.0 ) THEN
         PRINT *,'SGESVD did not converge'
      ELSE IF ( MIN(M,N) .GT. 0 ) THEN
         SERRBD  = EPSMCH * S(1)
*        Compute reciprocal condition numbers for singular vectors
         CALL SDISNA( 'Left', M, N, S, RCONDU, INFO )
         CALL SDISNA( 'Right', M, N, S, RCONDV, INFO )
         DO 10 I = 1, MIN(M,N)
            VERRBD( I ) = EPSMCH*( S(1)/RCONDV( I ) )
            UERRBD( I ) = EPSMCH*( S(1)/RCONDU( I ) )
10       CONTINUE
      END IF
\end{verbatim}

For example, if
${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$
and
\[
A = \bmat{ccc} 4 & 3 & 5 \\ 2 & 5 & 8 \\ 3 & 6 & 10 \\ 4 & 5 & 11 \emat \; ,
\]
then the singular values, approximate error bounds, and true errors are given below.

\begin{center}
\begin{tabular}{||c|c||c|c||c|c||c|c||}
\hline
$i$ & $\hat{\sigma}_i$ & {\tt SERRBD} & true $| \hat{\sigma}_i - \sigma_i |$ &
{\tt VERRBD}($i$) & true $\theta ( \hat{v}_i , v_i )$ &
{\tt UERRBD}($i$) & true $\theta ( \hat{u}_i , u_i )$  \\ \hline
1 & 21.05 & $1.3 \cdot 10^{-6}$ & $1.7 \cdot 10^{-6}$ &
$6.7 \cdot 10^{-8}$ & $8.1 \cdot 10^{-8}$ &
$6.7 \cdot 10^{-8}$ & $1.5 \cdot 10^{-7}$ \\
2 & 2.370 & $1.3 \cdot 10^{-6}$ & $5.8 \cdot 10^{-7}$ &
$1.0 \cdot 10^{-6}$ & $2.9 \cdot 10^{-7}$ &
$1.0 \cdot 10^{-6}$ & $2.4 \cdot 10^{-7}$  \\
3 & 1.143 & $1.3 \cdot 10^{-6}$ & $3.2 \cdot 10^{-7}$ &
$1.0 \cdot 10^{-6}$ & $3.0 \cdot 10^{-7}$ &
$1.1 \cdot 10^{-6}$ & $2.4 \cdot 10^{-7}$ \\ \hline
\end{tabular}
\end{center}

\subsection{Further Details:  Error Bounds for the Singular Value Decomposition}\label{secbackgroundsvd}

The usual error analysis of the SVD
algorithms\index{error bounds!singular value decomposition}
xGESVD and xGESDD in LAPACK (see subsection \ref{subsecdriveeigSVD})
is as follows \cite{demmelMA221,GVL2}:

\begin{quote}
The SVD algorithm is backward stable.
\index{backward stability}

\index{stability!backward}
This means that the computed SVD, $\hat{U} \hat{\Sigma} \hat{V}^T$,
is nearly the exact SVD of $A+E$ where $\|E\|_2 / \|A\|_2 \leq p(m,n) \epsilon$,
and $p(m,n)$ is a modestly growing function of $m$ and $n$. This means
$A+E = (\hat{U} + \delta \hat{U}) \hat{\Sigma} (\hat{V}+ \delta \hat{V})$
is the true SVD, so that $\hat{U}+ \delta \hat{U}$ and $\hat{V}+ \delta
\hat{V}$ are both orthogonal, where
$\| \delta \hat{U} \| \leq p(m,n) \epsilon$, and
$\| \delta \hat{V} \| \leq p(m,n) \epsilon$.
Each computed singular value $\sigmahat_i$ differs from true $\sigma_i$
by at most
\[
| \sigmahat_i - \sigma_i | \leq p(m,n) \cdot \epsilon \cdot \sigma_1
= {\tt SERRBD} \; ,
\]
(we take $p(m,n)=1$ in the code fragment).
Thus large singular values (those near $\sigma_1$) are computed to
high relative accuracy \index{accuracy!high} and small ones may not be.
\index{singular vector!error bound}
\index{singular value!error bound}

There are two questions to ask about the computed singular vectors:
``Are they orthogonal?'' and ``How much do they differ from the
true singular vectors?''
The answer to the first question is yes,
the computed singular vectors
are always nearly orthogonal to working precision, independent of
how much they differ from the true singular vectors. In other words
\[
|\hat{u}_i^T \hat{u}_j | = O( \epsilon )
\]
for $i \neq j$.

Here is the answer to the second question about singular vectors.
The angular difference between the computed left singular vector $\uhat_i$
and a true $u_i$ satisfies the approximate bound
\[
\theta ( \uhat_i , u_i ) \leapproxeq \frac{p(m,n) \epsilon \|A\|_2}{\gap_i}
= {\tt UERRBD}(i)
\]
where $\gap_i = \min_{j \neq i} | \sigma_i - \sigma_j |$ is the
{\bf absolute gap}\index{absolute gap}\index{gap}
between $\sigma_i$ and the nearest other singular value.
We take $p(m,n)=1$ in the code fragment.
Thus, if $\sigma_i$
is close to other singular values, its corresponding singular vector $u_i$
may be inaccurate. When $n < m$, then $\gap_n$ must be redefined
as $\min ( \min_{j \neq n} ( | \sigma_n - \sigma_j | ), \sigma_n )$.
The gaps may be easily computed from the array of computed singular values
using subroutine \indexR{SDISNA}SDISNA.
The gaps computed by SDISNA are ensured not to be so small as
to cause overflow when used as divisors.
\index{overflow}
\index{floating-point arithmetic!overflow}
The same bound applies to the computed right singular
vector $\vhat_i$ and a true vector $v_i$.

Let $\hatcalS$ be the space spanned by a collection of computed left singular
vectors $\{\hat{u}_i \, , \, i \in {\cal I}\}$, where $\cal I$ is a subset
of the integers from 1 to $n$. Let $\calS$ be the corresponding true space.
Then
\[
\theta ( \hatcalS , \calS ) \leapproxeq \frac{p(m,n) \epsilon \|A\|_2}
{\gap_{\cal I}}  .
\]
where
\[
\gap_{\cal I} = \min_{i \in {\cal I} \atop j \not\in {\cal I}}
| \sigma_i - \sigma_j |
\]
is the absolute gap between the singular values in $\cal I$ and the nearest
other singular value. Thus, a cluster\index{cluster!singular values}
of close singular values which is
far away from any other singular value may have a well determined
space $\hatcalS$ even if its individual singular vectors are ill-conditioned.
The same bound applies to a set of right singular vectors
$\{\hat{v}_i \, , \, i \in {\cal I}\}$.
 (These bounds are special
cases of those in sections~\ref{secsym} and \ref{secnonsym},
since the singular values
and vectors of $A$ are simply related to the eigenvalues and eigenvectors of
the Hermitian matrix $\bmat{cc} 0 & A^H \\ A & 0 \emat$ \cite[p. 427]{GVL2}.)

\end{quote}

In the special case of bidiagonal matrices, the singular values and
singular vectors may be computed much more accurately. A bidiagonal
matrix $B$ has nonzero entries only on the main diagonal and the diagonal
immediately
above it (or immediately below it). xGESVD computes the SVD of a general
\indexR{SGESVD}\indexR{CGESVD}
matrix by first reducing it to bidiagonal form $B$, and then calling xBDSQR
\indexR{SBDSQR}\indexR{CBDSQR}
(subsection \ref{subseccompsvd})
to compute the SVD of $B$.
xGESDD is similar, but calls xBDSDC to compute the SVD of $B$.
\indexR{SBDSDC}
Reduction of a dense matrix to bidiagonal form $B$ can introduce
additional errors, so the following bounds for the bidiagonal case
do not apply to the dense case.

\begin{quote}
Each computed singular value of a bidiagonal matrix
is accurate to nearly full relative accuracy\index{accuracy!high},
no matter how tiny it is:
\[
| \sigmahat_i - \sigma_i | \leq p(m,n) \cdot \epsilon \cdot \sigma_i.
\]
\index{singular vector!error bound}\index{singular value!error bound}
The following bounds apply only to xBDSQR.
The computed left singular vector $\uhat_i$ has an angular error
at most about
\[
\theta ( \uhat_i , u_i ) \leapproxeq \frac{p(m,n) \epsilon}{\relgap_i}
\]
where
$\relgap_i = \min_{j \neq i} | \sigma_i - \sigma_j | / ( \sigma_i + \sigma_j )$
is the {\bf relative gap} between $\sigma_i$ and the nearest other singular
value. The same bound applies to the right singular vector $\vhat_i$ and $v_i$.
Since the relative gap\index{relative gap}\index{gap} may be much larger than
the absolute gap\index{absolute gap}\index{gap},
this error bound may be much smaller than the previous one. The relative gaps
may be easily computed from the array of computed singular values.
\end{quote}

In the very special case of 2-by-2 bidiagonal matrices, xBDSQR and xBDSDC
call auxiliary routine xLASV2\indexR{SLASV2} to compute the SVD. xLASV2 will
actually compute nearly correctly rounded singular vectors independent of
the relative gap, but this requires accurate computer arithmetic:
if leading digits cancel during floating-point subtraction, the resulting
difference must be exact.
On machines without guard digits one has the slightly weaker result that the
algorithm is componentwise relatively backward stable, and therefore the
accuracy \index{accuracy} of the singular vectors depends on the relative gap as described
above.
\index{backward stability!componentwise}
\index{stability!backward}

SGEJSV\indexR{SGEJSV} is a pre-conditioned implentation of a one-sided Jacobi method 
\cite{Demmelveselic,veselicslapnicar,slapnicar1,drmacveselic1,drmacveselic2}.
It is slower than the algorithms based on first bidiagonalizing the matrix,
but is capable of computing more accurate answers in several important cases.

\section{Error Bounds for the Generalized Symmetric Definite
Eigenproblem}\label{secgendef}

There are four types of problems to consider.
\index{error bounds!generalized symmetric definite eigenproblem}
In all cases $A$ and $B$
are real symmetric (or complex Hermitian) and $B$ is positive definite.
These decompositions are computed for real symmetric matrices
by the driver routines
xSYGV, xSYGVX, xSYGVD,
xSPGV, xSPGVX, xSPGVD,
and (for types 1 and 4 only)
xSBGV3, xSBGVX3 and xSBGVD3\indexR{SSYGV}\indexR{SSPGV}\indexR{SSBGV3}
\indexR{SSYGVX}\indexR{SSPGVX}\indexR{SSBGVX3}
\indexR{SSYGVD}\indexR{SSPGVD}\indexR{SSBGVD3}
(see subsection \ref{secGSEP}).
These decompositions are computed for complex Hermitian matrices
by the driver routines
xHEGV, xHEGVX, xHEGVD,
xHPGV, xHPGVX, xHPGVD,
and (for types 1 and 4 only)
xHBGV3, xHBGVX3, xHBGVD3\indexR{CHEGV}\indexR{CHPGV}\indexR{CHBGV3}
\indexR{CHEGVX}\indexR{CHPGVX}\indexR{CHBGVX3}
\indexR{CHEGVD}\indexR{CHPGVD}\indexR{CHBGVD3}
(see subsection \ref{secGSEP}).
In each of the following four decompositions,
$\Lambda$ is real and diagonal with diagonal entries
$\lambda_1 \leq \cdots \leq \lambda_n$, and
the columns $z_i$ of $Z$ are linearly independent vectors.
The $\lambda_i$ are called
{\bf eigenvalues} and the $z_i$ are
{\bf eigenvectors}.

\begin{enumerate}
\item $A- \lambda B$ or equivalently $B^{-1} A - \lambda I$.
The eigendecomposition may be written $Z^T A Z = \Lambda$ and
$Z^T B Z = I$ (or $Z^H A Z = \Lambda$ and $Z^H B Z = I$
if $A$ and $B$ are complex).
This may also be written $Az_i = \lambda_i B z_i$.

\item $AB - \lambda I$.
The eigendecomposition may be written $Z^{-1} A Z^{-T} = \Lambda$ and
$Z^T B Z = I$ ($Z^{-1} A Z^{-H} = \Lambda$ and $Z^H B Z = I$ if $A$
and $B$ are complex).
This may also be written $ABz_i = \lambda_i z_i$.

\item $BA - \lambda I$ or equivalently $A - \lambda B^{-1}$.
The eigendecomposition may be written $Z^T A Z = \Lambda$
and $Z^T B^{-1} Z = I$ ($Z^H A Z = \Lambda$ and $Z^H B^{-1} Z = I$ if $A$
and $B$ are complex).
This may also be written $BAz_i = \lambda_i z_i$.

\item $A B^{-1} - \lambda I$.
The eigendecomposition may be written $Z^{-1} A Z^{-T} = \Lambda$
and $Z^T B^{-1} Z = I$ ($Z^{-1} A Z^{-H} = \Lambda$ and $Z^H B^{-1} Z = I$ if $A$
and $B$ are complex).
This may also be written $A B^{-1} z_i = \lambda_i z_i$.

\end{enumerate}

The approximate error bounds
for the computed eigenvalues $\hat{\lambda}_1 \leq \cdots \leq \hat{\lambda}_n$
are
\[
| \hat{\lambda}_i - \lambda_i | \leq {\tt EERRBD}(i) \; .
\]
The approximate error
bounds\index{eigenvector!error bound}\index{eigenvalue!error bound}
for the computed eigenvectors $\hat{z}_i$,
which bound the acute angles between the computed eigenvectors and true
eigenvectors $z_i$, are
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
\[
\theta ( \hat{z}_i , z_i ) \leq {\tt ZERRBD}(i) \; .
\]
These bounds are computed differently, depending on which of the above four
problems are to be solved. The following code fragments show how.

\begin{enumerate}

\item First we consider error bounds for problems 1 and 4, which are the same.
We illustrate only type 1.


\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Solve the eigenproblem A - lambda B (ITYPE = 1)
      ITYPE = 1
*     Compute the norms of A and B
      ANORM = SLANSY( '1', UPLO, N, A, LDA, WORK )
      BNORM = SLANSY( '1', UPLO, N, B, LDB, WORK )
*     The eigenvalues are returned in W
*     The eigenvectors are returned in A
      CALL SSYGV( ITYPE, 'V', UPLO, N, A, LDA, B, LDB, W, WORK,
     $            LWORK, INFO )
      IF( INFO.GT.0 .AND. INFO.LE.N ) THEN
         PRINT *,'SSYGV did not converge'
      ELSE IF( INFO.GT.N ) THEN
         PRINT *,'B not positive definite'
      ELSE IF ( N.GT.0 ) THEN
*        Get reciprocal condition number RCONDB of Cholesky factor of B
         CALL STRCON( '1', UPLO, 'N', N, B, LDB, RCONDB, WORK, IWORK,
     $                INFO )
         RCONDB = MAX( RCONDB, EPSMCH )
         CALL SDISNA( 'Eigenvectors', N, N, W, RCONDZ, INFO )
         DO 10 I = 1, N
            EERRBD( I ) = ( EPSMCH / RCONDB**2 ) * ( ANORM / BNORM +
     $                      ABS( W(I) ) )
            ZERRBD( I ) = ( EPSMCH / RCONDB**3 ) * ( ( ANORM / BNORM )
     $                     / RCONDZ(I) + ( ABS( W(I) ) / RCONDZ(I) ) *
     $                     RCONDB )
10       CONTINUE
      END IF
\end{verbatim}
\index{condition number}

For example, if
${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$,
\[
A = \bmat{ccc} 100000 & 10099 & 2109 \\ 10099 & 100020 & 10012 \\
2109 & 10112 & -48461 \emat
\; \; {\rm and} \; \;
B = \bmat{ccc} 99 & 10 & 2 \\ 10 & 100 & 10 \\ 2 & 10 & 100 \emat
\]
then ${\tt ANORM} = 120231$,
${\tt BNORM} = 120$, and
${\tt RCONDB} = .8326$, and
the approximate eigenvalues, approximate error bounds,
and true errors are

\begin{center}
\begin{tabular}{||c|c||c|c||c|c||}
\hline
$i$ & $\lambda_i$ &
{\tt EERRBD}$(i)$ & true $| \hat{\lambda}_i - \lambda_i |$ &
{\tt ZERRBD}$(i)$ & true $\theta ( \hat{v}_i , v_i )$ \\ \hline
1 & $-500.0$ & $1.3 \cdot 10^{-4}$ & $9.0 \cdot 10^{-6}$ &
$9.8 \cdot 10^{-8}$ & $1.0 \cdot 10^{-9}$ \\
2 & $1000.$ & $1.7 \cdot 10^{-4}$ & $4.6 \cdot 10^{-5}$ &
$1.9 \cdot 10^{-5}$ & $1.2 \cdot 10^{-7}$ \\
3 & $1010.$ & $1.7 \cdot 10^{-4}$ & $1.0 \cdot 10^{-4}$ &
$1.9 \cdot 10^{-5}$ & $1.1 \cdot 10^{-7}$ \\ \hline
\end{tabular}
\end{center}

This code fragment cannot be adapted to use 
xSBGV3 (or xHBGV3)\indexR{SSBGV3}\indexR{CHBGV3},
because xSBGV does not return a conventional Cholesky factor in $B$,
but rather a ``split'' Cholesky factorization (performed by
xPBSTF\indexR{SPBSTF}\indexR{CPBSTF}).

\item Problem types 2 and 3 have the same error bounds. We illustrate only type 2.
\index{eigenvector!error bound}
\index{eigenvalue!error bound}

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Solve the eigenproblem A*B - lambda I (ITYPE = 2)
      ITYPE = 2
*     Compute the norms of A and B
      ANORM = SLANSY( '1', UPLO, N, A, LDA, WORK )
      BNORM = SLANSY( '1', UPLO, N, B, LDB, WORK )
*     The eigenvalues are returned in W
*     The eigenvectors are returned in A
      CALL SSYGV( ITYPE, 'V', UPLO, N, A, LDA, B, LDB, W, WORK,
     $            LWORK, INFO )
      IF( INFO.GT.0 .AND. INFO.LE.N ) THEN
         PRINT *,'SSYGV did not converge'
      ELSE IF( INFO.GT.N ) THEN
         PRINT *,'B not positive definite'
      ELSE IF ( N.GT.0 ) THEN
*        Get reciprocal condition number RCONDB of Cholesky factor of B
         CALL STRCON( '1', UPLO, 'N', N, B, LDB, RCONDB, WORK, IWORK,
     $                INFO )
         RCONDB = MAX( RCONDB, EPSMCH )
         CALL SDISNA( 'Eigenvectors', N, N, W, RCONDZ, INFO )
         DO 10 I = 1, N
            EERRBD(I) = ( ANORM * BNORM ) * EPSMCH +
     $                  ( EPSMCH / RCONDB**2 ) * ABS( W(I) )
            ZERRBD(I) = ( EPSMCH / RCONDB ) * ( ( ANORM * BNORM ) /
     $                    RCONDZ(I) + 1.0 / RCONDB )
10       CONTINUE
      END IF
\end{verbatim}
\index{condition number}

For the same $A$ and $B$ as above, the approximate eigenvalues,
approximate error bounds, and true errors are

\begin{center}
\begin{tabular}{||c|c||c|c||c|c||}
\hline
$i$ & $\lambda_i$ &
{\tt EERRBD}$(i)$ & true $| \hat{\lambda}_i - \lambda_i |$ &
{\tt ZERRBD}$(i)$ & true $\theta ( \hat{v}_i , v_i )$ \\ \hline
1 & $-4.817 \cdot 10^6$ & $1.3$ & $6.0 \cdot 10^{-3}$ &
$1.7 \cdot 10^{-7}$ & $7.0 \cdot 10^{-9}$ \\
2 & $8.094 \cdot 10^6$ & $1.6$ & $1.5$ &
$3.4 \cdot 10^{-7}$ & $3.3 \cdot 10^{-8}$ \\
3 & $1.219 \cdot 10^7$ & $1.9$ & $4.5$ &
$3.4 \cdot 10^{-7}$ & $4.7 \cdot 10^{-8}$ \\ \hline
\end{tabular}
\end{center}

\end{enumerate}

\subsection{Further Details:  Error Bounds for the Generalized Symmetric Definite
Eigenproblem}\label{secGSEPFurtherDetails}

The error analysis of the driver routines for the generalized symmetric definite
eigenproblem goes as follows.
In all cases
$\gap_i = \min_{j \neq i} | \lambda_i - \lambda_j |$ is
the {\bf absolute gap}\index{absolute gap}\index{gap}
between $\lambda_i$ and the nearest other eigenvalue.
\index{error bounds!generalized symmetric definite eigenproblem}

\begin{enumerate}

\item $A - \lambda B$ or $A B^{-1} -\lambda$.

The computed eigenvalues $\hat{\lambda}_i$ can differ
from true eigenvalues $\lambda_i$ by at most about
\[
|\hat{\lambda}_i - \lambda_i | \leapproxeq p(n) \epsilon
\cdot ( \|B^{-1}\|_2 \|A\|_2  + \kappa_2 (B) \cdot | \hat{\lambda}_i | )
 = {\tt EERRBD}(i) \; .
\]
\index{eigenvalue!error bound}

The angular difference between the computed eigenvector
$\hat{z}_i$ and a true eigenvector $z_i$ is
\[
\theta ( \hat{z_i} , z_i ) \leapproxeq
p(n) \epsilon
\frac{\|B^{-1} \|_2 \|A\|_2 ( \kappa_2 (B) )^{1/2}
+ \kappa_2 (B) | \hat{\lambda}_i |}
{ \gap_i } = {\tt ZERRBD}(i) \; .
\]
\index{eigenvector!error bound}

\item $AB - \lambda I$ or $BA - \lambda I$.

The computed eigenvalues $\hat{\lambda}_i$ can differ
from true eigenvalues $\lambda_i$ by at most about
\[
|\hat{\lambda}_i - \lambda_i | \leapproxeq p(n) \epsilon \cdot
( \|B\|_2 \|A\|_2 + \kappa_2 (B) \cdot | \hat{\lambda}_i | )
= {\tt EERRBD}(i) \; .
\]
\index{eigenvalue!error bound}

The angular difference between the computed eigenvector
$\hat{z}_i$ and a true eigenvector $z_i$ is
\[
\theta ( \hat{z_i} ,  z_i ) \leapproxeq
p(n) \epsilon
\frac{\|B\|_2 \|A\|_2 ( \kappa_2 (B) )^{1/2}
+ \kappa_2 (B) | \hat{\lambda}_i |}
{\gap_i } = {\tt ZERRBD}(i) \; .
\]
\index{eigenvector!error bound}

\end{enumerate}

The code fragments above replace $p(n)$ by 1, and make sure that
neither {\tt RCONDB} nor {\tt RCONDZ} is so small as to cause
overflow when used as divisors in the expressions for error bounds.

These error bounds are large when $B$ is ill-conditioned with respect to
inversion ($\kappa_2 (B)$ is large). It is often the case that the eigenvalues
and eigenvectors are much better conditioned than indicated here.
We mention three ways to get tighter bounds.

\begin{enumerate}

\item The first way is effective when the diagonal entries of $B$ differ
widely in magnitude.
(This bound is guaranteed
only if the Level 3 BLAS are implemented in a conventional way,
not in a fast way as described in section \ref{secfastblas}.)

\begin{enumerate}

\item $A- \lambda B$ or $A B^{-1} - \lambda I$. 
Let $D = \diag (b_{11}^{-1/2} , \ldots , b_{nn}^{-1/2})$
be a diagonal matrix.
Then replace $B$ by $DBD$ and $A$ by $DAD$ in the above bounds.

\item $AB- \lambda I$ or $BA - \lambda I$.
Let $D = \diag (b_{11}^{-1/2} , \ldots , b_{nn}^{-1/2})$
be a diagonal matrix.
Then replace $B$ by $DBD$ and $A$ by $D^{-1}AD^{-1}$ in the above bounds.

\end{enumerate}

\item The second way to get tighter bounds does not actually supply guaranteed
bounds, but its estimates are often better in practice.
It is not guaranteed because it assumes the algorithm is backward stable,
which is not necessarily true when $B$ is ill-conditioned.
\index{backward stability}
\index{stability!backward}
It estimates the {\bf chordal distance} between a
true eigenvalue $\lambda_i$ and a computed eigenvalue $\hat{\lambda}_i$:
\index{chordal distance}
\[
\chi ( \hat{\lambda}_i , \lambda_i ) =
\frac{| \hat{\lambda}_i - \lambda_i |}
{\sqrt{1 + \hat{\lambda}_i^2} \cdot \sqrt{1 +  \lambda_i^2} }
.\]
To interpret this measure we write $\lambda_i = \tan \theta$
and $\hat{\lambda}_i = \tan \hat{\theta}$. Then
$\chi ( \hat{\lambda}_i , \lambda_i ) = | \sin ( \hat{\theta} - \theta ) |$.
In other words, if $\hat{\lambda}_i$ represents the one-dimensional subspace
$\hat{\cal S}$ consisting of the line through the origin with slope $\hat{\lambda}_i$,
and $\lambda_i$ represents the analogous subspace $\cal S$, then
$\chi ( \hat{\lambda}_i , \lambda_i )$
is the sine of the acute angle $\theta ( \hat{\cal S} , {\cal S} )$ between these
subspaces.
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
Thus $\chi$ is bounded by one, and is small when both arguments are large.
(Another interpretation of chordal distance is as half the usual
Euclidean distance between the projections of $\hat{\lambda}_i$ and
$\lambda_i$ on the Riemann sphere, i.e., half the length of the chord
connecting the projections.)
This applies only to the first problem, $A - \lambda B$:

\begin{quote}
Suppose a computed eigenvalue $\hat{\lambda}_i$ of $A- \lambda B$ is
the exact eigenvalue of a perturbed problem $(A+E) - \lambda (B+F)$.
Let $x_i$ be the unit eigenvector ($\|x_i\|_2=1$) for the exact
eigenvalue $\lambda_i$.
Then if $\|E\|$ is small compared to
$\|A\|$, and if $\|F\|$ is small compared to $\|B\|$, we have
\[
\chi ( \hat{\lambda}_i , \lambda_i ) \leapproxeq
\frac{\|E\| + \|F\|}{\sqrt{(x_i^H Ax_i )^2 + (x_i^H Bx_i )^2}} \; \; .
\]
Thus $1/{\sqrt{(x_i^H Ax_i )^2 + (x_i^H Bx_i )^2}}$ is a condition number for
eigenvalue $\lambda_i$.
\index{condition number}
\end{quote}

\item The third way applies only to the first problem $A - \lambda B$, and only
when $A$ is positive definite. We use a different algorithm:

\begin{enumerate}

\item Compute the Cholesky factorization of $A = U^T_A U_A$, using xPOTRF.

\item Compute the Cholesky factorization of $B = U^T_B U_B$, using xPOTRF.

\item Compute the generalized singular value decomposition of the pair
$U_A$, $U_B$ using xTGSJA. The squares of the generalized singular
\indexR{STGSJA}\indexR{CTGSJA}
values are the desired eigenvalues.

\end{enumerate}

See sections~\ref{sectionGSVDdriver} and ~\ref{sectionGSVDcomputational}
for a discussion of the generalized singular
value decomposition, and section~\ref{secGSVDbound} for a discussion of
the relevant error bound. This approach can give a tighter error bound
than the above bounds when $B$ is ill-conditioned but $A+B$ is
well-conditioned.

\end{enumerate}

Other yet more refined algorithms and error bounds are discussed in
\cite{barlowdemmel,stewartsun90,wilkinson1}.

\section{Error Bounds for the Generalized Nonsymmetric Eigenproblem}
\label{sec_GNEPErrorBounds}

We start by stating the simplest error bounds for individual eigenvalues
and eigenvectors and leave the more complicated ones to subsequent
subsections. \index{error bounds!generalized nonsymmetric eigenproblem}

As discussed in section~\ref{sec_gnep_comp},
from a computational point of view it is more natural to define
the generalized nonsymmetric eigenvalue problem in
the form $\beta A x = \alpha B x$ with $\lambda = \alpha / \beta$
instead of $A x = \lambda B x$.  The eigenvalue $\lambda$ is represented
as a pair $(\alpha, \beta)$, where a finite
eigenvalue has $\beta \neq 0$ and an infinite eigenvalue has $\beta = 0$.
As in the standard nonsymmetric eigenvalue problem we have both right
and left eigenvectors $x \neq 0$ and $y \neq 0$, respectively, defined as
\bq\label{eq411.1}
\beta A x = \alpha B x, \quad \beta y^H A  = \alpha y^H B .
\eq
Error bounds for eigenvalues are stated in terms of
the distance between pairs $(\alpha, \beta)$ and $(\alpha',\beta')$.
Let $\lambda = \alpha/\beta$ and $\lambda' = \alpha'/\beta'$.
Then the {\em chordal distance}\index{chordal distance} between $\lambda$ and
$\lambda'$ (see section~\ref{secGSEPFurtherDetails})
can equivalently be expressed as the chordal distance between two pairs:
\bq\label{eq411.2}
{\cal X}( \lambda,\lambda') =
{\cal X}((\alpha, \beta), (\alpha',\beta')) = \frac{|\alpha \beta' -
\beta \alpha' |}{\sqrt{|\alpha|^2 + |\beta|^2} \sqrt{|\alpha'|^2 + |\beta'|^2}}.
\eq

Now we state our error bounds.
Let $({\alpha}_i, {\beta}_i), i = 1, \ldots, n$ be the eigenvalues of $(A, B)$,
let $x_i$ be a right eigenvector corresponding to $({\alpha}_i, {\beta}_i)$:
${\beta}_i A x_i = {\alpha}_i B x_i$ and let
$({\hat{\alpha}}_i, {\hat{\beta}}_i)$
and $\hat{x}_i$ be the corresponding eigenvalues and
eigenvectors computed by the expert driver routine
xGGEVX (see subsection \ref{sec_gnep_driver}).
\indexR{SGGEVX}\indexR{CGGEVX}

The approximate error bounds for the computed eigenvalues are
\[
{\cal X}(({\hat{\alpha}}_i, {\hat{\beta}}_i), ({\alpha}_i, {\beta}_i)) \leq
{\tt EERRBD}(i) .
\]
The approximate error bounds
\index{eigenvector!error bound}\index{eigenvalue!error bound}
for the computed eigenvectors $\hat{x}_i$, which
bound the acute angles between the computed eigenvectors and the true
eigenvectors $x_i$, are
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
\[
\theta(\hat{x}_i, x_i) \leq {\tt VERRBD}(i) .
\]
The same bounds also hold for the computed left eigenvectors.

These bounds can be computed by the following code fragment:

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Compute the generalized eigenvalues and eigenvectors of (A,B)
*     ALPHAR/BETA contains the real parts of the eigenvalues
*     ALPHAI/BETA contains the imaginary parts of the eigenvalues
*     VL contains the left eigenvectors
*     VR contains the right eigenvectors
      CALL SGGEVX( 'P', 'V', 'V', 'B', N, A, LDA, B, LDB, ALPHAR,
     $             ALPHAI, BETA, VL, LDVL, VR, LDVR, ILO, IHI, LSCALE,
     $             RSCALE, ABNRM, BBNRM, RCONDE, RCONDV, WORK, LWORK,
     $             IWORK, BWORK, INFO )
      IF( INFO.GT.0 ) THEN
         PRINT *,'INFO =', info, ' from SGGEVX.'
      ELSE IF( N.GT.0 ) THEN
         ABNORM = SLAPY2( ABNRM, BBNRM )
         DO 10 I = 1,N
            EERRBD(I) = EPSMCH*ABNORM/RCONDE(I)
            VERRBD(I) = EPSMCH*ABNORM/RCONDV(I)
   10    CONTINUE
      END IF
\end{verbatim}

For example, suppose
\verb+SLAMCH('E')+ $ = 2^{-24} = 5.960  \cdot 10^{-8}$ and
\[
A = \left( \begin{array}{cccc}
           -132~~   &   -88~~  &    84    &    104     \\
           -158.4   &   -79.2  &  ~~76.8  &  ~~129.6   \\
          ~~129.6   &  ~~81.6  &   -79.2  &   -100.8   \\
            160     &    84    &   -80~~  &   -132~~   \\
           \end{array} \right )
\]
and
\[
B = \left( \begin{array}{cccc}
           -60~~  &  -50~~  &   40   &   50   \\
           -69~~  &  -46.4  &   38   & ~~58.2 \\
          ~~58.8  &   46    &  -37.6 &  -48~~ \\
            70    &   50    &  -40~~ &  -60~~ \\
           \end{array} \right ).
\]

For this problem, the exact eigenvalues, eigenvectors, and eigenvalue
condition numbers are known.  Then the true eigenvalues, computed eigenvalues,
approximate error bounds, and true error bounds are
given in the following table.

\begin{center}
\begin{tabular}{||c|c c c c||} \hline
  $i$   &      1   &      2       &       3       & 4 \\ \hline
$\lambda_i$        & $1$ & $2$ & $3$ & $4$ \\
$\hat{\lambda}_i$
        &  $1.0000529 \cdot 10^0~~$ & $1.9999847 \cdot 10^0~~$
        &  $2.9999785 \cdot 10^0~~$  & $4.0002117 \cdot 10^0~~$ \\ \hline
\verb+EERRBD+$(i)$
        &  $9.4562565 \cdot 10^{-5}$ & $5.3651773 \cdot 10^{-5}$
        &  $5.6895351 \cdot 10^{-5}$ & $1.2976544 \cdot 10^{-4}$ \\
${\cal X}(\hat{\lambda}_i, \lambda_i )$
        &  $3.6398560 \cdot 10^{-5}$ & $2.3351108 \cdot 10^{-5}$
        &  $2.2801253 \cdot 10^{-6}$ & $1.0059956 \cdot 10^{-5}$ \\ \hline
\verb+VERRBD+$(i)$
        &  $1.4116328 \cdot 10^{-4}$ & $1.4498082 \cdot 10^{-4}$
        &  $6.8483077 \cdot 10^{-4}$ & $5.5552053 \cdot 10^{-4}$ \\
$\theta(\hat{l}_i,l_i)$
        &  $1.0050300 \cdot 10^{-5}$ & $6.9755580 \cdot 10^{-6}$
        &  $1.3587955 \cdot 10^{-5}$ & $4.2988235 \cdot 10^{-6}$ \\
$\theta(\hat{r}_i,r_i)$
        &  $5.2165419 \cdot 10^{-5}$ & $1.4475762 \cdot 10^{-5}$
        &  $5.1648690 \cdot 10^{-5}$ & $7.9673846 \cdot 10^{-5}$ \\ \hline
\end{tabular}
\end{center}

\subsection{Further Details: Error Bounds for the Generalized
Nonsymmetric Eigenproblem}

\subsubsection{Overview}\label{GNEP311}
In this subsection, we summarize all the available error bounds.
Later sections will provide further details.
The error bounds presented here apply to regular matrix pairs only.

Bounds for individual eigenvalues and eigenvectors are provided by
the driver
xGGEVX\indexR{SGGEVX}\indexR{CGGEVX}
(subsection \ref{sec_gnep_driver})
or the computational routine xTGSNA
\indexR{STGSNA}\indexR{CTGSNA}
(subsection \ref{sec_gnep_comp}).
Bounds for cluster of eigenvalues
\index{error bounds!clustered eigenvalues}
and their associated pair of deflating
subspaces are provided by the driver
xGGESX\indexR{SGGESX}\indexR{CGGESX}
(subsection \ref{sec_gnep_driver}) or the
computational routine xTGSEN
\indexR{STGSEN}\indexR{CTGSEN}
(subsection \ref{sec_gnep_comp}).
\index{error bounds!generalized nonsymmetric eigenproblem}

We let $({\hat{\alpha}}_i, {\hat{\beta}}_i)$ be the $i^{th}$ computed
eigenvalue pair and $({\alpha}_i, {\beta}_i)$ the $i^{th}$ exact eigenvalue
pair.
As for the nonsymmetric eigenvalue problem there is a
one-to-one correspondence between computed and exact eigenvalue pairs,
but the correspondence is not as simple to describe as in the generalized
symmetric definite case. (Since the eigenvalues are real, sorting provides
the one-to-one correspondence). For well-conditioned generalized eigenvalues
$({\hat{\alpha}}_i, {\hat{\beta}}_i)$ is usually the closest eigenvalue pair
to $({\alpha}_i, {\beta}_i)$ in the chordal metric,
but in ill-conditioned cases this is not always true. 
The (nonconstructive) correspondence between computed and exact
eigenvalues described 
for the standard nonsymmetric eigenvalue problem is also applicable here.

Let $\hat{x}_i$ and $\hat{y}_i$ be the corresponding computed right
and left eigenvectors, and $x_i$ and $y_i$ the exact right and left
eigenvectors (so that ${\beta}_i A x_i = {\alpha}_i B x_i$ and
${\beta}_i y_i^H A  = {\alpha}_i y_i^H B$).
As in the standard nonsymmetric eigenvalue problem, we also want to
bound the error in the average of a cluster of eigenvalues, corresponding
to a subset $\cal I$ of the integers from 1 to $n$.
However, since there are both finite and infinite eigenvalues,
we need a proper definition for the average of the eigenvalues
${\lambda}_i = {\alpha}_i/{\beta}_i$ for $i \in {\cal I}$.
Here we let $( {\alpha}_{\cal I}, {\beta}_{\cal I} )$
denote the average of the selected eigenvalues
$( {\alpha}_{\cal I}, {\beta}_{\cal I} ) =
  (\sum_{i \in {\cal I}} {\alpha}_i, \sum_{i \in {\cal I}} {\beta}_i )/
  (\sum_{i \in {\cal I}} 1)$, and similarly for
$( \hat{{\alpha}}_{\cal I}, \hat{{\beta}}_{\cal I} )$.
We also let ${\cal L}_{\cal I}$ and ${\cal R}_{\cal I}$ denote the
exact pair of left
and right deflating subspaces associated with the cluster of selected
eigenvalues.
Similarly, $\widehat{{\cal L}}_{\cal I}$ and
$\widehat{{\cal R}}_{\cal I}$ are the
corresponding computed pair of left and right deflating subspaces.

In order to make
the definition $( {\alpha}_{\cal I}, {\beta}_{\cal I} )$
of the cluster average in chordal sense meaningful
for all cases that can appear we have to make a normalization of the
matrix pair in generalized Schur form.
First, we make each nonzero $b_{ii}$ nonnegative real
by premultiplying by the appropriate complex number with unit absolute value.
Secondly, if all $b_{ii}$ in the cluster are zero, then
we make all real parts of each nonzero $a_{ii}$ nonnegative real.
This means that if there is at least one finite eigenvalue in the
cluster (i.e., at least one $b_{ii}$ is nonzero in $B_{11}$), then
${\rm trace}(B_{11}) = \sum_{i \in {\cal I}} {\beta}_i$ is nonzero and
the cluster average is finite.
If all $b_{ii}$ are zero and some $a_{ii}$ is nonzero then
${\rm trace}(A_{11}) = \sum_{i \in {\cal I}} {\alpha}_i$ is nonzero
and the cluster average is infinity.
Note the pencil is singular if one pair $(a_{ii}, b_{ii}) = (0,0)$
or close to singular if both $a_{ii}$ and $b_{ii}$ are tiny
(see Section \ref{sec_singular}).

The algorithms for the generalized nonsymmetric eigenproblem are normwise
backward stable\index{backward stability}\index{stability!backward}; the computed eigenvalues, eigenvectors and deflating
subspaces are the exact ones of slightly perturbed matrices $A + E$ and $B +F$,
where $\normf{(E, F)} \leq p(n) \epsilon \normf{(A, B)}$.
The code fragment in the previous subsection approximates
$\normf{(E, F)}$ by $\epsilon \cdot {\tt ABNORM}$, where
${\tt ABNORM} = \sqrt{ {\tt ABNRM}^2 + {\tt BBNRM}^2 }$,
and the values {\tt ABNRM} and {\tt BBNRM} returned by xGGEVX
are the 1-norm of the matrices $A$ and $B$, respectively.

xGGEVX (or xTGSNA) returns reciprocal condition numbers
for each eigenvalue pair
$({\hat{\alpha}}_i, {\hat{\beta}}_i)$ and corresponding
left and right eigenvectors $\hat{y}_i$ and $\hat{x}_i$:
$s_i$ and ${\rm Dif}_l(i)$. $s_i$ is a reciprocal condition
number for the computed eigenpair $({\hat{\alpha}}_i, {\hat{\beta}}_i)$,
and is referred to as {\tt RCONDE(i)} by xGGEVX.
\index{condition number}
\index{arguments!RCONDE}
${\rm Dif}_l(i)$ is a reciprocal condition number for the left and right
eigenvectors $\hat{y}_i$ and $\hat{x}_i$, and is referred to as
{\tt RCONDV(i)} by xGGEVX (see subsection \ref{GNEP33} for definitions).
Similarly, xGGESX (or xTGSEN) returns condition numbers for
eigenvalue clusters and deflating subspaces corresponding to
a subset $\cal I$ of the eigenvalues.
\index{arguments!RCONDV}
These are $l_{\cal I}$ and $r_{\cal I}$, the reciprocal values of
the left and right projection norms $p$ and $q$, and
estimates of the separation between two matrix pairs
defined by ${\rm Dif}_u({\cal I})$ and ${\rm Dif}_l({\cal I})$
(see subsection \ref{GNEP33} for definitions).
xGGESX reports $l_{\cal I}$ and $r_{\cal I}$ in {\tt RCONDE(1)}
and {\tt RCONDE(2)} ({\tt PL} and {\tt PR} in xTGSEN)), respectively,
and estimates of ${\rm Dif}_u({\cal I})$ and ${\rm Dif}_l({\cal I})$
in {\tt RCONDV(1)}
and {\tt RCONDV(2)} ({\tt DIF(1)} and {\tt DIF(2)} in xTGSEN), respectively.

As for the nonsymmetric eigenvalue problem we provide both asymptotic
and global error bounds.  The asymptotic approximate error bounds for
eigenvalues, averages of eigenvalues, eigenvectors, and deflating
subspaces provided in Table \ref{asymp} are true only for
sufficiently small $\normf{(E,F)}$.

\begin{table}[h]
\caption{Asymptotic error bounds for the generalized nonsymmetric eigenvalue
problem}\label{asymp}

\begin{center}
\begin{tabular}{|l|c|} \hline
Simple eigenvalue:  &
  ${\cal X}(({\hat{\alpha}}_i,{\hat{\beta}}_i), ({\alpha}_i,{\beta}_i))
    \lapproxeq \normf{(E,F)}/ s_i $ \\ \hline
Eigenvalue cluster: &
  ${\cal X}( ({\hat{\alpha}}_{\cal I},{\hat{\beta}}_{\cal I}),
             ({\alpha}_{\cal I},{\beta}_{\cal I}) )
    \lapproxeq \normf{(E,F)}/ l_{\cal I}$  \\ \hline
Eigenvector pair: &  \\
~~ Left & $\theta_{\max} (\hat{{y}}_i , {y}_i) \lapproxeq
{\normf{(E,F)}}/{{\rm Dif}_l(i)}$\\
~~ Right & $\theta_{\max} (\hat{{x}}_i , {x}_i) \lapproxeq
{\normf{(E,F)}}/{{\rm Dif}_l(i)}$\\ \hline
Deflating subspace pair: & \\
~~ Left & $\theta_{\max} ({\widehat{{\cal L}}}_{\cal I} , {\cal L}_{\cal I})
     \lapproxeq {\normf{(E,F)}}/{{\rm Dif}_l({\cal I})}$ \\
~~ Right & $\theta_{\max} ({\widehat{{\cal R}}}_{\cal I} , {\cal R}_{\cal I})
     \lapproxeq {\normf{(E,F)}}/{{\rm Dif}_l({\cal I})}$ \\ \hline
\end{tabular}
\end{center}
\end{table}
\index{cluster!generalized eigenvalues!error bound}
\index{generalized eigenvector!error bound}
\index{generalized eigenvalue!error bound}
\index{deflating subspaces!error bound}

If the problem is ill--conditioned, the asymptotic bounds
may only hold for extremely small values of $\normf{(E,F)}$. Therefore, we also
provide similar global error bounds, which are valid for
all perturbations that satisfy an upper bound on $\normf{(E,F)}$.
The global error bounds in Table \ref{global} are guaranteed to hold for all
$\normf{(E,F)} < {\Delta}$, where
\begin{itemize}
\item ${\Delta} \equiv \frac{1}{4} \min (l_i,r_i)
{\min({\rm Dif}_u(i), {\rm Dif}_l(i))}$ for an individual eigenvector pair, and
\item ${\Delta} \equiv \frac{1}{4} \min (l_{\cal I},r_{\cal I})
{\min({\rm Dif}_u({\cal I}), {\rm Dif}_l({\cal I}))}$ for a cluster of
eigenvalues or a deflating subspace pair.
\end{itemize}

We let $\delta \equiv \normf{(E,F)} / {\Delta}$ in Table \ref{global}.
If $\delta$ is small, then the computed pair of left and right deflating
subspaces (or computed left and right eigenvectors) are small perturbations of
the exact pair of deflating subspaces (or the true left and right eigenvectors).
The error bounds conform with the corresponding bounds for the nonsymmetric
eigenproblem (see subsection~\ref{secnepsummary} ).

\begin{table}[h]
\caption{ Global error bounds for the generalized nonsymmetric
eigenvalue problem assuming $\delta \equiv \normf{(E,F)}/{\Delta} < 1$.}
\label{global}
\begin{center}
\begin{tabular}{|l|c|} \hline
Eigenvalue cluster: &
  ${\cal X}( ({\hat{\alpha}}_{\cal I},{\hat{\beta}}_{\cal I}),
             ({\alpha}_{\cal I},{\beta}_{\cal I}) )
    \leq 2 \normf{(E,F)}/ l_{\cal I}$  \\ \hline
Eigenvector pair: &  \\
~~ Left & $\theta_{\max} (\hat{y}_i , y_i) \leq
\arctan \left(
{\delta \cdot l_i}/{(1 - \delta \sqrt{1 - l_i^2})}
\right)$  \\
~~ Right  & $\theta_{\max} (\hat{x}_i , x_i) \leq
\arctan \left(
{\delta \cdot r_i}/{(1 - \delta \sqrt{1 - r_i^2})}
\right)$ \\
\hline
Deflating subspace pair: &  \\
~~ Left & $\theta_{\max} ({\widehat{{\cal L}}}_{\cal I},{\cal L}_{\cal I})\leq
\arctan \left(
{\delta \cdot l_{\cal I}}/{(1 - \delta \sqrt{1 - l_{\cal I}^2})}
\right)$  \\
~~ Right & $\theta_{\max}({\widehat{{\cal R}}}_{\cal I},{\cal R}_{\cal I})\leq
\arctan \left(
{\delta \cdot r_{\cal I}}/{(1 - \delta \sqrt{1 - r_{\cal I}^2 })}
\right)$ \\
\hline
\end{tabular}
\end{center}
\end{table}

For ill--conditioned problems the restriction $\Delta$
on $\normf{(E,F)}$ may also be small.
Indeed, a small value of $\Delta$ shows that the cluster of
eigenvalues (in the (1,1)--blocks of $(A, B)$) is ill--conditioned in
the sense that small perturbations of $(A, B)$ may imply that one eigenvalue in
the cluster moves and coalesces with another eigenvalue (outside the cluster).
Accordingly, this also means that the associated (left and right)
deflating subspaces are sensitive to small perturbations,
since the size of the
perturbed subspaces may change for small perturbations of $(A, B)$.
See also the discussion of singular problems in section~\ref{sec_singular}.

As for the nonsymmetric eigenvalue problem we have global error bounds for
eigenvalues which are true for all $E$ and $F$.
Let $(A, B)$ be a diagonalizable matrix pair. We let the columns of
$\widehat{Y}$ and $\widehat{X}$ be the computed left and right
eigenvectors associated with
the computed generalized eigenvalue pairs
$({\hat{\alpha}}_i, {\hat{\beta}}_i), i = 1, \ldots, n$.
Moreover, we assume that $\widehat{Y}$ and $\widehat{X}$
are normalized such that $|\hat{\alpha}_i|^2 + |\hat{\beta}_i|^2 = 1$
and $\normt{\hat{y}_i} = 1$, i.e., we
overwrite $\hat{y}_i$ with  $\hat{y_i}/ \normt{\hat{y_i}}$, $({\hat{\alpha}}_i,
{\hat{\beta}}_i)$ with  $({\hat{\alpha}}_i, {\hat{\beta}}_i) /
(|\hat{\alpha}_i|^2 + |\hat{\beta}_i|^2)^{1/2}$ and $\hat{x}_i$ with
$\hat{x}_i \normt{\hat{y}_i} / (|\hat{\alpha}_i|^2 + |\hat{\beta}_i|^2)^{1/2}$.
Then all eigenvalues $({\alpha}_i, {\beta}_i)$ of $(A, B)$ with
$|{\alpha}_i|^2 + |{\beta}_i|^2 = 1$ lie in the union of $n$ regions
(``spheres'')
\bq\label{eq411.29a}
\left\{  (\alpha,\beta), |\alpha|^2 + |\beta|^2 = 1:
{\cal X}((\alpha,\beta), ({\hat{\alpha}}_i, {\hat{\beta}}_i))
\leq n \normt{(E,F)}/ s_i \right\}.
\eq
If $k$ of the regions overlap, so that any two points inside the $k$
``spheres''
can be connected by a continuous curve lying entirely inside the $k$ regions,
and if no larger set of $k + 1$ regions has this property,
then exactly $k$ of the
eigenvalues  $({\alpha}_i, {\beta}_i)$ lie inside the union of these
``spheres''.
In other words, the global error bound with respect to an individual eigenvalue
$({\alpha}_i,{\beta}_i)$ is only useful if it defines a region
that does not intersect with regions corresponding to other eigenvalues.
If two or more regions intersect,
then we can only say that a (true) eigenvalue of $(A, B)$ lies in
the union of the overlapping regions. If $\|(E,F)\|_2$ is so large
that $(A+E,B+F)$ could be singular, which means that the eigenvalues
are not well-determined by the data, then the error bound from
(\ref{eq411.29a}) will be so large as to not limit the eigenvalues at all;
see section~\ref{sec_singular} for details.

{\bf Notation Conversion} For easy of reference,
the following table summarizes the notation used in mathematical
expression of the error bounds in tables \ref{asymp} and \ref{global}
and in the corresponding driver and computational routines.

\begin{center}
\begin{tabular}{|l|c|l|c|l|} \hline
Mathematical & \multicolumn{2}{c|}{Driver Routines} &
               \multicolumn{2}{c|}{Computational Routines} \\
notation     & name & parameter    & name &  parameter  \\ \hline
$s_i$             & xGGEVX & {\tt RCONDE(i)} & xTGSNA & {\tt S(i)}~~~ \\
$\mbox{Dif}_l(i)$ & xGGEVX & {\tt RCONDV(i)} & xTGSNA & {\tt DIF(i)} \\
$l_{\cal I}$ & xGGESX & {\tt RCONDE(1)} & xTGSEN & {\tt PL}  \\
$r_{\cal I}$ & xGGESX & {\tt RCONDE(2)} & xTGSEN & {\tt PR} \\
$\mbox{Dif}_u({\cal I})$ & xGGESX & {\tt RCONDV(1)} & xTGSEN & {\tt DIF(1)} \\
$\mbox{Dif}_l({\cal I})$ & xGGESX & {\tt RCONDV(2)} & xTGSEN & {\tt DIF(2)} \\
\hline
\end{tabular}
\end{center}

The quantities $l_i$, $r_i$, $\mbox{Dif}_u(i)$ and $\mbox{Dif}_l(i)$ used in
Table \ref{global} (for the global error bounds of
the $i^{\mbox{th}}$ computed eigenvalue pair $(\hat{\alpha},\hat{\beta})$
and the left and right eigenvectors $\hat{y}_i$ and $\hat{x}_i$)
can be obtained by calling xTGSEN with ${\cal I} = \{ i \}$.

\subsubsection{Balancing and Conditioning}\label{GENP32}

As in the standard nonsymmetric eigenvalue problem (section \ref{secbalance}),
two preprocessing steps\index{balancing of eigenproblems}
may be performed on the input matrix pair $(A, B)$.
The first one is a {\bf permutation}, reordering the rows and columns
to attempt to make $A$ and $B$ block upper triangular, and
therefore to reduce the order of the eigenvalue problems to be solved:
we let $(A',B') = P_1 (A, B) P_2$, where $P_1$ and $P_2$ are
permutation matrices.
The second one is a {\bf scaling}\index{scaling}
by two-sided diagonal transformations $D_1$
and $D_2$ to make the elements of $A''= D_1 A' D_2$ and $B'' = D_1 B' D_2$
have magnitudes as close to unity as possible, so as to reduce the effect
of the roundoff error made by the later algorithm \cite{ward81}.
We refer to these two operations as {\em balancing}.

Balancing is performed by driver xGGEVX, which calls computational
routine xGGBAL. The user may choose to optionally
permute, scale, do both or do neither;
\indexR{SGGBAL}\indexR{CGGBAL}
this is specified by the input parameter
{\tt BALANC}\index{arguments!BALANC} when xGGEVX is called.
Permuting has no effect on the condition numbers\index{condition number}
or their interpretation as described in the previous subsections. Scaling does,
however, change their interpretation, as we now describe.

The output parameters of xGGEVX --
{\tt ILO}(integer), {\tt IHI}(integer),
{\tt LSCALE}(real array of length N),
{\tt RSCALE}(real array of length N),
{\tt ABNRM}(real) and
{\tt BBNRM}(real) --
\index{arguments!LSCALE}
\index{arguments!RSCALE}
\index{arguments!ILO and IHI}
\index{arguments!ABNRM}
\index{arguments!BBNRM}
describe the result of balancing the matrix pair $(A, B)$ to
$(A'',B'')$, where N is the dimension of $(A,B)$.
The matrix pair $(A'',B'')$ has block upper triangular
structure, with at most three blocks: from 1 to {\tt ILO}-1,
from {\tt ILO} to {\tt IHI}, and from {\tt IHI}+1 to N (see section
\ref{sec_gnep_comp}). The first and last blocks are upper
triangular, and so already in generalized Schur form. These blocks are not
scaled; only the block from {\tt ILO} to {\tt IHI} is scaled.
Details of the left permutations ($P_1$) and scaling ($D_1$)
and the right permutations ($P_2$) and scaling ($D_2$)
are described in {\tt LSCALE} and {\tt RSCALE}, respectively.
(See the specification of xGGEVX or xGGBAL for more information).
The 1-norms of $A''$ and $B''$ are returned in
{\tt ABNRM} and {\tt BBNRM}, respectively.

The condition numbers
\index{condition number}
described in earlier subsections are computed
for the balanced matrix pair $(A'',B'')$ in xGGEVX,  and so
some interpretation is needed to apply them to the eigenvalues
and eigenvectors of the original matrix pair $(A, B)$.
To use the bounds for eigenvalues in Tables \ref{asymp} and \ref{global},
we must replace $\|(E,F)\|_F$ by
$O(\epsilon)\|(A'',B'')\|_F =
O(\epsilon)\sqrt{ {\tt ABNRM}^2 + {\tt BBNRM}^2 }$.
To use the bounds for eigenvectors, we also need to take
into account that bounds on rotation of the right and left eigenvectors
are for the right and left eigenvectors $x''$ and $y''$ of
$A''$ and $B''$, respectively, which are related to
the right and left eigenvectors $x$ and $y$ by
$x'' = D^{-1}_2 P^T_2 x$ and $y'' = D^{-1}_1 P_1 y$,
or $x = P_2 D_2 x''$ and $y = P^T_1 D_1 x''$ respectively.
Let $\theta''$ be the bound on the rotation of $x''$
from Table \ref{asymp} and Table \ref{global} and
let $\theta$ be the desired bound on the rotation of $x$. Let
\[
\kappa(D_2)=\frac{ \max_{ {\tt ILO} \leq i \leq {\tt IHI} } {\tt RSCALE}(i) }
                 { \min_{ {\tt ILO} \leq i \leq {\tt IHI} } {\tt RSCALE}(i) }
\]
be the condition number of the right scaling $D_2$ with respect
to matrix inversion. Then
\[
\sin \theta \leq \kappa(D_2) \sin\theta''.
\]
Similarly, for the bound of the angles $\phi$ and $\phi''$ of the
left eigenvectors $y''$ and $y$, we have
\[
\sin \phi \leq \kappa(D_1) \sin \phi'',
\]
where $\kappa(D_1)$ is the condition number of the left
scaling $D_1$ with respect to inversion,
\[
\kappa(D_1)=\frac{ \max_{ {\tt ILO} \leq i \leq {\tt IHI} } {\tt LSCALE}(i) }
                 { \min_{ {\tt ILO} \leq i \leq {\tt IHI} } {\tt LSCALE}(i) }.
\]
\index{eigenvector!error bound}
\index{eigenvalue!error bound}

The numerical example in section~\ref{sec_GNEPErrorBounds}
does no scaling, just permutation.

\subsubsection{Computing $s_i$, $l_{\cal I}$, $r_{\cal I}$ and
${\rm Dif}_u$, ${\rm Dif}_l$}\label{GNEP33}

To explain $s_i$, $l_{\cal I}$, $r_{\cal I}$,
${\rm Dif}_u$ and ${\rm Dif}_l$
in Table \ref{asymp} and Table \ref{global},
\index{s and dif}
we need to introduce a condition number for an individual eigenvalue,
block diagonalization of a matrix pair and
the separation of two matrix pairs.
\index{separation of two matrix pairs}

Let $(\alpha_i, \beta_i) \neq (0, 0)$ be a simple eigenvalue of $(A, B)$ with
left and right
eigenvectors $y_i$ and $x_i$, respectively.
$s_i$ is the reciprocal condition number for a
simple eigenvalue of $(A, B)$ \cite{stewartsun90}:
\bq\label{eq411.30}
s_i = \frac{\sqrt{|y^H_iAx_i|^2 + |y^H_iBx_i|^2}}{\normt{x_i}\normt{y_i}}.
\eq
Notice that $y^H_iAx_i / y^H_iBx_i$ is equal to
$\lambda_i = \alpha_i / \beta_i$.
The condition number $s_i$ in (\ref{eq411.30}) is independent of
the normalization of the eigenvectors.
In the error bound of Table \ref{asymp} for a simple eigenvalue and
in (\ref{eq411.29a}), $s_i$
is returned as {\tt RCONDE(i)} by xGGEVX (as {\tt S(i)} by xTGSNA).

We assume that the matrix pair $(A, B)$ is in the generalized Schur form.
Consider a cluster of $m$ eigenvalues, counting multiplicities.
Moreover, assume the $n$-by-$n$ matrix pair $(A, B)$ is
\bq\label{eq411.31}
(A, B) = \left(
\bmat{cc}  A_{11} & A_{12} \\ 0  & A_{22} \emat,
\bmat{cc}  B_{11} & B_{12} \\ 0  & B_{22} \emat
\right) ,
\eq
where the eigenvalues of the $m$-by-$m$ matrix pair
$(A_{11}, B_{11})$ are exactly those in which we are interested.
In practice, if the eigenvalues on the (block) diagonal
of $(A, B)$ are not in the desired order,
routine xTGEXC
\indexR{STGEXC}\indexR{CTGEXC}
can be used to put the
desired ones in the upper left corner as shown \cite{kagstromporomaa94a}.

An equivalence transformation that block--diagonalizes $(A, B)$
can be expressed as
\small
\bq\label{eq411.32}
\bmat{cc}  I_m & -L \\ 0  & I_{n - m} \emat
\left(
\bmat{cc}  A_{11} & A_{12} \\ 0  & A_{22} \emat,
\bmat{cc}  B_{11} & B_{12} \\ 0  & B_{22} \emat
\right)
\bmat{cc}  I_m & R \\ 0  & I_{n - m} \emat =
\left(
\bmat{cc}  A_{11} & 0 \\ 0  & A_{22} \emat,
\bmat{cc}  B_{11} & 0 \\ 0  & B_{22} \emat
\right).
\eq
\normalsize
Solving for $(L,R)$ in (\ref{eq411.32}) is equivalent to solving
the system of linear equations
\bq\label{eq411.33}
\begin{array}{c}
A_{11}  R  - L   A_{22}  = - A_{12} \\
B_{11}  R  - L   B_{22}  = - B_{12}
\end{array}.
\eq
Equation (\ref{eq411.33}) is called a {\em generalized Sylvester equation}
\index{Sylvester equation!generalized}
\cite{kagstrom94,kagstromwestin89}. Given the
generalized Schur form (\ref{eq411.31}), we solve equation (\ref{eq411.33}) for
$L$ and $R$ using the subroutine xTGSYL.
\indexR{STGSYL}\indexR{CTGSYL}

$l_{\cal I}$ and $r_{\cal I}$ for the eigenvalues of $(A_{11}, B_{11})$
are defined as
\bq\label{eq411.34}
l_{\cal I} = \frac{1}{\sqrt{1 + \normf{L}^2}}, \quad
r_{\cal I} = \frac{1}{\sqrt{1 + \normf{R}^2}}.
\eq
In the perturbation theory for the generalized eigenvalue problem,
$p = l^{-1}_{\cal I}$ and $q = r^{-1}_{\cal I}$ play the same role as the norm
of the spectral projector $\|P\|$
does for the standard eigenvalue problem in section~\ref{secspec}.
Indeed, if $B = I$, then $p = q$ and $p$ equals the norm of the
projection onto an invariant subspace of $A$.
For the generalized eigenvalue problem we need both a left and a right
projection norm since the left and right deflating subspaces are (usually)
different. In Table \ref{global},
$l_i$ and $l_{\cal I}$ denote the left projector norm corresponding
to an individual
eigenvalue pair $({\hat{\alpha}}_i, {\hat{\beta}}_i)$ and a
cluster of eigenvalues
defined by the subset ${\cal I}$, respectively. Similar notation is used
for $r_i$ and $r_{\cal I}$.
The values of $l_{\cal I}$ and $r_{\cal I}$ are returned as {\tt RCONDE(1)}
and {\tt RCONDE(2)} from xGGESX (as {\tt PL} and {\tt PR} from xTGSEN).
\index{arguments!RCONDE}

The {\em separation} of two matrix pairs $(A_{11}, B_{11})$ and
$(A_{22}, B_{22})$
is defined as the smallest singular value of the linear map in (\ref{eq411.33})
which takes $(L, R)$ to $(A_{11}  R  - L   A_{22}, B_{11}  R  - L  B_{22})$
\cite{stewart73}:
\bq\label{eq411.35}
{\rm Dif}_u[(A_{11}, B_{11}),(A_{22}, B_{22})] =
\inf_{\|(L,R)\|_F = 1} {\|(A_{11}  R  - L   A_{22},
 B_{11}  R  - L  B_{22})\|_F} .
\eq
${\rm Dif}_u$ is a generalization of the separation
between two matrices (${\rm sep}(A_{11}, A_{22})$ in (4.6))
to two matrix pairs, and it
measures the separation of their spectra in the following sense.
If $(A_{11}, B_{11})$ and $(A_{22}, B_{22})$ have a common eigenvalue,
then ${\rm Dif}_u$ is zero, and it is
small if there is a small perturbation of either $(A_{11}, B_{11})$ or $(A_{22},
B_{22})$ that makes them have a common eigenvalue.

Notice that ${\rm Dif}_u[(A_{22}, B_{22}),(A_{11}, B_{11})]$
does not generally equal
${\rm Dif}_u[(A_{11}, B_{11}),(A_{22}, B_{22})]$ (unless $A_{ii}$ and $B_{ii}$
are symmetric for $i = 1, 2$). Accordingly, the ordering of the arguments plays
a role for the separation of two matrix pairs, while it does not for the
separation of two matrices
(${\rm sep}(A_{11}, A_{22}) = {\rm sep}(A_{22}, A_{11})$).
Therefore, we introduce the notation
\bq\label{eq411.36}
{\rm Dif}_l[(A_{11}, B_{11}),(A_{22}, B_{22})] =
{\rm Dif}_u[(A_{22}, B_{22}),(A_{11}, B_{11})] .
\eq
An associated generalized Sylvester operator
$(A_{22}  R  - L   A_{11}, B_{22} R  - L  B_{11})$
in the definition of ${\rm Dif}_l$ is obtained from
block--diagonalizing a regular matrix pair in {\em lower} block triangular
form, just as the operator
$(A_{11}  R  - L   A_{22}, B_{11}  R  - L B_{22})$
in the definition of ${\rm Dif}_u$ arises from
block--diagonalizing a regular matrix pair (\ref{eq411.31})
in {\em upper} block triangular form.

In the error bounds of Tables \ref{asymp} and \ref{global},
${\rm Dif}_l(i)$ and ${\rm Dif}_l({\cal I})$ denote ${\rm Dif}_l[(A_{11},
B_{11}),(A_{22}, B_{22})]$, where $(A_{11}, B_{11})$ corresponds
to an individual eigenvalue pair $({\hat{\alpha}}_i, {\hat{\beta}}_i)$
and a cluster of eigenvalues
defined by the subset ${\cal I}$, respectively. Similar notation is used
for ${\rm Dif}_u(i)$ and ${\rm Dif}_u({\cal I})$. xGGESX reports estimates of
${\rm Dif}_u({\cal I})$ and ${\rm Dif}_l({\cal I})$ in {\tt RCONDV(1)}
and {\tt RCONDV(2)} ({\tt DIF(1)} and {\tt DIF(2)} in xTGSEN), respectively.
\index{arguments!RCONDV}

From a matrix representation of (\ref{eq411.33}) it is possible to formulate an
exact expression of ${\rm Dif}_u$ as
\bq\label{eq411.37}
{\rm Dif}_u[(A_{11}, B_{11}),(A_{22}, B_{22})]
= \sigma_{\min} (Z_u)
= {\rm Dif}_u,
\eq
where $Z_u$ is the $2m(n - m)$-by-$2m(n - m)$ matrix
\[
Z_u = \bmat{cc} I_{n-m} \otimes A_{11} & -A_{22}^T \otimes I_{m} \\
          I_{n-m} \otimes B_{11} & -B_{22}^T \otimes I_{m} \emat ,
\]
and $\otimes$ is the Kronecker product. A method
based directly on forming $Z_u$ is generally impractical,
since $Z_u$ can be as large as $n^2/2 \times n^2/2$.
Thus we would require as much as $O(n^4)$ extra workspace and $O(n^6)$
operations, much more than
the estimation methods that we now describe.

We instead compute an estimate of ${\rm Dif}_u$ as the reciprocal value of an
estimate of ${\rm Dif}_u^{-1} = {\|Z_u^{-1}\|}_2 = 1 / \sigma_{\min} (Z_u)$,
where $Z_u$ is the matrix representation of the generalized Sylvester
operator.  It is possible to estimate ${\|Z_u^{-1}\|}_2$ by solving
generalized Sylvester equations\index{Sylvester equation!generalized}
in triangular form.
We provide both Frobenius norm and
1-norm ${\rm Dif}_u$ estimates \cite{kagstromporomaa93a}.
\index{condition number!estimate}
The 1-norm estimate makes the condition estimation uniform with the
nonsymmetric eigenvalue problem. The Frobenius norm estimate
offers a low cost and equally reliable estimator.
The 1-norm estimate is a factor 3 to 10 times more
expensive \cite{kagstromporomaa93a}. From
the definition of ${\rm Dif}_l$ (\ref{eq411.36}) we see that
${\rm Dif}_l$ estimates can be computed by using the algorithms for
estimating ${\rm Dif}_u$.

{\bf Frobenius norm estimate}: From
the $Z_ux = b$ representation of the
generalized Sylvester equation (\ref{eq411.33}) we get a
lower bound on ${\rm Dif}_u^{-1}$:
\bq\label{eq411.37a}
{\|(L, R)\|}_F / {\|(C, F)\|}_F = {\|x\|}_2 / {\|b\|}_2 \leq {\|Z_u^{-1}\|}_2 .
\eq
To get an improved estimate we try to choose right hand sides
$(C, F)$ such that
the associated solution $(L, R)$ has as large a norm as possible, giving the
${\rm Dif}_u$ estimator
\bq\label{eq411.38}
{\tt DIF(1)} \equiv {\|(C, F)\|}_F / {\|(L, R)\|}_F  .
\eq
Methods for computing such $(C, F)$ are described in
\cite{kagstromwestin89,kagstromporomaa93a}.
The work to compute {\tt DIF(1)} is comparable to solving a generalized
Sylvester equation, which costs only $2m^2(n-m) + 2m(n-m)^2$
operations if the matrix pairs are in generalized Schur form.
{\tt DIF(1)} is the Frobenius norm ${\rm Dif}_l$ estimate.

{\bf 1-norm estimate}: From the relationship
\bq\label{eq411.39}
\frac{1}{\sqrt{2m(n-m)}} {\|Z_u^{-1}\|}_{1} \leq {\|Z_u^{-1}\|}_2 \leq \sqrt{2m(n-m)}
{\|Z_u^{-1}\|}_{1},
\eq
we know that ${\|Z_u^{-1}\|}_{1}$ can never differ by more than a factor $\sqrt{2m(n -m)}$
from ${\|Z_u^{-1}\|}_2$. So it makes sense to compute a
1-norm estimate of ${\rm Dif}_u$.
xLACN2 implements a method for estimating the 1-norm of a square matrix,
using reverse communication for evaluating matrix and vector products
\cite{hager84,higham89}. We apply this method to ${\|Z_u^{-1}\|}_1$
by providing the solution vectors $x$ and $y$ of $Z_ux = z$  and
a transposed system $Z_u^Ty = z$, where $z$ is determined by xLACN2.
In each step only one of these generalized Sylvester equations is solved
using blocked algorithms \cite{kagstromporomaa93a}.
xLACN2 returns $v$ and ${\tt EST}$ such that $Z_u^{-1}w = v$ and ${\tt EST}
= {\|v\|}_{1}/ {\|w\|}_{1} \leq {\|Z_u^{-1}\|}_{1}$,
resulting in the 1-norm-based estimate
\bq\label{eq411.40}
{\tt DIF(1)} \equiv 1 / {\tt EST} .
\eq
The cost for computing this bound is roughly equal to the number of steps
in the reverse communication times the cost for one generalized
Sylvester solve. {\tt DIF(2)} is the 1-norm ${\rm Dif}_l$ estimate.

The expert driver routines xGGEVX and xGGESX compute the Frobenius
norm estimate (\ref{eq411.38}).
The routine xTGSNA also computes the Frobenius
norm estimate (\ref{eq411.38}) of ${\rm Dif}_u$ and ${\rm Dif}_l$.
The routine xTGSEN optionally computes the Frobenius norm estimate
(\ref{eq411.38}) or the 1-norm estimate (\ref{eq411.40}).
The choice of estimate is controlled by the input parameter {\tt IJOB}.

\subsubsection{Singular Eigenproblems}\label{sec_singular}

In this section, we give a brief discussion of singular
matrix pairs $(A,B)$\index{eigenvalue problem!singular}.

If the determinant of $A - \lambda B$ is zero for all values of $\lambda$
(or the determinant of $\beta A  - \alpha B$ is zero for all $(\alpha,\beta)$),
the pair $(A,B)$ is said to be {\bf singular}.
The eigenvalue problem of a singular pair is much more complicated
than for a regular pair.

Consider for example the singular pair
\[
A  = \left( \begin{array}{cc}
                          1 & 0 \\ 0 & 0 \\
            \end{array} \right), \quad\quad
B  = \left( \begin{array}{cc}
                         1 & 0 \\ 0 & 0 \\
            \end{array} \right),
\]
which has one finite eigenvalue 1 and one indeterminate eigenvalue 0/0.
To see that neither eigenvalue is well determined by the data,
consider the slightly different problem
\[
A'  = \left( \begin{array}{cc}
                          1 & \epsilon_1 \\ \epsilon_2 & 0 \\
            \end{array} \right), \quad\quad
B'  = \left( \begin{array}{cc}
                         1 & \epsilon_3 \\ \epsilon_4 & 0 \\
            \end{array} \right),
\]
where the $\epsilon_i$ are tiny nonzero numbers. Then it is easy to see
that $(A',B')$ is regular with eigenvalues
$\epsilon_1 / \epsilon_3$ and
$\epsilon_2 / \epsilon_4$. Given {\em any} two complex numbers $\lambda_1$ and
$\lambda_2$, we can find arbitrarily tiny $\epsilon_i$ such that
$\lambda_1 = \epsilon_1 / \epsilon_3$ and
$\lambda_2 = \epsilon_2 / \epsilon_4$ are the eigenvalues of $(A',B')$.
Since, in principle, roundoff could change $(A,B)$ to $(A',B')$, we
cannot hope to compute accurate or even meaningful eigenvalues of
singular problems, without further information.

\ignore{
If an 1-by-1 block $(\alpha,\beta) = (0,0)$ appears in the generalized
Schur form of $(A,B)$, then $(A,B)$ is singular.
In the presence of roundoff, it is often signaled by tiny $(\alpha,\beta)$
on the diagonal. For example, the regular pair
\[
A  = \left( \begin{array}{cc}
                          1 & 0 \\ 0 & 10^{-7}  \\
            \end{array} \right), \quad\quad
B  = \left( \begin{array}{cc}
                           1 & 0 \\ 0 & -10^{-7}\\
            \end{array} \right),
\]
has eigenvalues 1 and $-1$ but can also be interpreted as
singular in single precision arithmetic.
}

It is possible for a pair $(A,B)$ in Schur form to be very close to singular,
and so have very sensitive eigenvalues, even if no diagonal entries of
$A$ or $B$ are small.
It suffices, for example, for $A$ and $B$ to nearly have a common null space
(though this condition is not necessary).
For example, consider the 16-by-16 matrices
\[
A'' = \left( \begin{array}{ccccc}
           0.1 & 1   &        &        &     \\
               & 0.1 & 1      &        &     \\
               &     & \ddots & \ddots &      \\
               &     &        & 0.1    &  1   \\
               &     &        &        & 0.1   \\
           \end{array} \right) \quad\quad
\mbox{and} \quad\quad B'' = A''.
\]
Changing the $(n,1)$ entries of $A''$ and $B''$ to $10^{-16}$, respectively,
makes both $A$ and $B$ singular, with a common null vector.
Then, using a technique analogous to the one applied to $(A,B)$ above,
we can show that there is a
perturbation of $A''$ and $B''$ of norm $10^{-16} + \epsilon$,
for {\em any} $\epsilon>0$, that makes the 16 perturbed eigenvalues
have {\em any} arbitrary 16 complex values.

A complete understanding of the structure of a singular eigenproblem
$(A,B)$ requires a study of its {\em Kronecker canonical form},
a generalization of the {\em Jordan canonical form}.
In addition to Jordan blocks for finite and infinite eigenvalues,
the Kronecker form can contain ``singular blocks'', which occur only
if ${\rm det}(A- \lambda B) \equiv 0$ for all $\lambda$ (or if $A$ and $B$
are nonsquare).
See \cite{gantmacher,stewart72,wilkinson79,vandooren79,demmelkagstrom87} for
more details. Other numerical software, called GUPTRI\index{GUPTRI},
is available for
computing a generalization of the
Schur canonical form for singular eigenproblems
\cite{demmelkagstrom93a,demmelkagstrom93b}.

The error bounds discussed in this guide hold for regular pairs only
(they become unbounded, or otherwise provide no information, when
$(A,B)$ is close to singular).  If a (nearly) singular pencil is reported
by the software discussed in this guide, then a further study of the matrix
pencil should be conducted, in order to determine whether meaningful results
have been computed.

\section{Error Bounds for the Generalized Singular Value Decomposition}
\label{secGSVDbound}

The generalized (or quotient) singular value decomposition
of an $m$-by-$n$ matrix
$A$ and a $p$-by-$n$ matrix $B$ is the pair of factorizations
\index{error bounds!generalized singular value decomposition}
\[
A = U \Sigma_1 [0,R] Q^T \; \; {\rm and} \; \;
B = V \Sigma_2 [0,R] Q^T
\]
where $U$, $V$, $Q$, $R$, $\Sigma_1$ and $\Sigma_2$ are defined
as follows.
\begin{itemize}
\item $U$ is $m$-by-$m$, $V$ is $p$-by-$p$, $Q$ is $n$-by-$n$,
and all three matrices are orthogonal.  If $A$ and
$B$ are complex, these matrices are unitary instead
of orthogonal, and $Q^T$ should be
replaced by $Q^H$ in the pair of factorizations.
\item $R$ is $r$-by-$r$, upper triangular and nonsingular.
$[0,R]$ is $r$-by-$n$. The integer $r$ is the rank of
$\bmat{c} A \\ B \emat$, and satisfies $r \leq n$.
\item $\Sigma_1$ is $m$-by-$r$,
$\Sigma_2$ is $p$-by-$r$, both are real, non-negative and diagonal,
and $\Sigma_1^T \Sigma_1 + \Sigma_2^T \Sigma_2 = I$.
Write
$\Sigma_1^T \Sigma_1 = {\rm diag} ( \alpha_1^2 , \ldots , \alpha_r^2 )$ and
$\Sigma_2^T \Sigma_2 = {\rm diag} ( \beta_1^2 , \ldots , \beta_r^2 )$,
where $\alpha_i$ and $\beta_i$ lie in the interval from 0 to 1.
The ratios
$\alpha_1 / \beta_1 , \ldots,,  \alpha_r / \beta_r$
are called the {\bf generalized singular values} of the pair $A$, $B$.
If $\beta_i = 0$, then the generalized singular value
$\alpha_i / \beta_i$ is {\bf infinite}.
For details on the structure of $\Sigma_1$, $\Sigma_2$ and $R$, see
section~\ref{sectionGSVDdriver}.
\end{itemize}

The generalized singular value decomposition is
computed by driver routine xGGSVD (see section~\ref{sectionGSVDdriver}).
\indexR{SGGSVD}\indexR{CGGSVD}
We will give error bounds for the generalized
\index{error bounds!generalized singular value decomposition}
singular values in the
common case where $\bmat{c} A \\ B \emat$ has full
rank $r=n$.
Let $\hat{\alpha}_i$ and $\hat{\beta}_i$
be the values of $\alpha_i$ and $\beta_i$, respectively,
computed by xGGSVD.
The approximate error
bound
for these values is
\[
| \hat{\alpha}_i - \alpha_i | +
| \hat{\beta}_i - \beta_i |  \leq {\tt SERRBD} \; \; .
\]
Note that if $\beta_i$ is close to zero, then a true
generalized singular value
$\alpha_i / \beta_i$ can differ greatly in magnitude from
the computed generalized singular value
$\hat{\alpha}_i / \hat{\beta}_i$, even if {\tt SERRBD} is
close to its minimum $\epsilon$.
\index{singular value!error bound}

Here is another way to interpret {\tt SERRBD}:
if we think of $\alpha_i$
and $\beta_i$ as representing the {\em subspace} $\cal S$
consisting of the straight line through the origin with slope
$\alpha_i / \beta_i$, and similarly $\hat{\alpha}_i$
and $\hat{\beta}_i$ representing the subspace $\hat{\cal S}$,
then ${\tt SERRBD}$ bounds the acute angle between
$\cal S$ and $\hat{\cal S}$.
\index{angle between vectors and subspaces}
\index{subspaces!angle between}
Note that any two
lines through the origin with nearly vertical slopes
(very large $\alpha / \beta$) are close together in angle.
(This is related to the {\em chordal distance} in
section~\ref{secGSEPFurtherDetails}.)

{\tt SERRBD} can be computed by the following code fragment,
which for simplicity assumes $m \geq n$.
(The assumption $r=n$ implies only that $p+m \geq n$.
Error bounds can also be computed when $p+m \geq n > m$,
with slightly more complicated code.)

\begin{verbatim}
      EPSMCH = SLAMCH( 'E' )
*     Compute generalized singular values of A and B
      CALL SGGSVD( 'N', 'N', 'N', M, N, P, K, L, A, LDA, B,
     $             LDB, ALPHA, BETA, U, LDU, V, LDV, Q, LDQ,
     $             WORK, IWORK, INFO )
*     Compute rank of [A',B']'
      RANK = K+L
      IF( INFO.GT.0 ) THEN
         PRINT *,'SGGSVD did not converge'
      ELSE IF( RANK.LT.N ) THEN
         PRINT *,'[A**T,B**T]**T not full rank'
      ELSE IF ( M .GE. N .AND. N .GT. 0 ) THEN
*        Compute reciprocal condition number RCOND of R
         CALL STRCON( 'I', 'U', 'N', N, A, LDA, RCOND, WORK, IWORK,
     $                INFO )
         RCOND = MAX( RCOND, EPSMCH )
         SERRBD = EPSMCH / RCOND
      END IF
\end{verbatim}
\index{condition number}

For example, if
${\tt SLAMCH('E')} = 2^{-24} = 5.961 \cdot 10^{-8}$,
\[
A = \bmat{ccc} 1 & 2 & 3 \\ 3 & 2 & 1 \\ 4 & 5 & 6 \\ 7 & 8 & 8 \emat
\; \; {\rm and} \; \;
B = \bmat{ccc} -2 & -3 & 3 \\ 4 & 6 & 5 \emat
\]
then, to 4 decimal places,
\[
\bmat{c} \alpha_1 \\ \alpha_2 \\ \alpha_3 \emat =
\bmat{c} \hat{\alpha}_1 \\ \hat{\alpha}_2 \\ \hat{\alpha}_3 \emat =
\bmat{c} 1.000 \\ .7960 \\ 7.993 \cdot 10^{-2} \emat
\; \; {\rm and} \; \;
\bmat{c} \beta_1 \\ \beta_2 \\ \beta_3 \emat =
\bmat{c} \hat{\beta}_1 \\ \hat{\beta}_2 \\ \hat{\beta}_3 \emat =
\bmat{c} 0 \\ .6053 \\ .9968 \emat,
\]
${\tt SERRBD} = 1.4 \cdot 10^{-6}$, and the true errors
are $0$, $4.3 \cdot 10^{-7}$ and $1.5 \cdot 10^{-7}$.

\subsection{Further Details:  Error Bounds for the Generalized Singular Value Decomposition}

The GSVD algorithm used in LAPACK (\cite{paige86a,baidemmel92b,baizha93})
is backward stable:
\index{backward stability}
\index{stability!backward}
\index{error bounds!generalized singular value decomposition}

\begin{quote}

Let the computed GSVD of $A$ and $B$ be $\hat{U} \hat{\Sigma}_1 [0,\hat{R}] \hat{Q}^T$ and
$\hat{V} \hat{\Sigma}_2 [0,\hat{R}] \hat{Q}^T$.
This is nearly the exact GSVD of
$A+E$ and $B+F$ in the following sense. $E$ and $F$ are small:
\[
\|E\|_2 / \|A\|_2 \leq p(n) \epsilon \; \; {\rm and} \; \;
\|F\|_2 / \|B\|_2 \leq p(n) \epsilon  \; ;
\]
there exist small
$\delta \hat{Q}$, $\delta \hat{U}$, and $\delta \hat{V}$
such that
$\hat{Q} + \delta \hat{Q}$,
$\hat{U} + \delta \hat{U}$, and
$\hat{V} + \delta \hat{V}$ are exactly orthogonal (or unitary):
\[
\| \delta \hat{Q} \|_2 \leq p(n) \epsilon \; \; , \; \;
\| \delta \hat{U} \|_2 \leq p(n) \epsilon \; \; {\rm and} \; \;
\| \delta \hat{V} \|_2 \leq p(n) \epsilon \; \; ;
\]
and
\[
A + E = ( \hat{U} + \delta \hat{U} ) \hat{\Sigma}_1 [ 0, \hat{R}] (\hat{Q} + \delta \hat{Q})^T
\; \; {\rm and} \; \;
B + F = ( \hat{V} + \delta \hat{V} ) \hat{\Sigma}_2 [ 0, \hat{R}] (\hat{Q} + \delta \hat{Q})^T
\]
is the exact GSVD of $A+E$ and $B+F$. Here $p(n)$ is a modestly growing function of $n$, and
we take $p(n)=1$ in the above code fragment.

Let $\alpha_i$ and $\beta_i$ be the square roots of the diagonal entries of the exact
$\Sigma_1^T \Sigma_1$ and $\Sigma_2^T \Sigma_2$,
and let $\hat{\alpha}_i$ and $\hat{\beta}_i$ the square roots of the diagonal entries
of the computed $\hat{\Sigma}_1^T \hat{\Sigma}_1$ and $\hat{\Sigma}_2^T \hat{\Sigma}_2$.
Let
\[
G = \bmat{c} A \\ B \emat \; \; {\rm and} \; \; \hat{G} = \bmat{c} \hat{A} \\ \hat{B} \emat \; .
\]
Then provided $G$ and $\hat{G}$ have full rank $n$, one can show \cite{sun83,paige84} that
\[
\left( \sum_{i=1}^n [( \hat{\alpha}_i - \alpha_i )^2 +
                    ( \hat{\beta}_i - \beta_i )^2 ]  \right)^{1/2} \leq
\frac{\sqrt{2} \left\| \bmat{c} E \\ F \emat \right\|}
{\min ( \sigma_{\min} (G) , \sigma_{\min} (\hat{G}) )}  \; \; .
\]
In the code fragment we approximate the numerator of the last expression by
$\epsilon \|\hat{R}\|_{\infty}$ and approximate the denominator by
$\| \hat{R}^{-1} \|^{-1}_{\infty}$ in order to compute {\tt SERRBD};
xTRCON\indexR{STRCON}\indexR{CTRCON} returns an approximation {\tt RCOND} to
$1/ (\| \hat{R}^{-1} \|_{\infty} \| \hat{R} \|_{\infty})$.
\end{quote}
\index{singular value!error bound}

We assume that the rank $r$ of $G$ equals $n$, because otherwise the
$\alpha_i$s and $\beta_i$s are not well determined. For example, if
\[
A = \bmat{cc} 10^{-16} & 0 \\ 0 & 1 \emat
\; \; , \; \;
B = \bmat{cc} 0 & 0 \\ 0 & 1 \emat
\; \; {\rm and} \; \;
A' = \bmat{cc} 0 & 0 \\ 0 & 1 \emat
\; \; , \; \;
B' = \bmat{cc} 10^{-16} & 0 \\ 0 & 1 \emat
\]
then $A$ and $B$ have
$\alpha_{1,2} = 1, .7071$ and $\beta_{1,2} = 0, .7071$, whereas
$A'$ and $B'$ have
$\alpha'_{1,2} = 0, .7071$ and $\beta'_{1,2} = 1, .7071$, which
are completely different, even though $\|A - A' \| = 10^{-16}$ and
$\|B - B' \| = 10^{-16}$. In this case, $\sigma_{\min} (G) = 10^{-16}$,
so $G$ is nearly rank-deficient.

The reason the code fragment assumes $m \geq n$ is that in this case $\hat{R}$ is
stored overwritten on $A$, and can be passed to STRCON in order to compute
{\tt RCOND}. If $m \leq n$, then the
first $m$ rows of $\hat{R}$ are
stored in $A$, and the last $n-m$ rows of $\hat{R}$ are stored in $B$. This
complicates the computation of {\tt RCOND}: either $\hat{R}$ must be copied to
a single array before calling STRCON, or else the lower level subroutine SLACN2
must be used with code capable of solving linear equations with $\hat{R}$
and $\hat{R}^T$ as coefficient matrices.

\section{Error Bounds for Fast Level 3 BLAS}\label{secfastblas}

\def\Xhat{\widehat X}
\def\Chat{\widehat C}
\def\normo#1{\|#1\|_{\infty}}
\def\u{\epsilon}

The Level 3 BLAS specifications \cite{blas3} specify the input, output
and calling sequence for each routine, but allow freedom of
implementation, subject to the requirement that the routines be
numerically stable\index{error bounds!required for fast Level 3 BLAS}.
Level 3 BLAS implementations can therefore be
built using matrix multiplication algorithms that achieve a more
favorable operation count (for suitable dimensions) than the standard
multiplication technique, provided that these ``fast''  algorithms are
numerically stable.  The simplest fast matrix multiplication
technique is Strassen's
method\index{Strassen's method}\index{BLAS!Level 3, fast}, which can
multiply two $n$-by-$n$
matrices in fewer than $4.7 n^{\log_2 7}$ operations, where
$\log_2 7 \approx 2.807$.

The effect on the results in this chapter of using a fast Level~3 BLAS
implementation can be explained as follows. In general, reasonably
implemented fast Level~3 BLAS preserve all the bounds presented here
(except those at the end of subsection~\ref{secgendef}), but the constant
$p(n)$ may increase somewhat. Also, the iterative refinement
routine\index{iterative refinement}
xyyRFS may take more steps to converge.

This is what we mean by reasonably implemented fast Level~3 BLAS.
Here, $c_i$ denotes a constant depending on the specified matrix dimensions.

\begin{enumerate}

\item If $A$ is $m$-by-$n$, $B$ is $n$-by-$p$ and $\Chat$ is the computed
approximation to $C=AB$, then
$$
    \normo{ \Chat - AB } \le c_1(m,n,p) \u \normo{A}\normo{B} + O(\u^2).
$$

\item The computed solution $\Xhat$ to the triangular systems $TX=B$,
where $T$ is $m$-by-$m$ and $B$ is $m$-by-$p$, satisfies
$$
    \normo{ T \Xhat - B } \le c_2(m,p) \u \normo{T} \normo{\Xhat}
        + O(\u^2).
$$

\end{enumerate}

For conventional Level~3 BLAS implementations these conditions
hold with $c_1(m,n,p) = n^2$ and $c_2(m,p)= m(m+1)$.
Strassen's method\index{Strassen's method} satisfies these
bounds for slightly larger $c_1$ and $c_2$.

For further details, and references to fast multiplication techniques,
see~\cite{Demmel-Higham-Wnote22}.
